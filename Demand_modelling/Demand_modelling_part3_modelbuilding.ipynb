{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:16:22.740920Z",
     "start_time": "2021-01-09T20:16:21.651477Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:16:23.485558Z",
     "start_time": "2021-01-09T20:16:23.293535Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(os.getcwd()+'\\sales_dir\\sales_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:01:23.376471Z",
     "start_time": "2021-01-09T20:01:23.353474Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Size</th>\n",
       "      <th>Type_A</th>\n",
       "      <th>Type_B</th>\n",
       "      <th>Type_C</th>\n",
       "      <th>Markdown_total</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>16567.69</td>\n",
       "      <td>0</td>\n",
       "      <td>49.01</td>\n",
       "      <td>3.157</td>\n",
       "      <td>219.714258</td>\n",
       "      <td>7.348</td>\n",
       "      <td>151315</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38166.78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>16894.40</td>\n",
       "      <td>0</td>\n",
       "      <td>48.53</td>\n",
       "      <td>3.261</td>\n",
       "      <td>219.892526</td>\n",
       "      <td>7.348</td>\n",
       "      <td>151315</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17423.86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-20</td>\n",
       "      <td>18365.10</td>\n",
       "      <td>0</td>\n",
       "      <td>54.11</td>\n",
       "      <td>3.268</td>\n",
       "      <td>219.985689</td>\n",
       "      <td>7.348</td>\n",
       "      <td>151315</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10881.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-27</td>\n",
       "      <td>18378.16</td>\n",
       "      <td>0</td>\n",
       "      <td>54.26</td>\n",
       "      <td>3.290</td>\n",
       "      <td>220.078852</td>\n",
       "      <td>7.348</td>\n",
       "      <td>151315</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3524.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-02-03</td>\n",
       "      <td>23510.49</td>\n",
       "      <td>0</td>\n",
       "      <td>56.55</td>\n",
       "      <td>3.360</td>\n",
       "      <td>220.172015</td>\n",
       "      <td>7.348</td>\n",
       "      <td>151315</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76351.07</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept        Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  \\\n",
       "0      1     1  2012-01-06      16567.69          0        49.01       3.157   \n",
       "1      1     1  2012-01-13      16894.40          0        48.53       3.261   \n",
       "2      1     1  2012-01-20      18365.10          0        54.11       3.268   \n",
       "3      1     1  2012-01-27      18378.16          0        54.26       3.290   \n",
       "4      1     1  2012-02-03      23510.49          0        56.55       3.360   \n",
       "\n",
       "          CPI  Unemployment    Size  Type_A  Type_B  Type_C  Markdown_total  \\\n",
       "0  219.714258         7.348  151315       1       0       0        38166.78   \n",
       "1  219.892526         7.348  151315       1       0       0        17423.86   \n",
       "2  219.985689         7.348  151315       1       0       0        10881.77   \n",
       "3  220.078852         7.348  151315       1       0       0         3524.39   \n",
       "4  220.172015         7.348  151315       1       0       0        76351.07   \n",
       "\n",
       "   month  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:01:33.443548Z",
     "start_time": "2021-01-09T20:01:33.436548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Temperature',\n",
       "       'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'Type_A', 'Type_B',\n",
       "       'Type_C', 'Markdown_total', 'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:16:27.166844Z",
     "start_time": "2021-01-09T20:16:27.151846Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns = ['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:44:00.780873Z",
     "start_time": "2021-01-09T20:43:41.105152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.3.1-py3-none-win_amd64.whl (95.2 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\sonia\\anaconda3\\lib\\site-packages (from xgboost) (1.18.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\sonia\\anaconda3\\lib\\site-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:44:06.539605Z",
     "start_time": "2021-01-09T20:44:06.308887Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:36:50.935717Z",
     "start_time": "2021-01-09T20:36:50.904443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "\n",
    "def knn():\n",
    "    knn = KNeighborsRegressor(n_neighbors=10)\n",
    "    return knn\n",
    "\n",
    "def extraTreesRegressor():\n",
    "    clf = ExtraTreesRegressor(n_estimators =100, max_features = 'auto', verbose = 1, n_jobs = 1)\n",
    "    return clf\n",
    "\n",
    "def randomForestRegressor():\n",
    "    clf = RandomForestRegressor(n_estimators = 100, max_features = 'log2', verbose = 1)\n",
    "    return clf\n",
    "\n",
    "def svm():\n",
    "    clf = SVR(kernel = 'rbf', gamma = 'auto')\n",
    "    return clf\n",
    "\n",
    "def nn():\n",
    "    clf = MLPRegressor(hidden_layer_sizes=(10,), activation = 'relu', verbose = 3)\n",
    "    return clf\n",
    "\n",
    "def linear_reg():\n",
    "    reg = LinearRegression()\n",
    "    return reg\n",
    "\n",
    "def xg():\n",
    "    xg = XGBRegressor()\n",
    "    return xg\n",
    "\n",
    "def ridge_reg():\n",
    "    rid = Ridge()\n",
    "    return rid\n",
    "\n",
    "def lasso_reg():\n",
    "    las = Lasso()\n",
    "    return las\n",
    "\n",
    "def predict_(m,test_x):\n",
    "    return pd.Series(m.predict(test_x))\n",
    "\n",
    "def model_(modelfn_name):\n",
    "    #return knn()\n",
    "    return modelfn_name()\n",
    "    #return randomForestRegressor()\n",
    "    #return svm()\n",
    "    #return nn()\n",
    "    \n",
    "    \n",
    "def train_(train_x, train_y,modelfn_name):\n",
    "    m = model_(modelfn_name)\n",
    "    if train_x.shape[0] > 10000:\n",
    "        length = int(train_x.shape[0]/10000)\n",
    "        start_idx = 0\n",
    "        for i in range(length):\n",
    "            tx_dat = train_x[start_idx:start_idx+10000]\n",
    "            ty_dat = train_y[start_idx:start_idx+10000]\n",
    "            m.fit(tx_dat, ty_dat)\n",
    "            start_idx +=10000\n",
    "            gc.collect()\n",
    "    else:\n",
    "        m.fit(train_x,train_y)\n",
    "    #m.fit(train_x,train_y)\n",
    "    return m\n",
    "\n",
    "def train_and_predict(train_x, train_y, test_x,modelfn_name):\n",
    "    m = train_(train_x,train_y,modelfn_name)\n",
    "    return predict_(m,test_x), m\n",
    "\n",
    "def calculate_error(test_y,predicted, weights):\n",
    "    # Print out the MAE, MSE & RMSE\n",
    "    print(\"MAE: \", mean_absolute_error(test_y, predicted,weights)) #MAE\n",
    "    print(\"MSE: \", mean_squared_error(test_y, predicted,weights)) #MSE\n",
    "    print(\"RMSE: \", np.sqrt(mean_squared_error(test_y, predicted, weights))) #RMSE\n",
    "    print(\"R2: \", r2_score(test_y, predicted,weights))# RSquared\n",
    "    return mean_absolute_error(test_y, predicted, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:16:49.741108Z",
     "start_time": "2021-01-09T20:16:33.887017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3075\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "splits = []\n",
    "\n",
    "for name,group in dataset.groupby([\"Store\", \"Dept\"]): # segregating for every store and their respective departments\n",
    "    group = group.reset_index(drop = True) # setting index to start from 0 and to drop the old index\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    if group.shape[0] <= 5: # for groups less than 5 entries, the rows are marked with variable fold as 0,1,2,3,4\n",
    "        f = np.array(range(5))\n",
    "        np.random.shuffle(f)\n",
    "        group['fold'] = f[:group.shape[0]]\n",
    "        continue\n",
    "    fold = 0\n",
    "    for train_idx, test_idx in (kf.split(group)):\n",
    "        group.loc[test_idx,'fold'] = fold # All the test rows have values for this variable fold.\n",
    "        fold +=1\n",
    "    splits.append(group)\n",
    "print(len(splits)) # 16065\n",
    "splits = pd.concat(splits).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:16:54.010037Z",
     "start_time": "2021-01-09T20:16:53.994412Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:17:02.938880Z",
     "start_time": "2021-01-09T20:17:02.893882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:07:14.859728Z",
     "start_time": "2021-01-09T20:07:09.166372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100454, 15) (26662, 15)\n",
      "MAE:  12013.703836596897\n",
      "MSE:  420253491.0947748\n",
      "RMSE:  20500.085148476206\n",
      "R2:  0.2964639410857689\n",
      "0 12013.703836596897\n",
      "----------------------------\n",
      "Find best model\n",
      "(100524, 15) (26592, 15)\n",
      "MAE:  5528.1504545454545\n",
      "MSE:  59484503.33383972\n",
      "RMSE:  7712.619745186438\n",
      "R2:  -64.8914373555292\n",
      "1 5528.1504545454545\n",
      "----------------------------\n",
      "Find best model\n",
      "(100605, 15) (26511, 15)\n",
      "MAE:  24795.431999999997\n",
      "MSE:  1288123560.1418812\n",
      "RMSE:  35890.43828294496\n",
      "R2:  -605842.9399228543\n",
      "2 24795.431999999997\n",
      "----------------------------\n",
      "(103416, 15) (23700, 15)\n",
      "MAE:  9729.405714285715\n",
      "MSE:  168066277.49750796\n",
      "RMSE:  12964.037854677375\n",
      "R2:  -359.77493601654953\n",
      "3 9729.405714285715\n",
      "----------------------------\n",
      "(103465, 15) (23651, 15)\n",
      "MAE:  11334.57279295922\n",
      "MSE:  360670098.31960887\n",
      "RMSE:  18991.31639248867\n",
      "R2:  0.34833586717733356\n",
      "4 11334.57279295922\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "#models\n",
    "\n",
    "\n",
    "best_model = None\n",
    "error_cv = 0\n",
    "best_error = np.iinfo(np.int32).max\n",
    "for fold in range(5):\n",
    "    dataset_train = splits.loc[splits['fold'] != fold]\n",
    "    dataset_test = splits.loc[splits['fold'] == fold]\n",
    "\n",
    "    train_y = dataset_train['Weekly_Sales']\n",
    "    train_x = dataset_train.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    test_y = dataset_test['Weekly_Sales']\n",
    "    test_x = dataset_test.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    print(dataset_train.shape, dataset_test.shape)\n",
    "    dataset_train = None\n",
    "    dataset_test = None\n",
    "    gc.collect()\n",
    "    predicted, model = train_and_predict(train_x, train_y, test_x,knn)\n",
    "    weights = test_x['IsHoliday'].replace(True, 5).replace(False, 1)\n",
    "    error = calculate_error(test_y, predicted, weights)\n",
    "    error_cv += error\n",
    "    print(fold, error)\n",
    "    print('----------------------------')\n",
    "    if error < best_error:\n",
    "        print('Find best model')\n",
    "        best_error = error\n",
    "        best_model = model\n",
    "error_cv /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:10:33.483851Z",
     "start_time": "2021-01-09T20:10:33.478854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12680.252959677455, 5528.1504545454545)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_cv, best_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:19:12.044875Z",
     "start_time": "2021-01-09T20:17:09.220414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100454, 15) (26662, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  8287.809934571234\n",
      "MSE:  245065863.8656965\n",
      "RMSE:  15654.579645129297\n",
      "R2:  0.5897412497648915\n",
      "0 8287.809934571234\n",
      "Find best model\n",
      "(100524, 15) (26592, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  6063.65538181818\n",
      "MSE:  175855640.91666666\n",
      "RMSE:  13261.057307645822\n",
      "R2:  -193.79663269685764\n",
      "1 6063.65538181818\n",
      "Find best model\n",
      "(100605, 15) (26511, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  3462.4336800000005\n",
      "MSE:  23369046.17426067\n",
      "RMSE:  4834.154132240787\n",
      "R2:  -10990.177744543193\n",
      "2 3462.4336800000005\n",
      "Find best model\n",
      "(103416, 15) (23700, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  4635.91131904762\n",
      "MSE:  179138026.41713896\n",
      "RMSE:  13384.245455651915\n",
      "R2:  -383.5418068460079\n",
      "3 4635.91131904762\n",
      "(103465, 15) (23651, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  8389.9679830603\n",
      "MSE:  249771108.88062197\n",
      "RMSE:  15804.14847059537\n",
      "R2:  0.5487098214374013\n",
      "4 8389.9679830603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# ExtraTreesRegressor\n",
    "\n",
    "best_model = None\n",
    "error_cv = 0\n",
    "best_error = np.iinfo(np.int32).max\n",
    "for fold in range(5):\n",
    "    dataset_train = splits.loc[splits['fold'] != fold]\n",
    "    dataset_test = splits.loc[splits['fold'] == fold]\n",
    "    \n",
    "    train_y = dataset_train['Weekly_Sales']\n",
    "    train_x = dataset_train.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    test_y = dataset_test['Weekly_Sales']\n",
    "    test_x = dataset_test.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    print(dataset_train.shape, dataset_test.shape)\n",
    "    dataset_train = None\n",
    "    dataset_test = None\n",
    "    gc.collect()\n",
    "    predicted, model = train_and_predict(train_x, train_y, test_x,extraTreesRegressor)\n",
    "    weights = test_x['IsHoliday'].replace(True, 5).replace(False, 1)\n",
    "    error = calculate_error(test_y, predicted, weights)\n",
    "    error_cv += error\n",
    "    print(fold, error)\n",
    "    if error < best_error:\n",
    "        print('Find best model')\n",
    "        best_error = error\n",
    "        best_model = model\n",
    "error_cv /= 5\n",
    "\n",
    "print(error_cv, best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:21:33.538996Z",
     "start_time": "2021-01-09T20:20:32.448717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100454, 15) (26662, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  9985.000570762999\n",
      "MSE:  327357441.89297765\n",
      "RMSE:  18093.02191158176\n",
      "R2:  0.45197893793655286\n",
      "0 9985.000570762999\n",
      "Find best model\n",
      "(100524, 15) (26592, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  6790.491063636363\n",
      "MSE:  179661178.21588486\n",
      "RMSE:  13403.774774886546\n",
      "R2:  -198.01205534480832\n",
      "1 6790.491063636363\n",
      "Find best model\n",
      "(100605, 15) (26511, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  6876.110599999998\n",
      "MSE:  87485468.66977003\n",
      "RMSE:  9353.366702410958\n",
      "R2:  -41146.09385414309\n",
      "2 6876.110599999998\n",
      "(103416, 15) (23700, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  6530.505130952381\n",
      "MSE:  159064826.48719364\n",
      "RMSE:  12612.090488384296\n",
      "R2:  -340.4522142864249\n",
      "3 6530.505130952381\n",
      "Find best model\n",
      "(103465, 15) (23651, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  10084.710981596376\n",
      "MSE:  306977513.970791\n",
      "RMSE:  17520.77378344892\n",
      "R2:  0.4453484323489387\n",
      "4 10084.710981596376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# randomForestRegressor\n",
    "\n",
    "best_model = None\n",
    "error_cv = 0\n",
    "best_error = np.iinfo(np.int32).max\n",
    "for fold in range(5):\n",
    "    dataset_train = splits.loc[splits['fold'] != fold]\n",
    "    dataset_test = splits.loc[splits['fold'] == fold]\n",
    "    \n",
    "    train_y = dataset_train['Weekly_Sales']\n",
    "    train_x = dataset_train.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    test_y = dataset_test['Weekly_Sales']\n",
    "    test_x = dataset_test.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    print(dataset_train.shape, dataset_test.shape)\n",
    "    dataset_train = None\n",
    "    dataset_test = None\n",
    "    gc.collect()\n",
    "    predicted, model = train_and_predict(train_x, train_y, test_x,randomForestRegressor)\n",
    "    weights = test_x['IsHoliday'].replace(True, 5).replace(False, 1)\n",
    "    error = calculate_error(test_y, predicted, weights)\n",
    "    error_cv += error\n",
    "    print(fold, error)\n",
    "    if error < best_error:\n",
    "        print('Find best model')\n",
    "        best_error = error\n",
    "        best_model = model\n",
    "error_cv /= 5\n",
    "\n",
    "print(error_cv, best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:29:24.730933Z",
     "start_time": "2021-01-09T20:21:48.613048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100454, 15) (26662, 15)\n",
      "MAE:  14397.872504284174\n",
      "MSE:  731656130.8235947\n",
      "RMSE:  27049.14288519314\n",
      "R2:  -0.22484757811085543\n",
      "0 14397.872504284174\n",
      "Find best model\n",
      "(100524, 15) (26592, 15)\n",
      "MAE:  4738.291121282027\n",
      "MSE:  23354000.49994658\n",
      "RMSE:  4832.597696885867\n",
      "R2:  -24.869404209479452\n",
      "1 4738.291121282027\n",
      "Find best model\n",
      "(100605, 15) (26511, 15)\n",
      "MAE:  4915.362618533159\n",
      "MSE:  24162915.44284874\n",
      "RMSE:  4915.578851249234\n",
      "R2:  -11363.558761976097\n",
      "2 4915.362618533159\n",
      "(103416, 15) (23700, 15)\n",
      "MAE:  5590.527264270527\n",
      "MSE:  31719747.95017305\n",
      "RMSE:  5632.028759707558\n",
      "R2:  -67.09034035608032\n",
      "3 5590.527264270527\n",
      "(103465, 15) (23651, 15)\n",
      "MAE:  14207.367899029792\n",
      "MSE:  672466032.6398172\n",
      "RMSE:  25931.950035425743\n",
      "R2:  -0.2150216944921053\n",
      "4 14207.367899029792\n",
      "8769.884281479937 4738.291121282027\n"
     ]
    }
   ],
   "source": [
    "# svm\n",
    "\n",
    "best_model = None\n",
    "error_cv = 0\n",
    "best_error = np.iinfo(np.int32).max\n",
    "for fold in range(5):\n",
    "    dataset_train = splits.loc[splits['fold'] != fold]\n",
    "    dataset_test = splits.loc[splits['fold'] == fold]\n",
    "    \n",
    "    train_y = dataset_train['Weekly_Sales']\n",
    "    train_x = dataset_train.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    test_y = dataset_test['Weekly_Sales']\n",
    "    test_x = dataset_test.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    print(dataset_train.shape, dataset_test.shape)\n",
    "    dataset_train = None\n",
    "    dataset_test = None\n",
    "    gc.collect()\n",
    "    predicted, model = train_and_predict(train_x, train_y, test_x,svm)\n",
    "    weights = test_x['IsHoliday'].replace(True, 5).replace(False, 1)\n",
    "    error = calculate_error(test_y, predicted, weights)\n",
    "    error_cv += error\n",
    "    print(fold, error)\n",
    "    if error < best_error:\n",
    "        print('Find best model')\n",
    "        best_error = error\n",
    "        best_model = model\n",
    "error_cv /= 5\n",
    "\n",
    "print(error_cv, best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:31:07.804084Z",
     "start_time": "2021-01-09T20:29:33.218369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100454, 15) (26662, 15)\n",
      "Iteration 1, loss = 432967936.40040475\n",
      "Iteration 2, loss = 399756846.49596012\n",
      "Iteration 3, loss = 399800999.78720820\n",
      "Iteration 4, loss = 399785693.63468081\n",
      "Iteration 5, loss = 399755616.39577663\n",
      "Iteration 6, loss = 399572934.70492619\n",
      "Iteration 7, loss = 399540717.75882530\n",
      "Iteration 8, loss = 399442664.53570437\n",
      "Iteration 9, loss = 399600199.48858464\n",
      "Iteration 10, loss = 399507877.78876990\n",
      "Iteration 11, loss = 399501176.88189358\n",
      "Iteration 12, loss = 399555913.82409215\n",
      "Iteration 13, loss = 399362512.64775842\n",
      "Iteration 14, loss = 399528597.02480292\n",
      "Iteration 15, loss = 399340959.13015467\n",
      "Iteration 16, loss = 399508142.74466193\n",
      "Iteration 17, loss = 399733615.14555722\n",
      "Iteration 18, loss = 399400159.98945415\n",
      "Iteration 19, loss = 399377264.51864505\n",
      "Iteration 20, loss = 399398433.66280383\n",
      "Iteration 21, loss = 399784800.82055354\n",
      "Iteration 22, loss = 399344260.52675045\n",
      "Iteration 23, loss = 399428162.00420970\n",
      "Iteration 24, loss = 399288454.64080524\n",
      "Iteration 25, loss = 398980778.06225652\n",
      "Iteration 26, loss = 399361810.48259413\n",
      "Iteration 27, loss = 399591335.49457181\n",
      "Iteration 28, loss = 399230428.77049947\n",
      "Iteration 29, loss = 399188999.96083993\n",
      "Iteration 30, loss = 399367354.09074241\n",
      "Iteration 31, loss = 399259147.12369812\n",
      "Iteration 32, loss = 399097885.46185327\n",
      "Iteration 33, loss = 399189103.05572391\n",
      "Iteration 34, loss = 399363178.76636189\n",
      "Iteration 35, loss = 399125963.23867780\n",
      "Iteration 36, loss = 399273093.16494632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 125245712.59158950\n",
      "Iteration 2, loss = 125059743.92825077\n",
      "Iteration 3, loss = 125121454.22947717\n",
      "Iteration 4, loss = 125058612.71710882\n",
      "Iteration 5, loss = 124990219.38323991\n",
      "Iteration 6, loss = 125007765.49149951\n",
      "Iteration 7, loss = 124982524.76604517\n",
      "Iteration 8, loss = 125001076.27800672\n",
      "Iteration 9, loss = 125019395.30889471\n",
      "Iteration 10, loss = 124993548.06298615\n",
      "Iteration 11, loss = 124991414.38556743\n",
      "Iteration 12, loss = 124903220.28078566\n",
      "Iteration 13, loss = 124946730.58722915\n",
      "Iteration 14, loss = 125041715.26575881\n",
      "Iteration 15, loss = 125145555.08560269\n",
      "Iteration 16, loss = 124938972.78171021\n",
      "Iteration 17, loss = 125027460.36126253\n",
      "Iteration 18, loss = 124926820.38235593\n",
      "Iteration 19, loss = 124910424.88868530\n",
      "Iteration 20, loss = 124972678.55345576\n",
      "Iteration 21, loss = 124939134.11681619\n",
      "Iteration 22, loss = 124906225.73148857\n",
      "Iteration 23, loss = 124899808.32296236\n",
      "Iteration 24, loss = 125042146.15216108\n",
      "Iteration 25, loss = 124876073.56552143\n",
      "Iteration 26, loss = 124894114.61067957\n",
      "Iteration 27, loss = 124925497.40219679\n",
      "Iteration 28, loss = 124858068.07105486\n",
      "Iteration 29, loss = 124906609.49465075\n",
      "Iteration 30, loss = 124947792.27389340\n",
      "Iteration 31, loss = 124860880.38490862\n",
      "Iteration 32, loss = 124948113.37137716\n",
      "Iteration 33, loss = 124886377.93217801\n",
      "Iteration 34, loss = 124868436.91861045\n",
      "Iteration 35, loss = 124814810.42731963\n",
      "Iteration 36, loss = 124832870.38217488\n",
      "Iteration 37, loss = 124940276.26398228\n",
      "Iteration 38, loss = 124829138.59528865\n",
      "Iteration 39, loss = 124714890.36190259\n",
      "Iteration 40, loss = 124893174.24994531\n",
      "Iteration 41, loss = 124849631.07680479\n",
      "Iteration 42, loss = 124841427.48907319\n",
      "Iteration 43, loss = 124894728.39200781\n",
      "Iteration 44, loss = 124845453.33978295\n",
      "Iteration 45, loss = 124922385.92717469\n",
      "Iteration 46, loss = 124825507.90736946\n",
      "Iteration 47, loss = 124803906.52781294\n",
      "Iteration 48, loss = 124793212.35293533\n",
      "Iteration 49, loss = 124875113.87727869\n",
      "Iteration 50, loss = 124710952.76937652\n",
      "Iteration 51, loss = 124697668.87724124\n",
      "Iteration 52, loss = 124816877.89830545\n",
      "Iteration 53, loss = 124877411.42514944\n",
      "Iteration 54, loss = 124907456.13399124\n",
      "Iteration 55, loss = 124808899.02279113\n",
      "Iteration 56, loss = 124830921.57524082\n",
      "Iteration 57, loss = 124924055.77592762\n",
      "Iteration 58, loss = 124847117.65284705\n",
      "Iteration 59, loss = 124764545.65187925\n",
      "Iteration 60, loss = 124822719.22503828\n",
      "Iteration 61, loss = 124818583.47628982\n",
      "Iteration 62, loss = 124736499.73576096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 360974712.44400507\n",
      "Iteration 2, loss = 263625214.68510342\n",
      "Iteration 3, loss = 260739936.83722842\n",
      "Iteration 4, loss = 258086419.45195296\n",
      "Iteration 5, loss = 255726666.25875700\n",
      "Iteration 6, loss = 253770571.05767775\n",
      "Iteration 7, loss = 252112612.44022685\n",
      "Iteration 8, loss = 250865145.47679442\n",
      "Iteration 9, loss = 249839908.56127730\n",
      "Iteration 10, loss = 248962046.55708638\n",
      "Iteration 11, loss = 248406404.15161791\n",
      "Iteration 12, loss = 247850728.83117720\n",
      "Iteration 13, loss = 247557331.44566920\n",
      "Iteration 14, loss = 247360203.43053871\n",
      "Iteration 15, loss = 247118941.41270986\n",
      "Iteration 16, loss = 247211762.13188627\n",
      "Iteration 17, loss = 246858705.29691914\n",
      "Iteration 18, loss = 246802614.46230155\n",
      "Iteration 19, loss = 246831816.11295834\n",
      "Iteration 20, loss = 247060660.38882935\n",
      "Iteration 21, loss = 246670493.63165942\n",
      "Iteration 22, loss = 246747753.60136601\n",
      "Iteration 23, loss = 246768149.66860604\n",
      "Iteration 24, loss = 246750036.35967001\n",
      "Iteration 25, loss = 246758394.49141178\n",
      "Iteration 26, loss = 246716660.09221110\n",
      "Iteration 27, loss = 246796912.68388486\n",
      "Iteration 28, loss = 246693952.71471342\n",
      "Iteration 29, loss = 246588505.91421232\n",
      "Iteration 30, loss = 246580831.06414589\n",
      "Iteration 31, loss = 246639038.64549291\n",
      "Iteration 32, loss = 246669719.83062607\n",
      "Iteration 33, loss = 246622753.19251275\n",
      "Iteration 34, loss = 246631558.56681231\n",
      "Iteration 35, loss = 247016551.88381338\n",
      "Iteration 36, loss = 246583093.65683272\n",
      "Iteration 37, loss = 246661915.38058209\n",
      "Iteration 38, loss = 246564014.95962316\n",
      "Iteration 39, loss = 246896726.08702949\n",
      "Iteration 40, loss = 246621632.25836352\n",
      "Iteration 41, loss = 246618672.38579854\n",
      "Iteration 42, loss = 246626704.65209371\n",
      "Iteration 43, loss = 246706093.78609151\n",
      "Iteration 44, loss = 246567942.83238646\n",
      "Iteration 45, loss = 246848643.21165770\n",
      "Iteration 46, loss = 246659731.75205874\n",
      "Iteration 47, loss = 246717648.47381094\n",
      "Iteration 48, loss = 246684149.62908852\n",
      "Iteration 49, loss = 246709729.98584980\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 517197344.17100155\n",
      "Iteration 2, loss = 328607751.70302927\n",
      "Iteration 3, loss = 285714651.49649644\n",
      "Iteration 4, loss = 280063132.38090515\n",
      "Iteration 5, loss = 279626617.89898396\n",
      "Iteration 6, loss = 279556611.92792678\n",
      "Iteration 7, loss = 279491836.16313171\n",
      "Iteration 8, loss = 279457983.84693903\n",
      "Iteration 9, loss = 279391340.47525996\n",
      "Iteration 10, loss = 279370631.13818055\n",
      "Iteration 11, loss = 279294429.69583243\n",
      "Iteration 12, loss = 279263496.65444648\n",
      "Iteration 13, loss = 279160077.38686478\n",
      "Iteration 14, loss = 279158671.51942742\n",
      "Iteration 15, loss = 279132158.55830681\n",
      "Iteration 16, loss = 279113159.31402564\n",
      "Iteration 17, loss = 279149889.28991669\n",
      "Iteration 18, loss = 279009638.60793221\n",
      "Iteration 19, loss = 279083563.87234807\n",
      "Iteration 20, loss = 278952557.29453748\n",
      "Iteration 21, loss = 278833095.56128955\n",
      "Iteration 22, loss = 278809281.87020135\n",
      "Iteration 23, loss = 278810806.13684738\n",
      "Iteration 24, loss = 278810866.12002736\n",
      "Iteration 25, loss = 278775368.74625432\n",
      "Iteration 26, loss = 278672191.32538491\n",
      "Iteration 27, loss = 278784042.95553470\n",
      "Iteration 28, loss = 278608136.61075848\n",
      "Iteration 29, loss = 278710559.30893797\n",
      "Iteration 30, loss = 278661548.63419700\n",
      "Iteration 31, loss = 278763518.09089988\n",
      "Iteration 32, loss = 278539163.10679394\n",
      "Iteration 33, loss = 278769276.43439907\n",
      "Iteration 34, loss = 278558580.44251257\n",
      "Iteration 35, loss = 278637051.48915768\n",
      "Iteration 36, loss = 278524964.35352767\n",
      "Iteration 37, loss = 278587946.20728409\n",
      "Iteration 38, loss = 278461062.37945217\n",
      "Iteration 39, loss = 278406778.30179077\n",
      "Iteration 40, loss = 278573404.77152365\n",
      "Iteration 41, loss = 278418783.26973999\n",
      "Iteration 42, loss = 278612279.79416239\n",
      "Iteration 43, loss = 278429072.39550281\n",
      "Iteration 44, loss = 278454830.72584039\n",
      "Iteration 45, loss = 278339172.53992885\n",
      "Iteration 46, loss = 278390087.91090816\n",
      "Iteration 47, loss = 278454453.99098557\n",
      "Iteration 48, loss = 278399011.62969488\n",
      "Iteration 49, loss = 278346922.89472044\n",
      "Iteration 50, loss = 278317392.25584227\n",
      "Iteration 51, loss = 278331074.72916281\n",
      "Iteration 52, loss = 278487976.98197830\n",
      "Iteration 53, loss = 278319372.26317298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54, loss = 278406812.65623391\n",
      "Iteration 55, loss = 278269657.59629053\n",
      "Iteration 56, loss = 278348135.42928171\n",
      "Iteration 57, loss = 278461779.69649941\n",
      "Iteration 58, loss = 278212566.67433912\n",
      "Iteration 59, loss = 278539299.82581639\n",
      "Iteration 60, loss = 278777719.16880655\n",
      "Iteration 61, loss = 278200058.56918442\n",
      "Iteration 62, loss = 278304373.47324646\n",
      "Iteration 63, loss = 278155040.20961523\n",
      "Iteration 64, loss = 278114466.63647532\n",
      "Iteration 65, loss = 278249970.60445100\n",
      "Iteration 66, loss = 278227834.83263779\n",
      "Iteration 67, loss = 278289804.29836339\n",
      "Iteration 68, loss = 278090048.93146062\n",
      "Iteration 69, loss = 278066543.10894006\n",
      "Iteration 70, loss = 278163795.64983791\n",
      "Iteration 71, loss = 278101597.47112918\n",
      "Iteration 72, loss = 278020598.27975279\n",
      "Iteration 73, loss = 278013345.06033915\n",
      "Iteration 74, loss = 278052162.95189589\n",
      "Iteration 75, loss = 278193592.71906453\n",
      "Iteration 76, loss = 278188993.29789764\n",
      "Iteration 77, loss = 277760855.01622790\n",
      "Iteration 78, loss = 278224865.79557389\n",
      "Iteration 79, loss = 278186465.16187954\n",
      "Iteration 80, loss = 277924686.02263141\n",
      "Iteration 81, loss = 277980313.80081409\n",
      "Iteration 82, loss = 278066315.23800713\n",
      "Iteration 83, loss = 277959832.61400127\n",
      "Iteration 84, loss = 278226737.28711003\n",
      "Iteration 85, loss = 277802235.28529608\n",
      "Iteration 86, loss = 277956546.00316030\n",
      "Iteration 87, loss = 277928760.70133793\n",
      "Iteration 88, loss = 277927308.09697235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1089159154.72746181\n",
      "Iteration 2, loss = 312145539.58373147\n",
      "Iteration 3, loss = 291831009.86729068\n",
      "Iteration 4, loss = 290188821.11421263\n",
      "Iteration 5, loss = 288471520.77615112\n",
      "Iteration 6, loss = 286843101.38146579\n",
      "Iteration 7, loss = 285170737.52107608\n",
      "Iteration 8, loss = 283563373.32931924\n",
      "Iteration 9, loss = 282054609.19770253\n",
      "Iteration 10, loss = 280768489.22471464\n",
      "Iteration 11, loss = 279595503.46954226\n",
      "Iteration 12, loss = 278448910.17950046\n",
      "Iteration 13, loss = 277507272.64141047\n",
      "Iteration 14, loss = 276700947.99707198\n",
      "Iteration 15, loss = 276003669.88399529\n",
      "Iteration 16, loss = 275407697.70615149\n",
      "Iteration 17, loss = 274861494.62825042\n",
      "Iteration 18, loss = 274527221.38804913\n",
      "Iteration 19, loss = 274096347.23360443\n",
      "Iteration 20, loss = 274286473.64753836\n",
      "Iteration 21, loss = 273788048.52534837\n",
      "Iteration 22, loss = 273632934.00782019\n",
      "Iteration 23, loss = 273391072.64065421\n",
      "Iteration 24, loss = 273465576.83822680\n",
      "Iteration 25, loss = 273282219.23956907\n",
      "Iteration 26, loss = 273256602.81824332\n",
      "Iteration 27, loss = 273143981.73540461\n",
      "Iteration 28, loss = 273012208.93493354\n",
      "Iteration 29, loss = 273316306.50106591\n",
      "Iteration 30, loss = 273090180.45235944\n",
      "Iteration 31, loss = 273087493.37208295\n",
      "Iteration 32, loss = 273043761.64366931\n",
      "Iteration 33, loss = 272981818.28174824\n",
      "Iteration 34, loss = 273231274.36888558\n",
      "Iteration 35, loss = 273041660.86011386\n",
      "Iteration 36, loss = 273564494.18749428\n",
      "Iteration 37, loss = 273229618.37040395\n",
      "Iteration 38, loss = 272965022.01753098\n",
      "Iteration 39, loss = 273220443.68188733\n",
      "Iteration 40, loss = 273152675.08350748\n",
      "Iteration 41, loss = 273162955.97746456\n",
      "Iteration 42, loss = 273060165.94060373\n",
      "Iteration 43, loss = 273139452.94469744\n",
      "Iteration 44, loss = 273282774.69776136\n",
      "Iteration 45, loss = 273180125.21999800\n",
      "Iteration 46, loss = 273190488.71838099\n",
      "Iteration 47, loss = 273073995.12450618\n",
      "Iteration 48, loss = 273203013.38489127\n",
      "Iteration 49, loss = 273458026.74946404\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 334953575.44502825\n",
      "Iteration 2, loss = 197335574.62971896\n",
      "Iteration 3, loss = 192264513.51502720\n",
      "Iteration 4, loss = 189464378.28929758\n",
      "Iteration 5, loss = 187642965.60652256\n",
      "Iteration 6, loss = 186430181.40316710\n",
      "Iteration 7, loss = 185864206.15314606\n",
      "Iteration 8, loss = 185526309.80035815\n",
      "Iteration 9, loss = 185569399.91768655\n",
      "Iteration 10, loss = 185307433.38190195\n",
      "Iteration 11, loss = 185543111.47977692\n",
      "Iteration 12, loss = 185381316.49961036\n",
      "Iteration 13, loss = 185252840.45327675\n",
      "Iteration 14, loss = 185264445.43592110\n",
      "Iteration 15, loss = 185232565.94149700\n",
      "Iteration 16, loss = 185261172.89844245\n",
      "Iteration 17, loss = 185301430.73791850\n",
      "Iteration 18, loss = 185279621.17035109\n",
      "Iteration 19, loss = 185375880.57410255\n",
      "Iteration 20, loss = 185257624.62765956\n",
      "Iteration 21, loss = 185311143.69932324\n",
      "Iteration 22, loss = 185262259.38319546\n",
      "Iteration 23, loss = 185265196.43056598\n",
      "Iteration 24, loss = 185336221.91582081\n",
      "Iteration 25, loss = 185264758.10745686\n",
      "Iteration 26, loss = 185271964.92350420\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 441050480.25615597\n",
      "Iteration 2, loss = 226811272.67088741\n",
      "Iteration 3, loss = 213395635.56370190\n",
      "Iteration 4, loss = 213116744.82959336\n",
      "Iteration 5, loss = 212756190.12547565\n",
      "Iteration 6, loss = 212460171.04328045\n",
      "Iteration 7, loss = 212204918.75514489\n",
      "Iteration 8, loss = 212023632.17583311\n",
      "Iteration 9, loss = 211919271.08981189\n",
      "Iteration 10, loss = 211807950.57693481\n",
      "Iteration 11, loss = 211652421.68946511\n",
      "Iteration 12, loss = 211547324.75457519\n",
      "Iteration 13, loss = 211480657.13390744\n",
      "Iteration 14, loss = 211457789.31688735\n",
      "Iteration 15, loss = 211300962.92327672\n",
      "Iteration 16, loss = 211437785.69361103\n",
      "Iteration 17, loss = 211464015.19179982\n",
      "Iteration 18, loss = 211189937.64205495\n",
      "Iteration 19, loss = 211302459.78560784\n",
      "Iteration 20, loss = 211226568.55096212\n",
      "Iteration 21, loss = 211290544.77313200\n",
      "Iteration 22, loss = 211182828.31039742\n",
      "Iteration 23, loss = 211274974.04408732\n",
      "Iteration 24, loss = 211130620.35152587\n",
      "Iteration 25, loss = 211379408.49297026\n",
      "Iteration 26, loss = 211083292.89252278\n",
      "Iteration 27, loss = 211206594.37728626\n",
      "Iteration 28, loss = 211292616.80099788\n",
      "Iteration 29, loss = 211409783.09655008\n",
      "Iteration 30, loss = 211081311.84757829\n",
      "Iteration 31, loss = 211035759.47915456\n",
      "Iteration 32, loss = 211069459.99718273\n",
      "Iteration 33, loss = 210987320.74891171\n",
      "Iteration 34, loss = 210903860.19224566\n",
      "Iteration 35, loss = 211038682.87418541\n",
      "Iteration 36, loss = 211004516.85467300\n",
      "Iteration 37, loss = 211035156.33617172\n",
      "Iteration 38, loss = 210919703.30367953\n",
      "Iteration 39, loss = 211173774.77334452\n",
      "Iteration 40, loss = 210870931.21717167\n",
      "Iteration 41, loss = 210961355.20458031\n",
      "Iteration 42, loss = 210896273.85566929\n",
      "Iteration 43, loss = 210870367.84927988\n",
      "Iteration 44, loss = 210903602.15833268\n",
      "Iteration 45, loss = 210821636.15559313\n",
      "Iteration 46, loss = 210789407.47788858\n",
      "Iteration 47, loss = 210720399.74331912\n",
      "Iteration 48, loss = 210803437.33465040\n",
      "Iteration 49, loss = 210829781.08598903\n",
      "Iteration 50, loss = 210616449.41447556\n",
      "Iteration 51, loss = 210713797.36470443\n",
      "Iteration 52, loss = 210746555.27924812\n",
      "Iteration 53, loss = 210578397.95349756\n",
      "Iteration 54, loss = 210756405.04819244\n",
      "Iteration 55, loss = 210567814.63323560\n",
      "Iteration 56, loss = 210594186.40314126\n",
      "Iteration 57, loss = 210555716.78338867\n",
      "Iteration 58, loss = 210605970.28879601\n",
      "Iteration 59, loss = 210585815.00754502\n",
      "Iteration 60, loss = 210371455.57520586\n",
      "Iteration 61, loss = 210820531.45263353\n",
      "Iteration 62, loss = 211033011.07841727\n",
      "Iteration 63, loss = 211093249.09905145\n",
      "Iteration 64, loss = 210955528.28427947\n",
      "Iteration 65, loss = 210526482.83452207\n",
      "Iteration 66, loss = 210704828.05017716\n",
      "Iteration 67, loss = 210256164.67840230\n",
      "Iteration 68, loss = 210368267.15938982\n",
      "Iteration 69, loss = 210163125.92568254\n",
      "Iteration 70, loss = 210351247.75321507\n",
      "Iteration 71, loss = 210281812.90184110\n",
      "Iteration 72, loss = 210208833.46026361\n",
      "Iteration 73, loss = 210436861.67751941\n",
      "Iteration 74, loss = 210156507.90647992\n",
      "Iteration 75, loss = 210263038.94980687\n",
      "Iteration 76, loss = 210352856.51634145\n",
      "Iteration 77, loss = 210187188.78711632\n",
      "Iteration 78, loss = 210042743.40936926\n",
      "Iteration 79, loss = 210331506.99006394\n",
      "Iteration 80, loss = 210063335.43800512\n",
      "Iteration 81, loss = 210238566.79111603\n",
      "Iteration 82, loss = 210039686.69655484\n",
      "Iteration 83, loss = 210330095.19896075\n",
      "Iteration 84, loss = 210143923.27363181\n",
      "Iteration 85, loss = 210020782.62668011\n",
      "Iteration 86, loss = 209936725.60606241\n",
      "Iteration 87, loss = 209855944.63060731\n",
      "Iteration 88, loss = 210163708.54854658\n",
      "Iteration 89, loss = 209835686.30424446\n",
      "Iteration 90, loss = 209935275.45616764\n",
      "Iteration 91, loss = 209843696.33487508\n",
      "Iteration 92, loss = 209963619.96195289\n",
      "Iteration 93, loss = 209783885.16253683\n",
      "Iteration 94, loss = 209984071.86853951\n",
      "Iteration 95, loss = 210138526.64978403\n",
      "Iteration 96, loss = 209721003.76188511\n",
      "Iteration 97, loss = 209704516.19094455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 98, loss = 209735045.36091146\n",
      "Iteration 99, loss = 209600428.58757728\n",
      "Iteration 100, loss = 209652160.10262039\n",
      "Iteration 101, loss = 209748657.41995242\n",
      "Iteration 102, loss = 209579091.05962354\n",
      "Iteration 103, loss = 209689315.46457231\n",
      "Iteration 104, loss = 209522091.24208090\n",
      "Iteration 105, loss = 209545717.41341615\n",
      "Iteration 106, loss = 209562205.45665538\n",
      "Iteration 107, loss = 209733156.61109167\n",
      "Iteration 108, loss = 209992931.19324768\n",
      "Iteration 109, loss = 209394953.80634388\n",
      "Iteration 110, loss = 209537524.20619443\n",
      "Iteration 111, loss = 209515322.56389165\n",
      "Iteration 112, loss = 209387076.64875433\n",
      "Iteration 113, loss = 209360961.90654197\n",
      "Iteration 114, loss = 210011362.21424922\n",
      "Iteration 115, loss = 209510143.25750110\n",
      "Iteration 116, loss = 209338501.42055929\n",
      "Iteration 117, loss = 209272848.43318185\n",
      "Iteration 118, loss = 209260341.23316473\n",
      "Iteration 119, loss = 209524852.41803297\n",
      "Iteration 120, loss = 209464637.39205402\n",
      "Iteration 121, loss = 209088049.04330453\n",
      "Iteration 122, loss = 209351025.57501033\n",
      "Iteration 123, loss = 209176201.80367985\n",
      "Iteration 124, loss = 209149774.05201182\n",
      "Iteration 125, loss = 209458085.57831654\n",
      "Iteration 126, loss = 209425138.61381832\n",
      "Iteration 127, loss = 209000590.54298747\n",
      "Iteration 128, loss = 209054668.20933235\n",
      "Iteration 129, loss = 208965893.42712712\n",
      "Iteration 130, loss = 209074636.72293273\n",
      "Iteration 131, loss = 208859847.83790013\n",
      "Iteration 132, loss = 209133508.90170005\n",
      "Iteration 133, loss = 209076329.51097557\n",
      "Iteration 134, loss = 208731444.09315002\n",
      "Iteration 135, loss = 209392586.33174878\n",
      "Iteration 136, loss = 209108574.28804404\n",
      "Iteration 137, loss = 208792006.60135895\n",
      "Iteration 138, loss = 208918736.24989918\n",
      "Iteration 139, loss = 208770347.94585216\n",
      "Iteration 140, loss = 208681462.65170574\n",
      "Iteration 141, loss = 208610350.87582484\n",
      "Iteration 142, loss = 208798948.20515081\n",
      "Iteration 143, loss = 208606779.08578494\n",
      "Iteration 144, loss = 208542389.24148282\n",
      "Iteration 145, loss = 208892387.12553582\n",
      "Iteration 146, loss = 208684299.07589436\n",
      "Iteration 147, loss = 208697623.06305316\n",
      "Iteration 148, loss = 208553007.42512020\n",
      "Iteration 149, loss = 208623062.97647798\n",
      "Iteration 150, loss = 208429463.04417154\n",
      "Iteration 151, loss = 208449484.58385438\n",
      "Iteration 152, loss = 208619260.99590522\n",
      "Iteration 153, loss = 208969520.88008463\n",
      "Iteration 154, loss = 208411678.06754670\n",
      "Iteration 155, loss = 208490130.62096751\n",
      "Iteration 156, loss = 208397849.57015917\n",
      "Iteration 157, loss = 208349252.57363799\n",
      "Iteration 158, loss = 208179966.85066423\n",
      "Iteration 159, loss = 208220856.43364096\n",
      "Iteration 160, loss = 208883394.12495822\n",
      "Iteration 161, loss = 208303354.98740655\n",
      "Iteration 162, loss = 208135257.79859981\n",
      "Iteration 163, loss = 208057375.95771337\n",
      "Iteration 164, loss = 208115826.50562006\n",
      "Iteration 165, loss = 208266250.40963200\n",
      "Iteration 166, loss = 208137274.54035673\n",
      "Iteration 167, loss = 208015333.82585141\n",
      "Iteration 168, loss = 207907473.70445845\n",
      "Iteration 169, loss = 207913368.62752616\n",
      "Iteration 170, loss = 207893786.28480422\n",
      "Iteration 171, loss = 208162754.64337909\n",
      "Iteration 172, loss = 208017972.63409606\n",
      "Iteration 173, loss = 208167651.33929759\n",
      "Iteration 174, loss = 207943916.18475977\n",
      "Iteration 175, loss = 207913369.71667230\n",
      "Iteration 176, loss = 207835539.75242418\n",
      "Iteration 177, loss = 208251597.52465826\n",
      "Iteration 178, loss = 207704153.55094469\n",
      "Iteration 179, loss = 207871523.06719363\n",
      "Iteration 180, loss = 207960556.35412490\n",
      "Iteration 181, loss = 208042842.90309027\n",
      "Iteration 182, loss = 208101682.60781732\n",
      "Iteration 183, loss = 207850619.61079991\n",
      "Iteration 184, loss = 207689250.71425226\n",
      "Iteration 185, loss = 207678087.21336538\n",
      "Iteration 186, loss = 207644440.23156655\n",
      "Iteration 187, loss = 208367350.64333773\n",
      "Iteration 188, loss = 207461398.31279120\n",
      "Iteration 189, loss = 207361658.88243794\n",
      "Iteration 190, loss = 207468741.36722964\n",
      "Iteration 191, loss = 207356320.69402331\n",
      "Iteration 192, loss = 207789149.86855340\n",
      "Iteration 193, loss = 207383005.33799493\n",
      "Iteration 194, loss = 207266800.75088885\n",
      "Iteration 195, loss = 207297435.69662905\n",
      "Iteration 196, loss = 207279198.03664371\n",
      "Iteration 197, loss = 207660880.35045937\n",
      "Iteration 198, loss = 207768506.70783228\n",
      "Iteration 199, loss = 207466540.10361603\n",
      "Iteration 200, loss = 207465263.88339254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 229220250.68067789\n",
      "Iteration 2, loss = 189936883.67597345\n",
      "Iteration 3, loss = 189122050.24908462\n",
      "Iteration 4, loss = 188488498.10345727\n",
      "Iteration 5, loss = 187900116.49867681\n",
      "Iteration 6, loss = 187495783.83929539\n",
      "Iteration 7, loss = 187095833.72703019\n",
      "Iteration 8, loss = 186790348.17668930\n",
      "Iteration 9, loss = 186505654.51967147\n",
      "Iteration 10, loss = 186605784.23051789\n",
      "Iteration 11, loss = 186301364.09275433\n",
      "Iteration 12, loss = 186178339.65531233\n",
      "Iteration 13, loss = 186162814.56101766\n",
      "Iteration 14, loss = 186159112.88968605\n",
      "Iteration 15, loss = 186085410.99959174\n",
      "Iteration 16, loss = 186032280.04003930\n",
      "Iteration 17, loss = 186000347.97970504\n",
      "Iteration 18, loss = 185992349.63906458\n",
      "Iteration 19, loss = 185923040.81947932\n",
      "Iteration 20, loss = 186179529.82592821\n",
      "Iteration 21, loss = 185795557.12147698\n",
      "Iteration 22, loss = 185857299.99230394\n",
      "Iteration 23, loss = 186009052.18455085\n",
      "Iteration 24, loss = 185784153.44405866\n",
      "Iteration 25, loss = 185851308.94435275\n",
      "Iteration 26, loss = 185799167.22816151\n",
      "Iteration 27, loss = 185819888.18515375\n",
      "Iteration 28, loss = 185818247.62134320\n",
      "Iteration 29, loss = 185746918.04767960\n",
      "Iteration 30, loss = 185748431.80304307\n",
      "Iteration 31, loss = 185715064.55317208\n",
      "Iteration 32, loss = 185752704.19546434\n",
      "Iteration 33, loss = 185962731.55257860\n",
      "Iteration 34, loss = 185983222.53787214\n",
      "Iteration 35, loss = 185583923.12893564\n",
      "Iteration 36, loss = 185696163.44223478\n",
      "Iteration 37, loss = 185765754.85210672\n",
      "Iteration 38, loss = 185639989.25955635\n",
      "Iteration 39, loss = 185861097.04191115\n",
      "Iteration 40, loss = 185462547.78387031\n",
      "Iteration 41, loss = 185700521.29695329\n",
      "Iteration 42, loss = 185536070.01030761\n",
      "Iteration 43, loss = 185506519.69077152\n",
      "Iteration 44, loss = 185417305.88002068\n",
      "Iteration 45, loss = 185583324.44951332\n",
      "Iteration 46, loss = 185507270.10962412\n",
      "Iteration 47, loss = 185581386.06676400\n",
      "Iteration 48, loss = 185276196.01996860\n",
      "Iteration 49, loss = 185471505.69374409\n",
      "Iteration 50, loss = 185337228.24120286\n",
      "Iteration 51, loss = 185320766.48895338\n",
      "Iteration 52, loss = 185730529.93898213\n",
      "Iteration 53, loss = 185474782.32270744\n",
      "Iteration 54, loss = 185431236.41162917\n",
      "Iteration 55, loss = 185223124.61961573\n",
      "Iteration 56, loss = 185085740.12198046\n",
      "Iteration 57, loss = 185455842.71599478\n",
      "Iteration 58, loss = 185071433.33598214\n",
      "Iteration 59, loss = 185306836.02944288\n",
      "Iteration 60, loss = 185142570.91519520\n",
      "Iteration 61, loss = 185140868.92764103\n",
      "Iteration 62, loss = 185295104.22480938\n",
      "Iteration 63, loss = 185248011.52601674\n",
      "Iteration 64, loss = 185382070.08270806\n",
      "Iteration 65, loss = 185068540.33914825\n",
      "Iteration 66, loss = 185088422.44605598\n",
      "Iteration 67, loss = 184963630.28954619\n",
      "Iteration 68, loss = 184990511.46282250\n",
      "Iteration 69, loss = 184981034.60109133\n",
      "Iteration 70, loss = 184923509.87545738\n",
      "Iteration 71, loss = 185233909.31408548\n",
      "Iteration 72, loss = 184945113.06899172\n",
      "Iteration 73, loss = 185034265.24543011\n",
      "Iteration 74, loss = 185007512.55313131\n",
      "Iteration 75, loss = 184993893.66366214\n",
      "Iteration 76, loss = 184923001.28049156\n",
      "Iteration 77, loss = 184823417.17210773\n",
      "Iteration 78, loss = 184994652.46467471\n",
      "Iteration 79, loss = 184860494.35292107\n",
      "Iteration 80, loss = 184711996.58992919\n",
      "Iteration 81, loss = 184656961.54125601\n",
      "Iteration 82, loss = 184654707.58633876\n",
      "Iteration 83, loss = 184610706.73573175\n",
      "Iteration 84, loss = 184815170.13166454\n",
      "Iteration 85, loss = 184690422.04353732\n",
      "Iteration 86, loss = 184753004.74217889\n",
      "Iteration 87, loss = 184560562.09399831\n",
      "Iteration 88, loss = 184938955.09439868\n",
      "Iteration 89, loss = 184790088.78295928\n",
      "Iteration 90, loss = 185017340.72084922\n",
      "Iteration 91, loss = 184464937.60666448\n",
      "Iteration 92, loss = 184404167.46795267\n",
      "Iteration 93, loss = 184636076.88506797\n",
      "Iteration 94, loss = 184454994.17241693\n",
      "Iteration 95, loss = 184264854.05058831\n",
      "Iteration 96, loss = 184471161.74761328\n",
      "Iteration 97, loss = 184238472.07444933\n",
      "Iteration 98, loss = 184405739.30531341\n",
      "Iteration 99, loss = 184288796.38740608\n",
      "Iteration 100, loss = 184211618.77562699\n",
      "Iteration 101, loss = 184255696.39636821\n",
      "Iteration 102, loss = 184365230.37807190\n",
      "Iteration 103, loss = 184176384.99423063\n",
      "Iteration 104, loss = 184479740.00609562\n",
      "Iteration 105, loss = 184055006.96341616\n",
      "Iteration 106, loss = 184077982.58236784\n",
      "Iteration 107, loss = 184542812.17250258\n",
      "Iteration 108, loss = 184018540.59515142\n",
      "Iteration 109, loss = 184193592.62435475\n",
      "Iteration 110, loss = 184113248.81722760\n",
      "Iteration 111, loss = 184072028.80740926\n",
      "Iteration 112, loss = 184012150.71898741\n",
      "Iteration 113, loss = 183954676.72577009\n",
      "Iteration 114, loss = 183957431.09184241\n",
      "Iteration 115, loss = 184072953.29490811\n",
      "Iteration 116, loss = 183877211.85075033\n",
      "Iteration 117, loss = 183905842.99680719\n",
      "Iteration 118, loss = 183924841.91527724\n",
      "Iteration 119, loss = 183963475.46654248\n",
      "Iteration 120, loss = 183817451.81471363\n",
      "Iteration 121, loss = 183897090.73682216\n",
      "Iteration 122, loss = 184025772.39494815\n",
      "Iteration 123, loss = 183702520.15599781\n",
      "Iteration 124, loss = 183752502.94163537\n",
      "Iteration 125, loss = 183683070.35767636\n",
      "Iteration 126, loss = 183940808.14919120\n",
      "Iteration 127, loss = 183623660.45291534\n",
      "Iteration 128, loss = 183714223.89771596\n",
      "Iteration 129, loss = 183735743.86283246\n",
      "Iteration 130, loss = 183616254.74997562\n",
      "Iteration 131, loss = 183957646.69387904\n",
      "Iteration 132, loss = 183736266.51666677\n",
      "Iteration 133, loss = 183491182.70640078\n",
      "Iteration 134, loss = 183357911.95397714\n",
      "Iteration 135, loss = 183482284.49988872\n",
      "Iteration 136, loss = 183342076.10463277\n",
      "Iteration 137, loss = 183267301.94058681\n",
      "Iteration 138, loss = 183762785.89535832\n",
      "Iteration 139, loss = 183352290.26213822\n",
      "Iteration 140, loss = 183725970.82898250\n",
      "Iteration 141, loss = 183601314.03761783\n",
      "Iteration 142, loss = 183268224.67697069\n",
      "Iteration 143, loss = 183196412.53520155\n",
      "Iteration 144, loss = 183326879.08654657\n",
      "Iteration 145, loss = 183277823.16836271\n",
      "Iteration 146, loss = 183217966.80913094\n",
      "Iteration 147, loss = 183290276.70031959\n",
      "Iteration 148, loss = 183380981.65050131\n",
      "Iteration 149, loss = 183098247.25378686\n",
      "Iteration 150, loss = 183245799.92264897\n",
      "Iteration 151, loss = 182920742.70353410\n",
      "Iteration 152, loss = 183118125.07956371\n",
      "Iteration 153, loss = 183042460.67395714\n",
      "Iteration 154, loss = 183030832.13503230\n",
      "Iteration 155, loss = 183220133.36277804\n",
      "Iteration 156, loss = 182931688.80331132\n",
      "Iteration 157, loss = 182885972.33793634\n",
      "Iteration 158, loss = 183152697.28037229\n",
      "Iteration 159, loss = 182821354.33557039\n",
      "Iteration 160, loss = 183018679.96357167\n",
      "Iteration 161, loss = 183084598.49044582\n",
      "Iteration 162, loss = 182839897.15057164\n",
      "Iteration 163, loss = 182707470.26542386\n",
      "Iteration 164, loss = 182907544.09839812\n",
      "Iteration 165, loss = 182671612.35227579\n",
      "Iteration 166, loss = 182644020.84358683\n",
      "Iteration 167, loss = 182712076.50592443\n",
      "Iteration 168, loss = 182591708.07961318\n",
      "Iteration 169, loss = 182791649.30018014\n",
      "Iteration 170, loss = 182887314.15342709\n",
      "Iteration 171, loss = 182577344.72087809\n",
      "Iteration 172, loss = 182473447.48619261\n",
      "Iteration 173, loss = 182428337.12645948\n",
      "Iteration 174, loss = 182457917.34827098\n",
      "Iteration 175, loss = 182542885.05386305\n",
      "Iteration 176, loss = 182641549.41027439\n",
      "Iteration 177, loss = 182476913.68738922\n",
      "Iteration 178, loss = 182392656.68491897\n",
      "Iteration 179, loss = 182254154.90627193\n",
      "Iteration 180, loss = 182521414.66268882\n",
      "Iteration 181, loss = 182660702.27532095\n",
      "Iteration 182, loss = 182526457.44629186\n",
      "Iteration 183, loss = 182092644.94012091\n",
      "Iteration 184, loss = 182184371.93367848\n",
      "Iteration 185, loss = 182303797.47569755\n",
      "Iteration 186, loss = 182088812.71570334\n",
      "Iteration 187, loss = 182146194.74322301\n",
      "Iteration 188, loss = 182039843.18176088\n",
      "Iteration 189, loss = 182072880.62218690\n",
      "Iteration 190, loss = 181956899.67549849\n",
      "Iteration 191, loss = 182029959.73060650\n",
      "Iteration 192, loss = 181992831.29796380\n",
      "Iteration 193, loss = 182086575.36274517\n",
      "Iteration 194, loss = 181817743.84734175\n",
      "Iteration 195, loss = 181885314.81746534\n",
      "Iteration 196, loss = 181844926.12802845\n",
      "Iteration 197, loss = 181867281.78414071\n",
      "Iteration 198, loss = 181957174.71117967\n",
      "Iteration 199, loss = 181764063.38457775\n",
      "Iteration 200, loss = 182432388.61762041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 164337188.12165409\n",
      "Iteration 2, loss = 162940350.26689723\n",
      "Iteration 3, loss = 162586875.80983934\n",
      "Iteration 4, loss = 162681795.20485768\n",
      "Iteration 5, loss = 162582373.16139442\n",
      "Iteration 6, loss = 162725167.26500055\n",
      "Iteration 7, loss = 162342117.62563747\n",
      "Iteration 8, loss = 162394503.90291238\n",
      "Iteration 9, loss = 162113444.36831775\n",
      "Iteration 10, loss = 162591634.78731501\n",
      "Iteration 11, loss = 162295172.12385401\n",
      "Iteration 12, loss = 162461378.37193984\n",
      "Iteration 13, loss = 162017781.31309313\n",
      "Iteration 14, loss = 161985465.40243894\n",
      "Iteration 15, loss = 161744774.39164862\n",
      "Iteration 16, loss = 161614884.97031146\n",
      "Iteration 17, loss = 161567010.79765192\n",
      "Iteration 18, loss = 161631848.83240208\n",
      "Iteration 19, loss = 161391181.44630778\n",
      "Iteration 20, loss = 161358267.18112737\n",
      "Iteration 21, loss = 161321526.87409252\n",
      "Iteration 22, loss = 161178140.62830117\n",
      "Iteration 23, loss = 161349846.70303348\n",
      "Iteration 24, loss = 161095969.73431084\n",
      "Iteration 25, loss = 160941250.87399623\n",
      "Iteration 26, loss = 160959266.32819220\n",
      "Iteration 27, loss = 160810680.09972641\n",
      "Iteration 28, loss = 160851271.26507363\n",
      "Iteration 29, loss = 160749491.74994954\n",
      "Iteration 30, loss = 160681665.74931544\n",
      "Iteration 31, loss = 160605092.74952206\n",
      "Iteration 32, loss = 160433928.90866676\n",
      "Iteration 33, loss = 160438269.00383690\n",
      "Iteration 34, loss = 160383049.51089445\n",
      "Iteration 35, loss = 160329268.10499620\n",
      "Iteration 36, loss = 160482987.00191072\n",
      "Iteration 37, loss = 160084784.19117853\n",
      "Iteration 38, loss = 159973337.52448305\n",
      "Iteration 39, loss = 160175628.22715476\n",
      "Iteration 40, loss = 160046301.53964800\n",
      "Iteration 41, loss = 160015781.49988204\n",
      "Iteration 42, loss = 159764009.94132513\n",
      "Iteration 43, loss = 159628008.37852430\n",
      "Iteration 44, loss = 159699420.95486411\n",
      "Iteration 45, loss = 159814993.17648125\n",
      "Iteration 46, loss = 159614726.47968820\n",
      "Iteration 47, loss = 159366175.06163591\n",
      "Iteration 48, loss = 159436127.68764913\n",
      "Iteration 49, loss = 159349116.96124575\n",
      "Iteration 50, loss = 159273206.56561768\n",
      "Iteration 51, loss = 159123470.99331024\n",
      "Iteration 52, loss = 158952492.42931336\n",
      "Iteration 53, loss = 159020070.68125394\n",
      "Iteration 54, loss = 158918631.35462019\n",
      "Iteration 55, loss = 158900675.86690223\n",
      "Iteration 56, loss = 158865293.48129088\n",
      "Iteration 57, loss = 158818788.67951852\n",
      "Iteration 58, loss = 158664806.60903850\n",
      "Iteration 59, loss = 158542856.82969308\n",
      "Iteration 60, loss = 158450001.65356600\n",
      "Iteration 61, loss = 158391959.86910519\n",
      "Iteration 62, loss = 158303208.83043724\n",
      "Iteration 63, loss = 158203106.67129958\n",
      "Iteration 64, loss = 158149933.25781679\n",
      "Iteration 65, loss = 158294306.77517891\n",
      "Iteration 66, loss = 158105738.35905761\n",
      "Iteration 67, loss = 157810275.57513553\n",
      "Iteration 68, loss = 158081262.63703996\n",
      "Iteration 69, loss = 157914812.28229532\n",
      "Iteration 70, loss = 157647434.89096665\n",
      "Iteration 71, loss = 157633096.90610510\n",
      "Iteration 72, loss = 157767906.11461723\n",
      "Iteration 73, loss = 157612790.80196625\n",
      "Iteration 74, loss = 157735505.73209590\n",
      "Iteration 75, loss = 157884334.86557147\n",
      "Iteration 76, loss = 157211546.14012694\n",
      "Iteration 77, loss = 157405098.13483685\n",
      "Iteration 78, loss = 157110086.24616346\n",
      "Iteration 79, loss = 157314574.45778954\n",
      "Iteration 80, loss = 156927984.41584104\n",
      "Iteration 81, loss = 156934708.72235388\n",
      "Iteration 82, loss = 157204213.86881483\n",
      "Iteration 83, loss = 157183722.72444168\n",
      "Iteration 84, loss = 156754248.18355817\n",
      "Iteration 85, loss = 156861585.43014583\n",
      "Iteration 86, loss = 156748097.20778736\n",
      "Iteration 87, loss = 156646003.77576256\n",
      "Iteration 88, loss = 156577509.93390614\n",
      "Iteration 89, loss = 156578304.83119437\n",
      "Iteration 90, loss = 156687752.81029174\n",
      "Iteration 91, loss = 156355595.36459321\n",
      "Iteration 92, loss = 156303708.76250389\n",
      "Iteration 93, loss = 156344200.17462224\n",
      "Iteration 94, loss = 156340838.11272871\n",
      "Iteration 95, loss = 156178219.17420673\n",
      "Iteration 96, loss = 156115653.17974484\n",
      "Iteration 97, loss = 156063053.42410433\n",
      "Iteration 98, loss = 155931311.78888434\n",
      "Iteration 99, loss = 156464553.02564666\n",
      "Iteration 100, loss = 156100709.96535265\n",
      "Iteration 101, loss = 155972893.29978794\n",
      "Iteration 102, loss = 155727388.02833033\n",
      "Iteration 103, loss = 155700289.99203005\n",
      "Iteration 104, loss = 155512224.97810784\n",
      "Iteration 105, loss = 155820182.53805360\n",
      "Iteration 106, loss = 155604840.35021710\n",
      "Iteration 107, loss = 155583929.21479899\n",
      "Iteration 108, loss = 155397159.73761189\n",
      "Iteration 109, loss = 155691509.52550358\n",
      "Iteration 110, loss = 155246211.16138887\n",
      "Iteration 111, loss = 155359427.99810094\n",
      "Iteration 112, loss = 155251707.73869908\n",
      "Iteration 113, loss = 155149353.38954788\n",
      "Iteration 114, loss = 155214340.68211818\n",
      "Iteration 115, loss = 155234398.17344087\n",
      "Iteration 116, loss = 155103060.49579257\n",
      "Iteration 117, loss = 155258729.57219642\n",
      "Iteration 118, loss = 155249729.99585146\n",
      "Iteration 119, loss = 155124565.27265939\n",
      "Iteration 120, loss = 154943969.61267632\n",
      "Iteration 121, loss = 154874937.08262923\n",
      "Iteration 122, loss = 154871450.58760121\n",
      "Iteration 123, loss = 154775517.86359081\n",
      "Iteration 124, loss = 154955923.50285828\n",
      "Iteration 125, loss = 155080929.65432414\n",
      "Iteration 126, loss = 155109948.57471871\n",
      "Iteration 127, loss = 154963272.84954721\n",
      "Iteration 128, loss = 154453894.29875171\n",
      "Iteration 129, loss = 155143647.40672216\n",
      "Iteration 130, loss = 154469911.79653615\n",
      "Iteration 131, loss = 154544660.06089771\n",
      "Iteration 132, loss = 154444259.93398961\n",
      "Iteration 133, loss = 154408212.81783119\n",
      "Iteration 134, loss = 154440402.39065731\n",
      "Iteration 135, loss = 154415034.38595048\n",
      "Iteration 136, loss = 154486200.64643955\n",
      "Iteration 137, loss = 154645174.40232828\n",
      "Iteration 138, loss = 154367941.41225418\n",
      "Iteration 139, loss = 154122416.80208802\n",
      "Iteration 140, loss = 154610665.53188413\n",
      "Iteration 141, loss = 154151713.43201217\n",
      "Iteration 142, loss = 154332869.05045906\n",
      "Iteration 143, loss = 154462169.28833395\n",
      "Iteration 144, loss = 154305077.82795221\n",
      "Iteration 145, loss = 154008768.61096886\n",
      "Iteration 146, loss = 153989740.26807079\n",
      "Iteration 147, loss = 154092474.77936676\n",
      "Iteration 148, loss = 154172284.74567980\n",
      "Iteration 149, loss = 154125596.79887748\n",
      "Iteration 150, loss = 153947202.16255876\n",
      "Iteration 151, loss = 153804707.30890843\n",
      "Iteration 152, loss = 154045450.58129686\n",
      "Iteration 153, loss = 153928049.94050437\n",
      "Iteration 154, loss = 153990941.47097450\n",
      "Iteration 155, loss = 153796609.95232809\n",
      "Iteration 156, loss = 153783276.39683488\n",
      "Iteration 157, loss = 154005439.57015485\n",
      "Iteration 158, loss = 153887232.83898979\n",
      "Iteration 159, loss = 153975017.80125767\n",
      "Iteration 160, loss = 154103833.11448532\n",
      "Iteration 161, loss = 153707660.91115153\n",
      "Iteration 162, loss = 153876171.85559094\n",
      "Iteration 163, loss = 153654766.78280532\n",
      "Iteration 164, loss = 153610253.36199790\n",
      "Iteration 165, loss = 153637567.86964071\n",
      "Iteration 166, loss = 153657896.14846444\n",
      "Iteration 167, loss = 153508537.12901798\n",
      "Iteration 168, loss = 153797051.32114047\n",
      "Iteration 169, loss = 153540242.07113552\n",
      "Iteration 170, loss = 153763077.88121751\n",
      "Iteration 171, loss = 153358696.09974143\n",
      "Iteration 172, loss = 153749760.75038114\n",
      "Iteration 173, loss = 153508083.72039136\n",
      "Iteration 174, loss = 153601107.41348010\n",
      "Iteration 175, loss = 153350523.51073131\n",
      "Iteration 176, loss = 153363078.59073985\n",
      "Iteration 177, loss = 153344430.62069547\n",
      "Iteration 178, loss = 153379391.51320103\n",
      "Iteration 179, loss = 153389299.95487314\n",
      "Iteration 180, loss = 153444404.20134428\n",
      "Iteration 181, loss = 153333942.20837992\n",
      "Iteration 182, loss = 153705421.85567775\n",
      "Iteration 183, loss = 153195792.43925807\n",
      "Iteration 184, loss = 153401894.33765045\n",
      "Iteration 185, loss = 153344780.29396927\n",
      "Iteration 186, loss = 153137162.29368719\n",
      "Iteration 187, loss = 153364266.92242563\n",
      "Iteration 188, loss = 153410572.58643577\n",
      "Iteration 189, loss = 153162789.26262391\n",
      "Iteration 190, loss = 153191324.43092287\n",
      "Iteration 191, loss = 153321683.88305798\n",
      "Iteration 192, loss = 153125236.12942937\n",
      "Iteration 193, loss = 153275325.84400839\n",
      "Iteration 194, loss = 153352147.76973224\n",
      "Iteration 195, loss = 153176025.57234213\n",
      "Iteration 196, loss = 153367741.72779980\n",
      "Iteration 197, loss = 152863273.97372210\n",
      "Iteration 198, loss = 152969399.87683204\n",
      "Iteration 199, loss = 152835487.94081289\n",
      "Iteration 200, loss = 152983433.50987577\n",
      "Iteration 1, loss = 641429328.15652609\n",
      "Iteration 2, loss = 240572105.83085996\n",
      "Iteration 3, loss = 199493354.37095103\n",
      "Iteration 4, loss = 197965487.27773136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 197046165.52115002\n",
      "Iteration 6, loss = 196033021.55970863\n",
      "Iteration 7, loss = 194936888.84516174\n",
      "Iteration 8, loss = 193832616.23149705\n",
      "Iteration 9, loss = 193041090.32531357\n",
      "Iteration 10, loss = 192532845.67358911\n",
      "Iteration 11, loss = 192089481.62782013\n",
      "Iteration 12, loss = 191843163.59680223\n",
      "Iteration 13, loss = 191606363.52336189\n",
      "Iteration 14, loss = 191426830.86348632\n",
      "Iteration 15, loss = 191221530.27816847\n",
      "Iteration 16, loss = 191106180.90947601\n",
      "Iteration 17, loss = 191060199.67582899\n",
      "Iteration 18, loss = 190805882.75371632\n",
      "Iteration 19, loss = 190644836.34251088\n",
      "Iteration 20, loss = 190617911.09375083\n",
      "Iteration 21, loss = 190438780.05577141\n",
      "Iteration 22, loss = 190392804.22049549\n",
      "Iteration 23, loss = 190094694.46168122\n",
      "Iteration 24, loss = 190065535.16587120\n",
      "Iteration 25, loss = 189945813.28202850\n",
      "Iteration 26, loss = 189737421.75381058\n",
      "Iteration 27, loss = 189588330.71113357\n",
      "Iteration 28, loss = 189570015.86867914\n",
      "Iteration 29, loss = 189397535.97703072\n",
      "Iteration 30, loss = 189238408.55818108\n",
      "Iteration 31, loss = 189083891.31944078\n",
      "Iteration 32, loss = 188944428.11751673\n",
      "Iteration 33, loss = 188688402.68197095\n",
      "Iteration 34, loss = 188684313.78860381\n",
      "Iteration 35, loss = 188500216.97383249\n",
      "Iteration 36, loss = 188363759.18193844\n",
      "Iteration 37, loss = 188167752.93095034\n",
      "Iteration 38, loss = 188135179.42089254\n",
      "Iteration 39, loss = 188039477.96451280\n",
      "Iteration 40, loss = 187768051.41752633\n",
      "Iteration 41, loss = 187739230.41562408\n",
      "Iteration 42, loss = 187345426.81782281\n",
      "Iteration 43, loss = 187383926.89001903\n",
      "Iteration 44, loss = 187159832.32341594\n",
      "Iteration 45, loss = 186966228.69247469\n",
      "Iteration 46, loss = 186850409.43260971\n",
      "Iteration 47, loss = 186923251.61890385\n",
      "Iteration 48, loss = 186627413.04193583\n",
      "Iteration 49, loss = 186334334.81092110\n",
      "Iteration 50, loss = 186235191.64985099\n",
      "Iteration 51, loss = 186104554.71256095\n",
      "Iteration 52, loss = 185912823.88539830\n",
      "Iteration 53, loss = 185853617.15163365\n",
      "Iteration 54, loss = 185619467.65940630\n",
      "Iteration 55, loss = 185563862.65485632\n",
      "Iteration 56, loss = 185291776.06767875\n",
      "Iteration 57, loss = 185178884.33848694\n",
      "Iteration 58, loss = 185119871.27771688\n",
      "Iteration 59, loss = 184810004.65814066\n",
      "Iteration 60, loss = 184728120.70318058\n",
      "Iteration 61, loss = 184723842.53510630\n",
      "Iteration 62, loss = 184483601.68295768\n",
      "Iteration 63, loss = 184461038.17821774\n",
      "Iteration 64, loss = 184232818.10113207\n",
      "Iteration 65, loss = 183874032.14228323\n",
      "Iteration 66, loss = 183846996.61173934\n",
      "Iteration 67, loss = 183600159.02475524\n",
      "Iteration 68, loss = 183599713.90147948\n",
      "Iteration 69, loss = 183377761.17082447\n",
      "Iteration 70, loss = 183418997.05481929\n",
      "Iteration 71, loss = 183155506.10071218\n",
      "Iteration 72, loss = 183015553.39217159\n",
      "Iteration 73, loss = 182982355.75249115\n",
      "Iteration 74, loss = 182676721.70021981\n",
      "Iteration 75, loss = 182863959.31411344\n",
      "Iteration 76, loss = 182439834.31626374\n",
      "Iteration 77, loss = 182368855.59798947\n",
      "Iteration 78, loss = 182141054.40899029\n",
      "Iteration 79, loss = 182082666.12054211\n",
      "Iteration 80, loss = 182437424.16054219\n",
      "Iteration 81, loss = 182098517.25914019\n",
      "Iteration 82, loss = 181620173.26979640\n",
      "Iteration 83, loss = 181505985.22897780\n",
      "Iteration 84, loss = 181430262.03356445\n",
      "Iteration 85, loss = 181376382.97424468\n",
      "Iteration 86, loss = 181437987.12705857\n",
      "Iteration 87, loss = 181177452.81735212\n",
      "Iteration 88, loss = 181417457.93103251\n",
      "Iteration 89, loss = 180992133.75996074\n",
      "Iteration 90, loss = 180873020.47715846\n",
      "Iteration 91, loss = 180782038.10998568\n",
      "Iteration 92, loss = 180649428.22262198\n",
      "Iteration 93, loss = 180723400.00978175\n",
      "Iteration 94, loss = 180520701.16410133\n",
      "Iteration 95, loss = 180313693.11743709\n",
      "Iteration 96, loss = 180152751.40178639\n",
      "Iteration 97, loss = 180423377.41689593\n",
      "Iteration 98, loss = 180153395.12544364\n",
      "Iteration 99, loss = 180082800.24582255\n",
      "Iteration 100, loss = 179770428.87559694\n",
      "Iteration 101, loss = 180495299.95261937\n",
      "Iteration 102, loss = 179751607.63069230\n",
      "Iteration 103, loss = 179684653.79150245\n",
      "Iteration 104, loss = 179577614.93552861\n",
      "Iteration 105, loss = 179949354.63035819\n",
      "Iteration 106, loss = 179340993.37325841\n",
      "Iteration 107, loss = 179214594.17805338\n",
      "Iteration 108, loss = 179353495.26183826\n",
      "Iteration 109, loss = 179021942.85936251\n",
      "Iteration 110, loss = 179281184.21968368\n",
      "Iteration 111, loss = 179052793.05976591\n",
      "Iteration 112, loss = 179024955.69005349\n",
      "Iteration 113, loss = 179003986.02696043\n",
      "Iteration 114, loss = 179314732.59609684\n",
      "Iteration 115, loss = 179139027.38232848\n",
      "Iteration 116, loss = 178738533.76777497\n",
      "Iteration 117, loss = 178653775.39183602\n",
      "Iteration 118, loss = 178504500.72694904\n",
      "Iteration 119, loss = 178476310.84812942\n",
      "Iteration 120, loss = 178146132.89565358\n",
      "Iteration 121, loss = 178664707.29982245\n",
      "Iteration 122, loss = 178221177.68322352\n",
      "Iteration 123, loss = 178155328.87536737\n",
      "Iteration 124, loss = 178057488.56414410\n",
      "Iteration 125, loss = 178037942.40009660\n",
      "Iteration 126, loss = 177935636.67449293\n",
      "Iteration 127, loss = 177698684.60976583\n",
      "Iteration 128, loss = 177632353.96704030\n",
      "Iteration 129, loss = 177576656.82695687\n",
      "Iteration 130, loss = 177373990.04161245\n",
      "Iteration 131, loss = 177401824.47279564\n",
      "Iteration 132, loss = 176897199.17888495\n",
      "Iteration 133, loss = 177245581.89399186\n",
      "Iteration 134, loss = 176976197.43277779\n",
      "Iteration 135, loss = 177724597.51557687\n",
      "Iteration 136, loss = 177077260.61440632\n",
      "Iteration 137, loss = 176889720.64112493\n",
      "Iteration 138, loss = 176412081.34714401\n",
      "Iteration 139, loss = 176498553.58851373\n",
      "Iteration 140, loss = 176577138.02048439\n",
      "Iteration 141, loss = 176572693.88142824\n",
      "Iteration 142, loss = 176323756.34043789\n",
      "Iteration 143, loss = 176623031.74338210\n",
      "Iteration 144, loss = 176179454.48387074\n",
      "Iteration 145, loss = 176113501.07173988\n",
      "Iteration 146, loss = 176107863.95637569\n",
      "Iteration 147, loss = 175964866.32759508\n",
      "Iteration 148, loss = 176354707.71249926\n",
      "Iteration 149, loss = 175741460.19415534\n",
      "Iteration 150, loss = 175882175.99725401\n",
      "Iteration 151, loss = 175590554.28061864\n",
      "Iteration 152, loss = 175428648.91554102\n",
      "Iteration 153, loss = 175512633.49588835\n",
      "Iteration 154, loss = 175660774.57950485\n",
      "Iteration 155, loss = 176055626.48317504\n",
      "Iteration 156, loss = 176169125.22547287\n",
      "Iteration 157, loss = 175096964.87260917\n",
      "Iteration 158, loss = 175235145.57253197\n",
      "Iteration 159, loss = 175148430.06482533\n",
      "Iteration 160, loss = 175299406.12834722\n",
      "Iteration 161, loss = 175010845.99279138\n",
      "Iteration 162, loss = 174904586.32336149\n",
      "Iteration 163, loss = 175191893.10905543\n",
      "Iteration 164, loss = 174888442.24263486\n",
      "Iteration 165, loss = 174960167.03319591\n",
      "Iteration 166, loss = 174600380.61888734\n",
      "Iteration 167, loss = 174857092.94127718\n",
      "Iteration 168, loss = 174987142.52725902\n",
      "Iteration 169, loss = 174647020.55198729\n",
      "Iteration 170, loss = 174291742.89296728\n",
      "Iteration 171, loss = 174400622.61515650\n",
      "Iteration 172, loss = 174066395.14426276\n",
      "Iteration 173, loss = 174315011.40948600\n",
      "Iteration 174, loss = 174165663.97471637\n",
      "Iteration 175, loss = 174309247.71529263\n",
      "Iteration 176, loss = 174105773.60938874\n",
      "Iteration 177, loss = 173679678.71608910\n",
      "Iteration 178, loss = 174016569.98269877\n",
      "Iteration 179, loss = 173610644.14385521\n",
      "Iteration 180, loss = 173962993.01864591\n",
      "Iteration 181, loss = 173996631.97491005\n",
      "Iteration 182, loss = 173305917.33136937\n",
      "Iteration 183, loss = 173375479.21715426\n",
      "Iteration 184, loss = 173466363.32933468\n",
      "Iteration 185, loss = 173303655.64306432\n",
      "Iteration 186, loss = 173027428.91683391\n",
      "Iteration 187, loss = 173050635.45145842\n",
      "Iteration 188, loss = 172857256.43109947\n",
      "Iteration 189, loss = 172983612.80886358\n",
      "Iteration 190, loss = 172850147.86070144\n",
      "Iteration 191, loss = 172913655.23213086\n",
      "Iteration 192, loss = 172800808.16163275\n",
      "Iteration 193, loss = 173240647.74926198\n",
      "Iteration 194, loss = 172371227.09707588\n",
      "Iteration 195, loss = 172474669.22764593\n",
      "Iteration 196, loss = 172615141.04842162\n",
      "Iteration 197, loss = 172443814.06915048\n",
      "Iteration 198, loss = 172109655.57415745\n",
      "Iteration 199, loss = 172908053.93544272\n",
      "Iteration 200, loss = 172146536.82637200\n",
      "MAE:  14340.925695793241\n",
      "MSE:  568080805.9669673\n",
      "RMSE:  23834.445786864173\n",
      "R2:  0.048989860063557655\n",
      "0 14340.925695793241\n",
      "Find best model\n",
      "(100524, 15) (26592, 15)\n",
      "Iteration 1, loss = 412894676.77212185\n",
      "Iteration 2, loss = 407047984.48833597\n",
      "Iteration 3, loss = 404124576.62672108\n",
      "Iteration 4, loss = 404134561.59357524\n",
      "Iteration 5, loss = 403623488.84980851\n",
      "Iteration 6, loss = 403789678.97888356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 403643022.95956719\n",
      "Iteration 8, loss = 403511153.18705654\n",
      "Iteration 9, loss = 403531401.60968047\n",
      "Iteration 10, loss = 403642997.17122275\n",
      "Iteration 11, loss = 403421426.90670359\n",
      "Iteration 12, loss = 403428852.38809162\n",
      "Iteration 13, loss = 403248010.89444602\n",
      "Iteration 14, loss = 403517844.65723753\n",
      "Iteration 15, loss = 403361695.42869645\n",
      "Iteration 16, loss = 403255183.95481139\n",
      "Iteration 17, loss = 403428910.11628932\n",
      "Iteration 18, loss = 403400396.53342110\n",
      "Iteration 19, loss = 403660855.22516364\n",
      "Iteration 20, loss = 403287672.17678154\n",
      "Iteration 21, loss = 403518862.35539734\n",
      "Iteration 22, loss = 403173389.66617858\n",
      "Iteration 23, loss = 403186855.25293827\n",
      "Iteration 24, loss = 403447646.41402119\n",
      "Iteration 25, loss = 403576923.30927491\n",
      "Iteration 26, loss = 403180947.03779942\n",
      "Iteration 27, loss = 403156670.91814852\n",
      "Iteration 28, loss = 403187578.20111465\n",
      "Iteration 29, loss = 403130383.16679257\n",
      "Iteration 30, loss = 403269541.68027395\n",
      "Iteration 31, loss = 403576108.85213685\n",
      "Iteration 32, loss = 403319957.08537436\n",
      "Iteration 33, loss = 403444903.72276825\n",
      "Iteration 34, loss = 403061573.22693092\n",
      "Iteration 35, loss = 403074515.07741255\n",
      "Iteration 36, loss = 403403855.51353186\n",
      "Iteration 37, loss = 403162118.77872932\n",
      "Iteration 38, loss = 402977738.56603992\n",
      "Iteration 39, loss = 403238926.59707582\n",
      "Iteration 40, loss = 403172750.63199139\n",
      "Iteration 41, loss = 403035897.23500437\n",
      "Iteration 42, loss = 403119316.48852968\n",
      "Iteration 43, loss = 403004794.15647292\n",
      "Iteration 44, loss = 402878143.70728129\n",
      "Iteration 45, loss = 403345142.80690843\n",
      "Iteration 46, loss = 403034660.05063599\n",
      "Iteration 47, loss = 402797417.98017317\n",
      "Iteration 48, loss = 402903566.16381818\n",
      "Iteration 49, loss = 403142896.34336585\n",
      "Iteration 50, loss = 402809104.30452889\n",
      "Iteration 51, loss = 403084401.59920490\n",
      "Iteration 52, loss = 402913267.91541809\n",
      "Iteration 53, loss = 402858942.62656730\n",
      "Iteration 54, loss = 402933021.74449140\n",
      "Iteration 55, loss = 403097275.02783674\n",
      "Iteration 56, loss = 402815198.62054241\n",
      "Iteration 57, loss = 402864190.29053342\n",
      "Iteration 58, loss = 402970798.19052690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 336294753.71091926\n",
      "Iteration 2, loss = 166482174.54011014\n",
      "Iteration 3, loss = 133567631.82783113\n",
      "Iteration 4, loss = 128408993.52356148\n",
      "Iteration 5, loss = 127647162.25398755\n",
      "Iteration 6, loss = 127435339.57338472\n",
      "Iteration 7, loss = 127312425.74298903\n",
      "Iteration 8, loss = 127225816.98906289\n",
      "Iteration 9, loss = 127134095.64489329\n",
      "Iteration 10, loss = 127031895.35463984\n",
      "Iteration 11, loss = 126949526.62617415\n",
      "Iteration 12, loss = 126873204.49632239\n",
      "Iteration 13, loss = 126797500.53955932\n",
      "Iteration 14, loss = 126746818.28642437\n",
      "Iteration 15, loss = 126675470.54774950\n",
      "Iteration 16, loss = 126640379.03546357\n",
      "Iteration 17, loss = 126593058.86283469\n",
      "Iteration 18, loss = 126544269.26116629\n",
      "Iteration 19, loss = 126500637.90408030\n",
      "Iteration 20, loss = 126471187.21924810\n",
      "Iteration 21, loss = 126440369.22222717\n",
      "Iteration 22, loss = 126446657.90892802\n",
      "Iteration 23, loss = 126420295.44812168\n",
      "Iteration 24, loss = 126404657.78504016\n",
      "Iteration 25, loss = 126365502.12030947\n",
      "Iteration 26, loss = 126353240.42272983\n",
      "Iteration 27, loss = 126359874.81251016\n",
      "Iteration 28, loss = 126336500.25215478\n",
      "Iteration 29, loss = 126339593.30327329\n",
      "Iteration 30, loss = 126318583.78090237\n",
      "Iteration 31, loss = 126357342.45775831\n",
      "Iteration 32, loss = 126330144.10762151\n",
      "Iteration 33, loss = 126372823.40288447\n",
      "Iteration 34, loss = 126338481.75512956\n",
      "Iteration 35, loss = 126356990.90209270\n",
      "Iteration 36, loss = 126312309.17482747\n",
      "Iteration 37, loss = 126326439.88531058\n",
      "Iteration 38, loss = 126290498.62223001\n",
      "Iteration 39, loss = 126296210.18364690\n",
      "Iteration 40, loss = 126310375.67940515\n",
      "Iteration 41, loss = 126383825.16377886\n",
      "Iteration 42, loss = 126336869.45038684\n",
      "Iteration 43, loss = 126290485.35313499\n",
      "Iteration 44, loss = 126299871.95391673\n",
      "Iteration 45, loss = 126342757.12813915\n",
      "Iteration 46, loss = 126299293.93487473\n",
      "Iteration 47, loss = 126302018.40967178\n",
      "Iteration 48, loss = 126292598.94710888\n",
      "Iteration 49, loss = 126292815.60855635\n",
      "Iteration 50, loss = 126284593.69163375\n",
      "Iteration 51, loss = 126296471.97392698\n",
      "Iteration 52, loss = 126279851.41616450\n",
      "Iteration 53, loss = 126334121.94992857\n",
      "Iteration 54, loss = 126252406.92273457\n",
      "Iteration 55, loss = 126250447.77421512\n",
      "Iteration 56, loss = 126280327.86826909\n",
      "Iteration 57, loss = 126282559.62633879\n",
      "Iteration 58, loss = 126305465.14896800\n",
      "Iteration 59, loss = 126342094.01891275\n",
      "Iteration 60, loss = 126312460.13392219\n",
      "Iteration 61, loss = 126269668.38168193\n",
      "Iteration 62, loss = 126293733.68319575\n",
      "Iteration 63, loss = 126265558.48147236\n",
      "Iteration 64, loss = 126242884.20814148\n",
      "Iteration 65, loss = 126307575.17040344\n",
      "Iteration 66, loss = 126301800.31315452\n",
      "Iteration 67, loss = 126248434.28538226\n",
      "Iteration 68, loss = 126220570.34189478\n",
      "Iteration 69, loss = 126260482.88212642\n",
      "Iteration 70, loss = 126283852.16753271\n",
      "Iteration 71, loss = 126240343.28392969\n",
      "Iteration 72, loss = 126288097.88502973\n",
      "Iteration 73, loss = 126230143.04683486\n",
      "Iteration 74, loss = 126223270.68025073\n",
      "Iteration 75, loss = 126345783.72672194\n",
      "Iteration 76, loss = 126264207.48616719\n",
      "Iteration 77, loss = 126251284.15344660\n",
      "Iteration 78, loss = 126342968.64439949\n",
      "Iteration 79, loss = 126254809.77458867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1975621362.41050148\n",
      "Iteration 2, loss = 937556004.28177750\n",
      "Iteration 3, loss = 477514094.47429806\n",
      "Iteration 4, loss = 320259227.03980881\n",
      "Iteration 5, loss = 282316799.51550174\n",
      "Iteration 6, loss = 274876054.90748650\n",
      "Iteration 7, loss = 272111437.71100372\n",
      "Iteration 8, loss = 269901255.02898264\n",
      "Iteration 9, loss = 267803049.16213945\n",
      "Iteration 10, loss = 265864826.80266491\n",
      "Iteration 11, loss = 264130755.32133994\n",
      "Iteration 12, loss = 262527739.95559874\n",
      "Iteration 13, loss = 261112407.04117677\n",
      "Iteration 14, loss = 259860307.16245449\n",
      "Iteration 15, loss = 257691669.44977564\n",
      "Iteration 16, loss = 254697498.48418111\n",
      "Iteration 17, loss = 253047109.58084437\n",
      "Iteration 18, loss = 252323320.55568585\n",
      "Iteration 19, loss = 252017969.15284371\n",
      "Iteration 20, loss = 251862966.12725687\n",
      "Iteration 21, loss = 251881044.12607723\n",
      "Iteration 22, loss = 251821826.08438399\n",
      "Iteration 23, loss = 251855000.80327237\n",
      "Iteration 24, loss = 251800392.67067578\n",
      "Iteration 25, loss = 251741538.76253530\n",
      "Iteration 26, loss = 251836047.59027290\n",
      "Iteration 27, loss = 251790157.90121660\n",
      "Iteration 28, loss = 251743619.80572584\n",
      "Iteration 29, loss = 251744448.72133514\n",
      "Iteration 30, loss = 251753303.86488110\n",
      "Iteration 31, loss = 251777949.23669097\n",
      "Iteration 32, loss = 251893734.12941197\n",
      "Iteration 33, loss = 251721305.79058170\n",
      "Iteration 34, loss = 251684786.52621758\n",
      "Iteration 35, loss = 251710746.03518936\n",
      "Iteration 36, loss = 251682640.24487841\n",
      "Iteration 37, loss = 251748677.90491188\n",
      "Iteration 38, loss = 251686431.72721115\n",
      "Iteration 39, loss = 251662882.77706030\n",
      "Iteration 40, loss = 251639484.11674589\n",
      "Iteration 41, loss = 251681498.27954522\n",
      "Iteration 42, loss = 251709412.12660092\n",
      "Iteration 43, loss = 251661348.82325396\n",
      "Iteration 44, loss = 251635709.59724903\n",
      "Iteration 45, loss = 251708359.92799765\n",
      "Iteration 46, loss = 251707974.57245529\n",
      "Iteration 47, loss = 251798886.81836310\n",
      "Iteration 48, loss = 251722414.95619926\n",
      "Iteration 49, loss = 251670449.01625678\n",
      "Iteration 50, loss = 251629682.73767504\n",
      "Iteration 51, loss = 251626186.33957848\n",
      "Iteration 52, loss = 251587061.01368764\n",
      "Iteration 53, loss = 251636952.60245684\n",
      "Iteration 54, loss = 251570734.17295840\n",
      "Iteration 55, loss = 251636476.09477752\n",
      "Iteration 56, loss = 251617283.23682374\n",
      "Iteration 57, loss = 251610088.96697339\n",
      "Iteration 58, loss = 251571004.81452206\n",
      "Iteration 59, loss = 251614867.66225004\n",
      "Iteration 60, loss = 251649016.05874479\n",
      "Iteration 61, loss = 251578114.24312687\n",
      "Iteration 62, loss = 251695057.51551738\n",
      "Iteration 63, loss = 251587888.71619815\n",
      "Iteration 64, loss = 251545160.19236866\n",
      "Iteration 65, loss = 251446334.35525465\n",
      "Iteration 66, loss = 251613143.63451484\n",
      "Iteration 67, loss = 251548701.06575391\n",
      "Iteration 68, loss = 251651324.99864835\n",
      "Iteration 69, loss = 251590880.84386459\n",
      "Iteration 70, loss = 251452645.50169203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71, loss = 251637518.58254707\n",
      "Iteration 72, loss = 251513533.98185560\n",
      "Iteration 73, loss = 251595900.09256381\n",
      "Iteration 74, loss = 251461116.17639419\n",
      "Iteration 75, loss = 251444750.13559169\n",
      "Iteration 76, loss = 251470000.08635625\n",
      "Iteration 77, loss = 251493267.28992236\n",
      "Iteration 78, loss = 251455796.46227860\n",
      "Iteration 79, loss = 251527748.25013179\n",
      "Iteration 80, loss = 251478433.23601744\n",
      "Iteration 81, loss = 251583400.24656168\n",
      "Iteration 82, loss = 251453978.08127904\n",
      "Iteration 83, loss = 251423328.29471949\n",
      "Iteration 84, loss = 251378325.90272051\n",
      "Iteration 85, loss = 251397620.90487227\n",
      "Iteration 86, loss = 251482034.80463815\n",
      "Iteration 87, loss = 251455619.64817593\n",
      "Iteration 88, loss = 251551711.26808524\n",
      "Iteration 89, loss = 251314680.34294766\n",
      "Iteration 90, loss = 251371776.50145981\n",
      "Iteration 91, loss = 251465105.17645812\n",
      "Iteration 92, loss = 251343199.35985917\n",
      "Iteration 93, loss = 251401157.50728130\n",
      "Iteration 94, loss = 251368669.13992935\n",
      "Iteration 95, loss = 251330440.68039393\n",
      "Iteration 96, loss = 251426838.59274584\n",
      "Iteration 97, loss = 251273387.09630889\n",
      "Iteration 98, loss = 251623405.30441225\n",
      "Iteration 99, loss = 251439347.55359185\n",
      "Iteration 100, loss = 251636677.67014083\n",
      "Iteration 101, loss = 251378685.29889917\n",
      "Iteration 102, loss = 251723780.98251167\n",
      "Iteration 103, loss = 251330262.19820097\n",
      "Iteration 104, loss = 251403474.16727203\n",
      "Iteration 105, loss = 251812504.36533305\n",
      "Iteration 106, loss = 251449796.05618897\n",
      "Iteration 107, loss = 251491065.12901148\n",
      "Iteration 108, loss = 251438638.59039307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 458165530.72419041\n",
      "Iteration 2, loss = 286088054.78911078\n",
      "Iteration 3, loss = 282412122.73492879\n",
      "Iteration 4, loss = 282286379.17770952\n",
      "Iteration 5, loss = 282132829.08204669\n",
      "Iteration 6, loss = 282054048.90971875\n",
      "Iteration 7, loss = 281911968.41899139\n",
      "Iteration 8, loss = 281920354.35946506\n",
      "Iteration 9, loss = 281750347.10457081\n",
      "Iteration 10, loss = 281989364.56485838\n",
      "Iteration 11, loss = 281602715.82315600\n",
      "Iteration 12, loss = 281477984.14233363\n",
      "Iteration 13, loss = 281775031.56399596\n",
      "Iteration 14, loss = 281483813.27871513\n",
      "Iteration 15, loss = 281411176.76043332\n",
      "Iteration 16, loss = 281379391.85234690\n",
      "Iteration 17, loss = 281343769.75896692\n",
      "Iteration 18, loss = 281518417.93059951\n",
      "Iteration 19, loss = 281285268.96481854\n",
      "Iteration 20, loss = 281277847.57521480\n",
      "Iteration 21, loss = 281406646.11348820\n",
      "Iteration 22, loss = 281156694.01686937\n",
      "Iteration 23, loss = 281382765.04486418\n",
      "Iteration 24, loss = 281477357.49737185\n",
      "Iteration 25, loss = 281370220.28875822\n",
      "Iteration 26, loss = 281388667.52392399\n",
      "Iteration 27, loss = 281268316.77134883\n",
      "Iteration 28, loss = 281170291.36694366\n",
      "Iteration 29, loss = 281202105.32492256\n",
      "Iteration 30, loss = 281101308.97247595\n",
      "Iteration 31, loss = 281152934.86419421\n",
      "Iteration 32, loss = 281167785.47091150\n",
      "Iteration 33, loss = 281131803.03410172\n",
      "Iteration 34, loss = 281394002.10139072\n",
      "Iteration 35, loss = 281153885.99684596\n",
      "Iteration 36, loss = 281054382.67923617\n",
      "Iteration 37, loss = 281118517.50756389\n",
      "Iteration 38, loss = 280989649.71207333\n",
      "Iteration 39, loss = 281619169.18596607\n",
      "Iteration 40, loss = 281083209.78988308\n",
      "Iteration 41, loss = 281117321.12678772\n",
      "Iteration 42, loss = 281140405.69030094\n",
      "Iteration 43, loss = 281109237.65777552\n",
      "Iteration 44, loss = 280989001.92299783\n",
      "Iteration 45, loss = 281146678.24957180\n",
      "Iteration 46, loss = 281231841.55297661\n",
      "Iteration 47, loss = 281202442.93136472\n",
      "Iteration 48, loss = 280989536.09611511\n",
      "Iteration 49, loss = 280892024.42062908\n",
      "Iteration 50, loss = 281127160.05201888\n",
      "Iteration 51, loss = 280890466.77844083\n",
      "Iteration 52, loss = 281025838.32384545\n",
      "Iteration 53, loss = 280797443.09917605\n",
      "Iteration 54, loss = 281222099.18685818\n",
      "Iteration 55, loss = 281084951.52555197\n",
      "Iteration 56, loss = 280847210.71811575\n",
      "Iteration 57, loss = 281152429.62506157\n",
      "Iteration 58, loss = 280928176.77083170\n",
      "Iteration 59, loss = 281074625.68574244\n",
      "Iteration 60, loss = 281025634.23524731\n",
      "Iteration 61, loss = 280960199.96162224\n",
      "Iteration 62, loss = 281096392.12271035\n",
      "Iteration 63, loss = 281204730.31235617\n",
      "Iteration 64, loss = 280783069.18346739\n",
      "Iteration 65, loss = 281145405.32726204\n",
      "Iteration 66, loss = 280793344.85370201\n",
      "Iteration 67, loss = 280798781.00092781\n",
      "Iteration 68, loss = 281002270.91311800\n",
      "Iteration 69, loss = 280760639.43126661\n",
      "Iteration 70, loss = 280655546.88491166\n",
      "Iteration 71, loss = 281155573.95474786\n",
      "Iteration 72, loss = 280681138.31523305\n",
      "Iteration 73, loss = 280670920.40196931\n",
      "Iteration 74, loss = 280984084.05716586\n",
      "Iteration 75, loss = 280954817.87673295\n",
      "Iteration 76, loss = 280924897.44190854\n",
      "Iteration 77, loss = 280664984.96851110\n",
      "Iteration 78, loss = 280702901.35570383\n",
      "Iteration 79, loss = 280931311.84211528\n",
      "Iteration 80, loss = 280622879.12452132\n",
      "Iteration 81, loss = 280575008.05033642\n",
      "Iteration 82, loss = 280643676.48183161\n",
      "Iteration 83, loss = 280702498.54671139\n",
      "Iteration 84, loss = 280797073.35718513\n",
      "Iteration 85, loss = 280689499.29997730\n",
      "Iteration 86, loss = 280690009.28662717\n",
      "Iteration 87, loss = 280745487.40163893\n",
      "Iteration 88, loss = 280677514.78618306\n",
      "Iteration 89, loss = 280760000.37682247\n",
      "Iteration 90, loss = 280627112.87419796\n",
      "Iteration 91, loss = 280507639.47252440\n",
      "Iteration 92, loss = 280435253.48985922\n",
      "Iteration 93, loss = 280444423.58258712\n",
      "Iteration 94, loss = 280329354.31104106\n",
      "Iteration 95, loss = 280471747.53103840\n",
      "Iteration 96, loss = 280326129.84923726\n",
      "Iteration 97, loss = 280503505.18763220\n",
      "Iteration 98, loss = 280287372.92441535\n",
      "Iteration 99, loss = 280771005.31739420\n",
      "Iteration 100, loss = 280316796.67402029\n",
      "Iteration 101, loss = 280395429.35402519\n",
      "Iteration 102, loss = 280276954.28781199\n",
      "Iteration 103, loss = 280527953.90822083\n",
      "Iteration 104, loss = 280463639.34320676\n",
      "Iteration 105, loss = 280267162.49226767\n",
      "Iteration 106, loss = 280513437.93143773\n",
      "Iteration 107, loss = 280655391.93518496\n",
      "Iteration 108, loss = 280498996.57810891\n",
      "Iteration 109, loss = 280456965.13628572\n",
      "Iteration 110, loss = 280797613.50567502\n",
      "Iteration 111, loss = 280305153.57966095\n",
      "Iteration 112, loss = 280253873.17489177\n",
      "Iteration 113, loss = 280227614.36604840\n",
      "Iteration 114, loss = 280378917.78352731\n",
      "Iteration 115, loss = 280078809.85649765\n",
      "Iteration 116, loss = 280470202.03438050\n",
      "Iteration 117, loss = 280316914.70146888\n",
      "Iteration 118, loss = 280107319.63396448\n",
      "Iteration 119, loss = 280315416.00292599\n",
      "Iteration 120, loss = 280117032.01433229\n",
      "Iteration 121, loss = 280087115.67112559\n",
      "Iteration 122, loss = 280291303.11795461\n",
      "Iteration 123, loss = 280103183.03135097\n",
      "Iteration 124, loss = 279997558.91324693\n",
      "Iteration 125, loss = 280419947.72045690\n",
      "Iteration 126, loss = 280246874.87325436\n",
      "Iteration 127, loss = 280293157.02772021\n",
      "Iteration 128, loss = 279887239.98464274\n",
      "Iteration 129, loss = 280661324.37184387\n",
      "Iteration 130, loss = 280363021.22669607\n",
      "Iteration 131, loss = 280032274.44410866\n",
      "Iteration 132, loss = 279964523.18797249\n",
      "Iteration 133, loss = 279978829.19651026\n",
      "Iteration 134, loss = 279957804.14057338\n",
      "Iteration 135, loss = 280027078.44721484\n",
      "Iteration 136, loss = 279841888.67963475\n",
      "Iteration 137, loss = 280061739.66191608\n",
      "Iteration 138, loss = 279864853.46053767\n",
      "Iteration 139, loss = 279970917.21226466\n",
      "Iteration 140, loss = 279943697.80876118\n",
      "Iteration 141, loss = 279922349.35207230\n",
      "Iteration 142, loss = 279990491.39290279\n",
      "Iteration 143, loss = 280265310.66238290\n",
      "Iteration 144, loss = 280037813.14380759\n",
      "Iteration 145, loss = 280050923.00202233\n",
      "Iteration 146, loss = 280045814.62343353\n",
      "Iteration 147, loss = 279967972.77461731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3389632764.42164707\n",
      "Iteration 2, loss = 1490817862.31930614\n",
      "Iteration 3, loss = 695289895.21188474\n",
      "Iteration 4, loss = 381873252.25651443\n",
      "Iteration 5, loss = 290210225.04032159\n",
      "Iteration 6, loss = 282103983.80076498\n",
      "Iteration 7, loss = 281046691.43621063\n",
      "Iteration 8, loss = 280324625.27427274\n",
      "Iteration 9, loss = 279842339.61236745\n",
      "Iteration 10, loss = 279664569.08816326\n",
      "Iteration 11, loss = 279494492.80435932\n",
      "Iteration 12, loss = 279385494.35250300\n",
      "Iteration 13, loss = 279298522.52394819\n",
      "Iteration 14, loss = 279323869.70384598\n",
      "Iteration 15, loss = 279274842.52437729\n",
      "Iteration 16, loss = 279278453.47749597\n",
      "Iteration 17, loss = 279267094.79519379\n",
      "Iteration 18, loss = 279284684.99583220\n",
      "Iteration 19, loss = 279248388.02851492\n",
      "Iteration 20, loss = 279219166.48515964\n",
      "Iteration 21, loss = 279248927.61828393\n",
      "Iteration 22, loss = 279291813.95785588\n",
      "Iteration 23, loss = 279242197.77382195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 279218984.20076454\n",
      "Iteration 25, loss = 279244292.38086897\n",
      "Iteration 26, loss = 279337895.60339427\n",
      "Iteration 27, loss = 279263470.22151119\n",
      "Iteration 28, loss = 279298371.58990496\n",
      "Iteration 29, loss = 279207376.45410317\n",
      "Iteration 30, loss = 279297049.62653518\n",
      "Iteration 31, loss = 279280819.87008440\n",
      "Iteration 32, loss = 279224850.40266472\n",
      "Iteration 33, loss = 279131132.28880084\n",
      "Iteration 34, loss = 279481995.45382768\n",
      "Iteration 35, loss = 279371262.45591837\n",
      "Iteration 36, loss = 279246255.13597918\n",
      "Iteration 37, loss = 279172530.41050172\n",
      "Iteration 38, loss = 279220748.60450661\n",
      "Iteration 39, loss = 279324832.17410499\n",
      "Iteration 40, loss = 279309899.75562322\n",
      "Iteration 41, loss = 279291845.00688964\n",
      "Iteration 42, loss = 279275447.52445173\n",
      "Iteration 43, loss = 279179422.63227099\n",
      "Iteration 44, loss = 279304036.29501718\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 596843401.80098510\n",
      "Iteration 2, loss = 238413200.41390640\n",
      "Iteration 3, loss = 197854840.58982673\n",
      "Iteration 4, loss = 190494272.93939921\n",
      "Iteration 5, loss = 187810283.64218566\n",
      "Iteration 6, loss = 186698968.33072820\n",
      "Iteration 7, loss = 186215748.07610983\n",
      "Iteration 8, loss = 186057201.33553648\n",
      "Iteration 9, loss = 185961768.58623207\n",
      "Iteration 10, loss = 185836985.12346193\n",
      "Iteration 11, loss = 185748651.75906616\n",
      "Iteration 12, loss = 185850400.70251492\n",
      "Iteration 13, loss = 185673630.64865628\n",
      "Iteration 14, loss = 185591890.96178129\n",
      "Iteration 15, loss = 185634769.84232295\n",
      "Iteration 16, loss = 185516046.34267259\n",
      "Iteration 17, loss = 185511983.94425133\n",
      "Iteration 18, loss = 185413820.76475212\n",
      "Iteration 19, loss = 185436437.49398595\n",
      "Iteration 20, loss = 185298046.39915520\n",
      "Iteration 21, loss = 185245909.12910855\n",
      "Iteration 22, loss = 185261558.75835851\n",
      "Iteration 23, loss = 185194546.79788405\n",
      "Iteration 24, loss = 185241357.18151563\n",
      "Iteration 25, loss = 185190680.32591173\n",
      "Iteration 26, loss = 185217136.17276359\n",
      "Iteration 27, loss = 185146983.05144238\n",
      "Iteration 28, loss = 185187916.55354214\n",
      "Iteration 29, loss = 185265268.94978273\n",
      "Iteration 30, loss = 185153487.28144559\n",
      "Iteration 31, loss = 185113062.00600439\n",
      "Iteration 32, loss = 185113784.17698878\n",
      "Iteration 33, loss = 185068630.44330597\n",
      "Iteration 34, loss = 185045372.78996992\n",
      "Iteration 35, loss = 185024389.00915757\n",
      "Iteration 36, loss = 185021034.68907064\n",
      "Iteration 37, loss = 185293716.86337599\n",
      "Iteration 38, loss = 185251462.37983292\n",
      "Iteration 39, loss = 185065618.40845165\n",
      "Iteration 40, loss = 185096509.96936697\n",
      "Iteration 41, loss = 185178078.35323384\n",
      "Iteration 42, loss = 184993885.48087686\n",
      "Iteration 43, loss = 184904906.24725980\n",
      "Iteration 44, loss = 185090938.01332143\n",
      "Iteration 45, loss = 184922641.63090008\n",
      "Iteration 46, loss = 184973016.27461475\n",
      "Iteration 47, loss = 184971806.24309546\n",
      "Iteration 48, loss = 184944668.90450469\n",
      "Iteration 49, loss = 185044491.20361397\n",
      "Iteration 50, loss = 184902703.56361651\n",
      "Iteration 51, loss = 185005070.62914926\n",
      "Iteration 52, loss = 184956348.42794019\n",
      "Iteration 53, loss = 185012555.97305560\n",
      "Iteration 54, loss = 184962601.54661927\n",
      "Iteration 55, loss = 184825620.08493584\n",
      "Iteration 56, loss = 184728281.98884690\n",
      "Iteration 57, loss = 184926948.42642918\n",
      "Iteration 58, loss = 184957997.04286814\n",
      "Iteration 59, loss = 184821403.76308200\n",
      "Iteration 60, loss = 184818380.70817086\n",
      "Iteration 61, loss = 184849785.49427789\n",
      "Iteration 62, loss = 184874456.98455074\n",
      "Iteration 63, loss = 184922583.14107943\n",
      "Iteration 64, loss = 184961681.03799447\n",
      "Iteration 65, loss = 185127809.94724664\n",
      "Iteration 66, loss = 185134013.87782422\n",
      "Iteration 67, loss = 184729093.22754031\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 264492132.57062072\n",
      "Iteration 2, loss = 219976780.58183807\n",
      "Iteration 3, loss = 217135456.43957418\n",
      "Iteration 4, loss = 215433783.79128465\n",
      "Iteration 5, loss = 214454741.46565580\n",
      "Iteration 6, loss = 214069105.20813388\n",
      "Iteration 7, loss = 213895825.49521819\n",
      "Iteration 8, loss = 213867059.34616876\n",
      "Iteration 9, loss = 213706166.10423651\n",
      "Iteration 10, loss = 213703677.47634700\n",
      "Iteration 11, loss = 213763420.82316574\n",
      "Iteration 12, loss = 213922866.16431445\n",
      "Iteration 13, loss = 213719648.21859214\n",
      "Iteration 14, loss = 213691154.14632729\n",
      "Iteration 15, loss = 213481944.55344853\n",
      "Iteration 16, loss = 213919472.37244806\n",
      "Iteration 17, loss = 213791048.52840576\n",
      "Iteration 18, loss = 213532934.94068262\n",
      "Iteration 19, loss = 213405610.33955216\n",
      "Iteration 20, loss = 213446951.16632923\n",
      "Iteration 21, loss = 213437869.21626663\n",
      "Iteration 22, loss = 213758244.22311333\n",
      "Iteration 23, loss = 213712364.59756204\n",
      "Iteration 24, loss = 213328446.77811152\n",
      "Iteration 25, loss = 213237978.07560885\n",
      "Iteration 26, loss = 213475308.98912042\n",
      "Iteration 27, loss = 213399711.91564918\n",
      "Iteration 28, loss = 213289157.02364695\n",
      "Iteration 29, loss = 213231768.03097048\n",
      "Iteration 30, loss = 213304109.11372206\n",
      "Iteration 31, loss = 213276850.67414030\n",
      "Iteration 32, loss = 213290569.47725934\n",
      "Iteration 33, loss = 213254199.59232938\n",
      "Iteration 34, loss = 213086085.11177814\n",
      "Iteration 35, loss = 212950526.07780087\n",
      "Iteration 36, loss = 213023036.48793942\n",
      "Iteration 37, loss = 212952211.07361653\n",
      "Iteration 38, loss = 213018143.13720465\n",
      "Iteration 39, loss = 212959258.39987874\n",
      "Iteration 40, loss = 212871563.53900695\n",
      "Iteration 41, loss = 212849041.93134809\n",
      "Iteration 42, loss = 212991551.73733068\n",
      "Iteration 43, loss = 213031308.49562013\n",
      "Iteration 44, loss = 212802274.33347878\n",
      "Iteration 45, loss = 212763416.30635479\n",
      "Iteration 46, loss = 212702884.48554710\n",
      "Iteration 47, loss = 212917329.08562189\n",
      "Iteration 48, loss = 213159656.78387940\n",
      "Iteration 49, loss = 212739085.94609866\n",
      "Iteration 50, loss = 212846704.10900965\n",
      "Iteration 51, loss = 212678151.35618410\n",
      "Iteration 52, loss = 212679195.94087410\n",
      "Iteration 53, loss = 212575516.12681723\n",
      "Iteration 54, loss = 212773582.03825122\n",
      "Iteration 55, loss = 212555561.25009263\n",
      "Iteration 56, loss = 212627797.96572828\n",
      "Iteration 57, loss = 212411793.64875907\n",
      "Iteration 58, loss = 212654028.39316365\n",
      "Iteration 59, loss = 212485291.11740807\n",
      "Iteration 60, loss = 212608889.14237863\n",
      "Iteration 61, loss = 212452725.54243898\n",
      "Iteration 62, loss = 212331931.98873201\n",
      "Iteration 63, loss = 212026270.25299564\n",
      "Iteration 64, loss = 212331225.12034425\n",
      "Iteration 65, loss = 212550457.37524816\n",
      "Iteration 66, loss = 212702080.86372122\n",
      "Iteration 67, loss = 212363375.99526298\n",
      "Iteration 68, loss = 212209809.51704523\n",
      "Iteration 69, loss = 212091362.93805507\n",
      "Iteration 70, loss = 212168285.60618496\n",
      "Iteration 71, loss = 212188261.07898375\n",
      "Iteration 72, loss = 212125259.06532100\n",
      "Iteration 73, loss = 212143679.59465113\n",
      "Iteration 74, loss = 212109366.74205250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1781070163.96227264\n",
      "Iteration 2, loss = 776050896.62823999\n",
      "Iteration 3, loss = 379518460.76405227\n",
      "Iteration 4, loss = 241845822.02626714\n",
      "Iteration 5, loss = 201351093.02042863\n",
      "Iteration 6, loss = 191848292.57967651\n",
      "Iteration 7, loss = 190012286.87442771\n",
      "Iteration 8, loss = 189671014.78839397\n",
      "Iteration 9, loss = 189539282.14259374\n",
      "Iteration 10, loss = 189455225.68204054\n",
      "Iteration 11, loss = 189376275.56727350\n",
      "Iteration 12, loss = 189353149.09562415\n",
      "Iteration 13, loss = 189306032.62070444\n",
      "Iteration 14, loss = 189231659.38203883\n",
      "Iteration 15, loss = 189196007.20178342\n",
      "Iteration 16, loss = 189156851.41656947\n",
      "Iteration 17, loss = 189133761.85539520\n",
      "Iteration 18, loss = 189114512.53467149\n",
      "Iteration 19, loss = 189055138.18193316\n",
      "Iteration 20, loss = 189013240.74064454\n",
      "Iteration 21, loss = 188992675.38507280\n",
      "Iteration 22, loss = 188974962.60583514\n",
      "Iteration 23, loss = 188965575.67851433\n",
      "Iteration 24, loss = 188930207.94683102\n",
      "Iteration 25, loss = 188903307.65310660\n",
      "Iteration 26, loss = 188867807.90181389\n",
      "Iteration 27, loss = 188858368.22237638\n",
      "Iteration 28, loss = 188804891.60985762\n",
      "Iteration 29, loss = 188852994.91514742\n",
      "Iteration 30, loss = 188779600.49429831\n",
      "Iteration 31, loss = 188780611.95790535\n",
      "Iteration 32, loss = 188780379.10562673\n",
      "Iteration 33, loss = 188775133.09202057\n",
      "Iteration 34, loss = 188755890.41605151\n",
      "Iteration 35, loss = 188776679.31304646\n",
      "Iteration 36, loss = 188747552.81646353\n",
      "Iteration 37, loss = 188709516.88801658\n",
      "Iteration 38, loss = 188690403.26641124\n",
      "Iteration 39, loss = 188678811.86371234\n",
      "Iteration 40, loss = 188685155.44292828\n",
      "Iteration 41, loss = 188648773.59252584\n",
      "Iteration 42, loss = 188693927.67911103\n",
      "Iteration 43, loss = 188621604.65797427\n",
      "Iteration 44, loss = 188623026.77110147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 188650809.68071470\n",
      "Iteration 46, loss = 188599713.27109799\n",
      "Iteration 47, loss = 188613360.59914926\n",
      "Iteration 48, loss = 188599606.68100274\n",
      "Iteration 49, loss = 188566769.28452939\n",
      "Iteration 50, loss = 188637075.65847757\n",
      "Iteration 51, loss = 188568485.35754684\n",
      "Iteration 52, loss = 188584586.48821649\n",
      "Iteration 53, loss = 188535640.82540929\n",
      "Iteration 54, loss = 188529227.00072145\n",
      "Iteration 55, loss = 188530414.72464633\n",
      "Iteration 56, loss = 188466958.02244523\n",
      "Iteration 57, loss = 188508103.44044814\n",
      "Iteration 58, loss = 188492857.27488607\n",
      "Iteration 59, loss = 188444079.21907142\n",
      "Iteration 60, loss = 188443457.02056345\n",
      "Iteration 61, loss = 188424518.18758777\n",
      "Iteration 62, loss = 188499872.87307599\n",
      "Iteration 63, loss = 188414716.82213023\n",
      "Iteration 64, loss = 188420769.03056207\n",
      "Iteration 65, loss = 188398940.08792821\n",
      "Iteration 66, loss = 188394150.07419860\n",
      "Iteration 67, loss = 188445729.77638400\n",
      "Iteration 68, loss = 188542157.84110942\n",
      "Iteration 69, loss = 188418627.04626927\n",
      "Iteration 70, loss = 188342126.68721905\n",
      "Iteration 71, loss = 188439259.36139616\n",
      "Iteration 72, loss = 188350980.73633933\n",
      "Iteration 73, loss = 188374076.26931420\n",
      "Iteration 74, loss = 188346739.99883711\n",
      "Iteration 75, loss = 188369505.40004009\n",
      "Iteration 76, loss = 188272329.54529676\n",
      "Iteration 77, loss = 188336392.45210040\n",
      "Iteration 78, loss = 188317027.31172159\n",
      "Iteration 79, loss = 188268652.72499293\n",
      "Iteration 80, loss = 188295105.24200636\n",
      "Iteration 81, loss = 188324922.99713695\n",
      "Iteration 82, loss = 188419352.28502288\n",
      "Iteration 83, loss = 188197702.05795285\n",
      "Iteration 84, loss = 188112412.46456096\n",
      "Iteration 85, loss = 188120663.69480339\n",
      "Iteration 86, loss = 188185267.88543949\n",
      "Iteration 87, loss = 188201927.35767013\n",
      "Iteration 88, loss = 188219433.65631980\n",
      "Iteration 89, loss = 188144729.71070734\n",
      "Iteration 90, loss = 188172023.79242048\n",
      "Iteration 91, loss = 188164964.45346698\n",
      "Iteration 92, loss = 188080417.41427064\n",
      "Iteration 93, loss = 188192596.78367770\n",
      "Iteration 94, loss = 188030066.85841927\n",
      "Iteration 95, loss = 188093484.26375577\n",
      "Iteration 96, loss = 188086710.90714428\n",
      "Iteration 97, loss = 188149191.84904134\n",
      "Iteration 98, loss = 188023393.08119261\n",
      "Iteration 99, loss = 188026269.47698435\n",
      "Iteration 100, loss = 187937599.07903653\n",
      "Iteration 101, loss = 188075165.07655311\n",
      "Iteration 102, loss = 188101304.83430132\n",
      "Iteration 103, loss = 187904960.95834857\n",
      "Iteration 104, loss = 188014842.88320741\n",
      "Iteration 105, loss = 187898606.18103227\n",
      "Iteration 106, loss = 188002343.38913533\n",
      "Iteration 107, loss = 187824028.07245821\n",
      "Iteration 108, loss = 187918211.40245277\n",
      "Iteration 109, loss = 187880450.82289627\n",
      "Iteration 110, loss = 187892258.67958969\n",
      "Iteration 111, loss = 187785064.64843214\n",
      "Iteration 112, loss = 187952027.66992518\n",
      "Iteration 113, loss = 187912084.56445467\n",
      "Iteration 114, loss = 187768002.01241750\n",
      "Iteration 115, loss = 187813452.51211822\n",
      "Iteration 116, loss = 187754993.12459671\n",
      "Iteration 117, loss = 187680334.98990080\n",
      "Iteration 118, loss = 187851184.60588527\n",
      "Iteration 119, loss = 187870585.87039584\n",
      "Iteration 120, loss = 187969102.72490355\n",
      "Iteration 121, loss = 188002545.50927794\n",
      "Iteration 122, loss = 187825293.23201412\n",
      "Iteration 123, loss = 187582259.58584961\n",
      "Iteration 124, loss = 187797848.04569069\n",
      "Iteration 125, loss = 188186166.97986430\n",
      "Iteration 126, loss = 187686648.44816512\n",
      "Iteration 127, loss = 187642725.59091714\n",
      "Iteration 128, loss = 187576466.05590674\n",
      "Iteration 129, loss = 187519202.08231965\n",
      "Iteration 130, loss = 187672765.03049895\n",
      "Iteration 131, loss = 187469047.91416526\n",
      "Iteration 132, loss = 187634346.70478314\n",
      "Iteration 133, loss = 187503457.66325870\n",
      "Iteration 134, loss = 187597029.69498146\n",
      "Iteration 135, loss = 187585634.47059408\n",
      "Iteration 136, loss = 187668220.77189454\n",
      "Iteration 137, loss = 187534661.62921169\n",
      "Iteration 138, loss = 187446318.91053504\n",
      "Iteration 139, loss = 187332877.70921835\n",
      "Iteration 140, loss = 187656936.01601985\n",
      "Iteration 141, loss = 187439734.10631302\n",
      "Iteration 142, loss = 187477657.22034472\n",
      "Iteration 143, loss = 187321957.64216229\n",
      "Iteration 144, loss = 187428146.10433522\n",
      "Iteration 145, loss = 187615937.64037499\n",
      "Iteration 146, loss = 187393913.72498170\n",
      "Iteration 147, loss = 187322374.21613175\n",
      "Iteration 148, loss = 187323169.42591581\n",
      "Iteration 149, loss = 187333898.51277712\n",
      "Iteration 150, loss = 187205488.64887181\n",
      "Iteration 151, loss = 187264498.18174490\n",
      "Iteration 152, loss = 187196735.12477943\n",
      "Iteration 153, loss = 187276702.12723091\n",
      "Iteration 154, loss = 187082454.57844573\n",
      "Iteration 155, loss = 187304580.78758243\n",
      "Iteration 156, loss = 187309273.21391988\n",
      "Iteration 157, loss = 187204029.58334613\n",
      "Iteration 158, loss = 187214683.73650360\n",
      "Iteration 159, loss = 187079372.28909767\n",
      "Iteration 160, loss = 187246404.35915926\n",
      "Iteration 161, loss = 186982820.58653006\n",
      "Iteration 162, loss = 187003645.28968424\n",
      "Iteration 163, loss = 187095805.90379989\n",
      "Iteration 164, loss = 186925932.62426725\n",
      "Iteration 165, loss = 186900077.35638714\n",
      "Iteration 166, loss = 187058479.32203901\n",
      "Iteration 167, loss = 186947653.57291010\n",
      "Iteration 168, loss = 187022338.89824784\n",
      "Iteration 169, loss = 187025177.83675963\n",
      "Iteration 170, loss = 187149126.05916899\n",
      "Iteration 171, loss = 187029429.59397504\n",
      "Iteration 172, loss = 186856848.30945203\n",
      "Iteration 173, loss = 186894875.44081253\n",
      "Iteration 174, loss = 186864827.42629907\n",
      "Iteration 175, loss = 187083306.38564515\n",
      "Iteration 176, loss = 186955853.31839377\n",
      "Iteration 177, loss = 186850980.33304447\n",
      "Iteration 178, loss = 186877777.85250741\n",
      "Iteration 179, loss = 186600534.58245480\n",
      "Iteration 180, loss = 186845426.66843691\n",
      "Iteration 181, loss = 186822789.67591324\n",
      "Iteration 182, loss = 186641677.01286528\n",
      "Iteration 183, loss = 186829038.57542259\n",
      "Iteration 184, loss = 186881429.34265634\n",
      "Iteration 185, loss = 186621366.97674707\n",
      "Iteration 186, loss = 186456033.16467509\n",
      "Iteration 187, loss = 186612530.40371644\n",
      "Iteration 188, loss = 186478675.33806151\n",
      "Iteration 189, loss = 186534169.60953602\n",
      "Iteration 190, loss = 186530894.99891922\n",
      "Iteration 191, loss = 186582564.23126671\n",
      "Iteration 192, loss = 186354418.55469444\n",
      "Iteration 193, loss = 186367338.55851343\n",
      "Iteration 194, loss = 186572845.97994381\n",
      "Iteration 195, loss = 186399771.67225507\n",
      "Iteration 196, loss = 186314617.63932988\n",
      "Iteration 197, loss = 186343996.02306351\n",
      "Iteration 198, loss = 186349315.74562597\n",
      "Iteration 199, loss = 186267725.56332526\n",
      "Iteration 200, loss = 186358945.21236232\n",
      "Iteration 1, loss = 1831961115.09027934\n",
      "Iteration 2, loss = 796674863.67902899\n",
      "Iteration 3, loss = 345425741.83688641\n",
      "Iteration 4, loss = 217488534.22712591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 197778557.23743194\n",
      "Iteration 6, loss = 189641651.65889084\n",
      "Iteration 7, loss = 183338389.08798939\n",
      "Iteration 8, loss = 178432934.40691245\n",
      "Iteration 9, loss = 174548251.17781085\n",
      "Iteration 10, loss = 171479092.69323584\n",
      "Iteration 11, loss = 169097989.35348383\n",
      "Iteration 12, loss = 167262052.78621525\n",
      "Iteration 13, loss = 165867687.42846975\n",
      "Iteration 14, loss = 164808684.94944835\n",
      "Iteration 15, loss = 164045912.27931222\n",
      "Iteration 16, loss = 163515364.70135471\n",
      "Iteration 17, loss = 163088316.84430745\n",
      "Iteration 18, loss = 162795657.71031368\n",
      "Iteration 19, loss = 162569980.45966798\n",
      "Iteration 20, loss = 162399155.92062873\n",
      "Iteration 21, loss = 162370413.13848868\n",
      "Iteration 22, loss = 162293654.94512373\n",
      "Iteration 23, loss = 162195011.24843866\n",
      "Iteration 24, loss = 162062250.87968114\n",
      "Iteration 25, loss = 162041206.06687254\n",
      "Iteration 26, loss = 162112996.74475870\n",
      "Iteration 27, loss = 161974330.58275041\n",
      "Iteration 28, loss = 162006799.80517113\n",
      "Iteration 29, loss = 161950667.24895886\n",
      "Iteration 30, loss = 161865738.26512694\n",
      "Iteration 31, loss = 161859466.00000659\n",
      "Iteration 32, loss = 161851194.43191943\n",
      "Iteration 33, loss = 161685625.77471039\n",
      "Iteration 34, loss = 161792848.47505608\n",
      "Iteration 35, loss = 161737244.31090516\n",
      "Iteration 36, loss = 161602889.28082839\n",
      "Iteration 37, loss = 161561702.93389308\n",
      "Iteration 38, loss = 161505261.93027282\n",
      "Iteration 39, loss = 161555412.87741515\n",
      "Iteration 40, loss = 161414225.47148439\n",
      "Iteration 41, loss = 161360490.22500446\n",
      "Iteration 42, loss = 161360315.11452165\n",
      "Iteration 43, loss = 161330897.95350188\n",
      "Iteration 44, loss = 161287203.73895591\n",
      "Iteration 45, loss = 161235472.84734237\n",
      "Iteration 46, loss = 161169762.60416168\n",
      "Iteration 47, loss = 161155616.69422927\n",
      "Iteration 48, loss = 161164718.96016157\n",
      "Iteration 49, loss = 160982613.68050432\n",
      "Iteration 50, loss = 160931589.44743058\n",
      "Iteration 51, loss = 160934169.53674972\n",
      "Iteration 52, loss = 161106528.36307693\n",
      "Iteration 53, loss = 160998729.15092024\n",
      "Iteration 54, loss = 160729412.68064860\n",
      "Iteration 55, loss = 160755221.57278720\n",
      "Iteration 56, loss = 160658622.11456823\n",
      "Iteration 57, loss = 160703359.78605157\n",
      "Iteration 58, loss = 160548813.71991393\n",
      "Iteration 59, loss = 160545218.10629725\n",
      "Iteration 60, loss = 160608352.97893277\n",
      "Iteration 61, loss = 160388657.39159933\n",
      "Iteration 62, loss = 160297275.94406113\n",
      "Iteration 63, loss = 160260032.07186574\n",
      "Iteration 64, loss = 160184831.46956715\n",
      "Iteration 65, loss = 160079582.57681674\n",
      "Iteration 66, loss = 160243706.78788328\n",
      "Iteration 67, loss = 160155115.95084849\n",
      "Iteration 68, loss = 159925137.21987277\n",
      "Iteration 69, loss = 159860043.23314950\n",
      "Iteration 70, loss = 159842723.37974009\n",
      "Iteration 71, loss = 159726977.43867624\n",
      "Iteration 72, loss = 160152932.00938874\n",
      "Iteration 73, loss = 159689930.81548712\n",
      "Iteration 74, loss = 159625353.21600941\n",
      "Iteration 75, loss = 159582580.83415562\n",
      "Iteration 76, loss = 159520489.39444673\n",
      "Iteration 77, loss = 159367470.59766006\n",
      "Iteration 78, loss = 159347767.95255220\n",
      "Iteration 79, loss = 159267667.69838324\n",
      "Iteration 80, loss = 159148227.63155204\n",
      "Iteration 81, loss = 159157963.01204765\n",
      "Iteration 82, loss = 159055336.11763236\n",
      "Iteration 83, loss = 158973690.42516834\n",
      "Iteration 84, loss = 158993112.70572144\n",
      "Iteration 85, loss = 158860434.93740690\n",
      "Iteration 86, loss = 158795223.31786534\n",
      "Iteration 87, loss = 158834386.77544624\n",
      "Iteration 88, loss = 158811626.41053078\n",
      "Iteration 89, loss = 158608978.78944030\n",
      "Iteration 90, loss = 158520830.32714224\n",
      "Iteration 91, loss = 158594232.27191490\n",
      "Iteration 92, loss = 158409004.51052234\n",
      "Iteration 93, loss = 158359436.04621196\n",
      "Iteration 94, loss = 158542961.08421698\n",
      "Iteration 95, loss = 158236187.04905099\n",
      "Iteration 96, loss = 157990091.50123912\n",
      "Iteration 97, loss = 157964320.65279743\n",
      "Iteration 98, loss = 157809885.96752945\n",
      "Iteration 99, loss = 157821202.47757304\n",
      "Iteration 100, loss = 157709519.01994872\n",
      "Iteration 101, loss = 157557207.36047116\n",
      "Iteration 102, loss = 157538368.45705536\n",
      "Iteration 103, loss = 157508317.79580265\n",
      "Iteration 104, loss = 157449754.99151835\n",
      "Iteration 105, loss = 157398453.75407362\n",
      "Iteration 106, loss = 157129737.72535634\n",
      "Iteration 107, loss = 157011722.66472331\n",
      "Iteration 108, loss = 156924441.60323733\n",
      "Iteration 109, loss = 156972162.44756490\n",
      "Iteration 110, loss = 156854121.61970377\n",
      "Iteration 111, loss = 156742762.33874601\n",
      "Iteration 112, loss = 156649786.79463136\n",
      "Iteration 113, loss = 156540472.76922569\n",
      "Iteration 114, loss = 156358439.11945900\n",
      "Iteration 115, loss = 156435831.34595037\n",
      "Iteration 116, loss = 156465362.61319765\n",
      "Iteration 117, loss = 156245171.47441140\n",
      "Iteration 118, loss = 156187415.33006549\n",
      "Iteration 119, loss = 156029805.50563711\n",
      "Iteration 120, loss = 155991410.41569763\n",
      "Iteration 121, loss = 156112235.48396412\n",
      "Iteration 122, loss = 155885027.64439675\n",
      "Iteration 123, loss = 155939272.50025347\n",
      "Iteration 124, loss = 155535900.32733271\n",
      "Iteration 125, loss = 155717017.98267704\n",
      "Iteration 126, loss = 155442616.93359059\n",
      "Iteration 127, loss = 155509765.78847766\n",
      "Iteration 128, loss = 155326743.71972340\n",
      "Iteration 129, loss = 155236441.85603568\n",
      "Iteration 130, loss = 155291677.22776806\n",
      "Iteration 131, loss = 155225920.00899395\n",
      "Iteration 132, loss = 155113198.46710771\n",
      "Iteration 133, loss = 154965549.93073690\n",
      "Iteration 134, loss = 154893721.44519851\n",
      "Iteration 135, loss = 154875035.01327896\n",
      "Iteration 136, loss = 155034727.85050336\n",
      "Iteration 137, loss = 155212035.50296015\n",
      "Iteration 138, loss = 154631288.17795226\n",
      "Iteration 139, loss = 154473259.64392591\n",
      "Iteration 140, loss = 154613111.14089602\n",
      "Iteration 141, loss = 154423146.96820500\n",
      "Iteration 142, loss = 154514022.87466332\n",
      "Iteration 143, loss = 154971054.91268536\n",
      "Iteration 144, loss = 154272639.36872432\n",
      "Iteration 145, loss = 154539801.59124869\n",
      "Iteration 146, loss = 154396336.54979721\n",
      "Iteration 147, loss = 154106714.27486989\n",
      "Iteration 148, loss = 154234665.68959215\n",
      "Iteration 149, loss = 154280230.94649494\n",
      "Iteration 150, loss = 154293346.65138400\n",
      "Iteration 151, loss = 154053074.27134240\n",
      "Iteration 152, loss = 153894046.94372296\n",
      "Iteration 153, loss = 153998786.15469307\n",
      "Iteration 154, loss = 154001550.90226662\n",
      "Iteration 155, loss = 153784314.33827558\n",
      "Iteration 156, loss = 153947388.43839166\n",
      "Iteration 157, loss = 153777491.62552547\n",
      "Iteration 158, loss = 153790616.80116060\n",
      "Iteration 159, loss = 153709343.14329168\n",
      "Iteration 160, loss = 153748326.42925274\n",
      "Iteration 161, loss = 153492102.95327049\n",
      "Iteration 162, loss = 153652199.77520147\n",
      "Iteration 163, loss = 153527469.66137111\n",
      "Iteration 164, loss = 153542474.66276139\n",
      "Iteration 165, loss = 153844523.00320834\n",
      "Iteration 166, loss = 153479001.93292665\n",
      "Iteration 167, loss = 153310710.24869072\n",
      "Iteration 168, loss = 153233816.94310555\n",
      "Iteration 169, loss = 153217524.15692943\n",
      "Iteration 170, loss = 153290003.65533882\n",
      "Iteration 171, loss = 153203478.78422982\n",
      "Iteration 172, loss = 153066419.62009874\n",
      "Iteration 173, loss = 153160930.82202032\n",
      "Iteration 174, loss = 153218950.89895737\n",
      "Iteration 175, loss = 153041677.58442768\n",
      "Iteration 176, loss = 153262098.71308351\n",
      "Iteration 177, loss = 153236739.27627248\n",
      "Iteration 178, loss = 153005523.52437237\n",
      "Iteration 179, loss = 153348435.06740242\n",
      "Iteration 180, loss = 152887579.24944571\n",
      "Iteration 181, loss = 152898233.08977154\n",
      "Iteration 182, loss = 153192992.85848150\n",
      "Iteration 183, loss = 152887572.74615768\n",
      "Iteration 184, loss = 152869301.59633189\n",
      "Iteration 185, loss = 152783993.75823617\n",
      "Iteration 186, loss = 152874709.92624611\n",
      "Iteration 187, loss = 152824378.88600174\n",
      "Iteration 188, loss = 153161404.84821278\n",
      "Iteration 189, loss = 152962278.59723067\n",
      "Iteration 190, loss = 152761915.00239342\n",
      "Iteration 191, loss = 153016957.83825564\n",
      "Iteration 192, loss = 152669419.49302223\n",
      "Iteration 193, loss = 152593817.32312396\n",
      "Iteration 194, loss = 152611463.90535358\n",
      "Iteration 195, loss = 152505712.57616544\n",
      "Iteration 196, loss = 152748926.57907841\n",
      "Iteration 197, loss = 152479752.02854690\n",
      "Iteration 198, loss = 152340041.68514934\n",
      "Iteration 199, loss = 152757093.02931178\n",
      "Iteration 200, loss = 152498377.68322399\n",
      "Iteration 1, loss = 812922127.55618596\n",
      "Iteration 2, loss = 388235278.29686069\n",
      "Iteration 3, loss = 244075276.59487295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 210222193.99533376\n",
      "Iteration 5, loss = 204788534.03807235\n",
      "Iteration 6, loss = 203449890.61942688\n",
      "Iteration 7, loss = 202478535.91581151\n",
      "Iteration 8, loss = 201594565.52077848\n",
      "Iteration 9, loss = 200645468.36377209\n",
      "Iteration 10, loss = 199500141.57456788\n",
      "Iteration 11, loss = 198357471.43540341\n",
      "Iteration 12, loss = 197469358.56075510\n",
      "Iteration 13, loss = 196807694.14705783\n",
      "Iteration 14, loss = 196110490.47950634\n",
      "Iteration 15, loss = 195599704.04014337\n",
      "Iteration 16, loss = 195330947.43030348\n",
      "Iteration 17, loss = 194930131.55901027\n",
      "Iteration 18, loss = 194741910.77688402\n",
      "Iteration 19, loss = 194491309.78265753\n",
      "Iteration 20, loss = 194271645.88902199\n",
      "Iteration 21, loss = 194065396.36124697\n",
      "Iteration 22, loss = 193879100.57555941\n",
      "Iteration 23, loss = 193976813.58036822\n",
      "Iteration 24, loss = 193658261.85737813\n",
      "Iteration 25, loss = 193547492.42247218\n",
      "Iteration 26, loss = 193393170.19242096\n",
      "Iteration 27, loss = 193243288.84537092\n",
      "Iteration 28, loss = 193372091.15915376\n",
      "Iteration 29, loss = 193031769.53611651\n",
      "Iteration 30, loss = 192806656.10373405\n",
      "Iteration 31, loss = 192758202.15228215\n",
      "Iteration 32, loss = 192588855.23999855\n",
      "Iteration 33, loss = 192533851.70861298\n",
      "Iteration 34, loss = 192328031.44801790\n",
      "Iteration 35, loss = 192339512.49911326\n",
      "Iteration 36, loss = 192037574.67251742\n",
      "Iteration 37, loss = 192010292.45230901\n",
      "Iteration 38, loss = 191835038.26302144\n",
      "Iteration 39, loss = 191715287.12493280\n",
      "Iteration 40, loss = 191614669.46032161\n",
      "Iteration 41, loss = 191503213.75024435\n",
      "Iteration 42, loss = 191167747.17828885\n",
      "Iteration 43, loss = 191306048.95000404\n",
      "Iteration 44, loss = 190959803.64072391\n",
      "Iteration 45, loss = 190802574.64841604\n",
      "Iteration 46, loss = 190928321.35261962\n",
      "Iteration 47, loss = 190630591.92881593\n",
      "Iteration 48, loss = 190400653.39142776\n",
      "Iteration 49, loss = 190176364.95305943\n",
      "Iteration 50, loss = 190061118.44280118\n",
      "Iteration 51, loss = 189935343.26449263\n",
      "Iteration 52, loss = 189737524.06822333\n",
      "Iteration 53, loss = 189628831.50930509\n",
      "Iteration 54, loss = 189500364.12478188\n",
      "Iteration 55, loss = 189344022.94466394\n",
      "Iteration 56, loss = 189194186.40249452\n",
      "Iteration 57, loss = 189355782.75982121\n",
      "Iteration 58, loss = 188776628.61662796\n",
      "Iteration 59, loss = 188638079.38945892\n",
      "Iteration 60, loss = 188490502.89758143\n",
      "Iteration 61, loss = 188342074.02761078\n",
      "Iteration 62, loss = 188186960.38827235\n",
      "Iteration 63, loss = 187932979.30043882\n",
      "Iteration 64, loss = 187776906.83010653\n",
      "Iteration 65, loss = 187571476.09177521\n",
      "Iteration 66, loss = 187323985.37027714\n",
      "Iteration 67, loss = 187141281.98300460\n",
      "Iteration 68, loss = 187001752.91788045\n",
      "Iteration 69, loss = 186811309.48987415\n",
      "Iteration 70, loss = 186975845.70568928\n",
      "Iteration 71, loss = 186764914.62214208\n",
      "Iteration 72, loss = 186371054.16200092\n",
      "Iteration 73, loss = 186229152.68767327\n",
      "Iteration 74, loss = 186378053.11737522\n",
      "Iteration 75, loss = 185826376.62404409\n",
      "Iteration 76, loss = 185667123.39053059\n",
      "Iteration 77, loss = 185688927.81767848\n",
      "Iteration 78, loss = 185277251.62253356\n",
      "Iteration 79, loss = 185146433.96677798\n",
      "Iteration 80, loss = 185087645.39967439\n",
      "Iteration 81, loss = 184910625.24787638\n",
      "Iteration 82, loss = 184934702.93121919\n",
      "Iteration 83, loss = 184485889.67210272\n",
      "Iteration 84, loss = 184697300.41741148\n",
      "Iteration 85, loss = 184196521.33772060\n",
      "Iteration 86, loss = 184246873.10437906\n",
      "Iteration 87, loss = 184008350.24068978\n",
      "Iteration 88, loss = 183813563.83034375\n",
      "Iteration 89, loss = 183938398.92896307\n",
      "Iteration 90, loss = 183272635.83657664\n",
      "Iteration 91, loss = 183393925.25021067\n",
      "Iteration 92, loss = 183194366.42233548\n",
      "Iteration 93, loss = 183092907.65533954\n",
      "Iteration 94, loss = 182817912.10148826\n",
      "Iteration 95, loss = 182866307.00072226\n",
      "Iteration 96, loss = 182487159.41981214\n",
      "Iteration 97, loss = 183516906.58480409\n",
      "Iteration 98, loss = 182325424.65596384\n",
      "Iteration 99, loss = 182172431.08374846\n",
      "Iteration 100, loss = 182754223.45276925\n",
      "Iteration 101, loss = 181991483.21653131\n",
      "Iteration 102, loss = 181934015.87885424\n",
      "Iteration 103, loss = 182053776.54969138\n",
      "Iteration 104, loss = 182147051.85965276\n",
      "Iteration 105, loss = 181960137.76829880\n",
      "Iteration 106, loss = 181609078.70066333\n",
      "Iteration 107, loss = 181612954.08240378\n",
      "Iteration 108, loss = 181491683.23029691\n",
      "Iteration 109, loss = 181505507.02118930\n",
      "Iteration 110, loss = 181354811.81110588\n",
      "Iteration 111, loss = 181512996.10611534\n",
      "Iteration 112, loss = 181183354.88118103\n",
      "Iteration 113, loss = 181213327.67704052\n",
      "Iteration 114, loss = 181130219.56526542\n",
      "Iteration 115, loss = 181390184.39710811\n",
      "Iteration 116, loss = 181031757.09542945\n",
      "Iteration 117, loss = 180846054.03598002\n",
      "Iteration 118, loss = 180498855.58764020\n",
      "Iteration 119, loss = 180549940.06762099\n",
      "Iteration 120, loss = 180234075.84447148\n",
      "Iteration 121, loss = 180398161.71914768\n",
      "Iteration 122, loss = 180301129.85410497\n",
      "Iteration 123, loss = 180201715.98687440\n",
      "Iteration 124, loss = 180637989.19164315\n",
      "Iteration 125, loss = 180017175.29054543\n",
      "Iteration 126, loss = 180088158.11788946\n",
      "Iteration 127, loss = 180360372.05960578\n",
      "Iteration 128, loss = 179924229.58546826\n",
      "Iteration 129, loss = 179820245.46012917\n",
      "Iteration 130, loss = 179988662.15036595\n",
      "Iteration 131, loss = 179722345.13555697\n",
      "Iteration 132, loss = 179538050.95448351\n",
      "Iteration 133, loss = 180245164.34703836\n",
      "Iteration 134, loss = 179659789.63100117\n",
      "Iteration 135, loss = 179460758.62730038\n",
      "Iteration 136, loss = 179677615.66085139\n",
      "Iteration 137, loss = 179481516.12240976\n",
      "Iteration 138, loss = 179359288.84223300\n",
      "Iteration 139, loss = 179461417.53167522\n",
      "Iteration 140, loss = 179886007.49482095\n",
      "Iteration 141, loss = 179190586.25785142\n",
      "Iteration 142, loss = 178869956.36873946\n",
      "Iteration 143, loss = 178880928.30781496\n",
      "Iteration 144, loss = 178971080.60247174\n",
      "Iteration 145, loss = 178766880.19606295\n",
      "Iteration 146, loss = 179002660.53806198\n",
      "Iteration 147, loss = 178686819.92550650\n",
      "Iteration 148, loss = 178666262.89535600\n",
      "Iteration 149, loss = 178536702.92763358\n",
      "Iteration 150, loss = 178486910.30810335\n",
      "Iteration 151, loss = 178438194.49784935\n",
      "Iteration 152, loss = 178499782.60621339\n",
      "Iteration 153, loss = 178309412.32925585\n",
      "Iteration 154, loss = 178461939.38311338\n",
      "Iteration 155, loss = 178059648.73836592\n",
      "Iteration 156, loss = 178547983.50577456\n",
      "Iteration 157, loss = 178222043.20564774\n",
      "Iteration 158, loss = 178026704.19730166\n",
      "Iteration 159, loss = 177985943.46408129\n",
      "Iteration 160, loss = 177824203.39492193\n",
      "Iteration 161, loss = 178764327.65676633\n",
      "Iteration 162, loss = 177908035.20212027\n",
      "Iteration 163, loss = 178171918.88524878\n",
      "Iteration 164, loss = 178056699.63578168\n",
      "Iteration 165, loss = 177968545.73711184\n",
      "Iteration 166, loss = 177361121.82408610\n",
      "Iteration 167, loss = 177765056.26082146\n",
      "Iteration 168, loss = 177332203.22147709\n",
      "Iteration 169, loss = 177371695.78848949\n",
      "Iteration 170, loss = 177214268.72719330\n",
      "Iteration 171, loss = 177041836.82006589\n",
      "Iteration 172, loss = 177564961.76167873\n",
      "Iteration 173, loss = 177158213.58781394\n",
      "Iteration 174, loss = 177049027.66576955\n",
      "Iteration 175, loss = 177035628.22463146\n",
      "Iteration 176, loss = 177182102.64161173\n",
      "Iteration 177, loss = 176944808.59720764\n",
      "Iteration 178, loss = 177346337.81807962\n",
      "Iteration 179, loss = 176940847.09501061\n",
      "Iteration 180, loss = 176493723.69684237\n",
      "Iteration 181, loss = 176544436.20019659\n",
      "Iteration 182, loss = 176401516.48239267\n",
      "Iteration 183, loss = 176454911.99357018\n",
      "Iteration 184, loss = 176294892.64743075\n",
      "Iteration 185, loss = 176399085.39387241\n",
      "Iteration 186, loss = 176121113.30571815\n",
      "Iteration 187, loss = 176205424.45172706\n",
      "Iteration 188, loss = 176326820.11491460\n",
      "Iteration 189, loss = 176419088.98985723\n",
      "Iteration 190, loss = 176072053.37585470\n",
      "Iteration 191, loss = 175983909.50041720\n",
      "Iteration 192, loss = 176222854.30340058\n",
      "Iteration 193, loss = 176035250.27394029\n",
      "Iteration 194, loss = 175873546.68892109\n",
      "Iteration 195, loss = 175587346.54994124\n",
      "Iteration 196, loss = 175661914.51586089\n",
      "Iteration 197, loss = 175802305.99969473\n",
      "Iteration 198, loss = 175865232.51518399\n",
      "Iteration 199, loss = 175461598.35065329\n",
      "Iteration 200, loss = 175358114.27123520\n",
      "MAE: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9818.704465978451\n",
      "MSE:  104514245.19293924\n",
      "RMSE:  10223.220881548987\n",
      "R2:  -114.77122534321151\n",
      "1 9818.704465978451\n",
      "Find best model\n",
      "(100605, 15) (26511, 15)\n",
      "Iteration 1, loss = 402073018.95091671\n",
      "Iteration 2, loss = 401345933.81290120\n",
      "Iteration 3, loss = 401355935.13597268\n",
      "Iteration 4, loss = 401341898.27277946\n",
      "Iteration 5, loss = 401144430.26182377\n",
      "Iteration 6, loss = 401322665.15498984\n",
      "Iteration 7, loss = 401214723.59094369\n",
      "Iteration 8, loss = 401144946.94916391\n",
      "Iteration 9, loss = 401031758.59857637\n",
      "Iteration 10, loss = 401339330.14789295\n",
      "Iteration 11, loss = 401295817.17231369\n",
      "Iteration 12, loss = 401196072.71148217\n",
      "Iteration 13, loss = 401232827.56245136\n",
      "Iteration 14, loss = 401247523.88315654\n",
      "Iteration 15, loss = 401235577.38182843\n",
      "Iteration 16, loss = 401067454.80322033\n",
      "Iteration 17, loss = 401014057.94016528\n",
      "Iteration 18, loss = 401300389.03523517\n",
      "Iteration 19, loss = 401000488.33812624\n",
      "Iteration 20, loss = 401101518.11100495\n",
      "Iteration 21, loss = 401164387.82211989\n",
      "Iteration 22, loss = 400870551.31412816\n",
      "Iteration 23, loss = 400808394.82219428\n",
      "Iteration 24, loss = 400723944.46645498\n",
      "Iteration 25, loss = 401062891.13987553\n",
      "Iteration 26, loss = 400829096.12038654\n",
      "Iteration 27, loss = 400865806.19238156\n",
      "Iteration 28, loss = 400791285.17048174\n",
      "Iteration 29, loss = 400820361.18177968\n",
      "Iteration 30, loss = 400838055.69918668\n",
      "Iteration 31, loss = 401116211.11369383\n",
      "Iteration 32, loss = 401094000.33085859\n",
      "Iteration 33, loss = 400980331.92770296\n",
      "Iteration 34, loss = 400929494.79828918\n",
      "Iteration 35, loss = 400795947.66471553\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 386858841.60839701\n",
      "Iteration 2, loss = 152196295.55313689\n",
      "Iteration 3, loss = 143412874.88095170\n",
      "Iteration 4, loss = 137245734.83129933\n",
      "Iteration 5, loss = 133186457.10972446\n",
      "Iteration 6, loss = 130549267.20680986\n",
      "Iteration 7, loss = 128780093.49913450\n",
      "Iteration 8, loss = 127193994.75022905\n",
      "Iteration 9, loss = 126164857.70708120\n",
      "Iteration 10, loss = 125558789.65430957\n",
      "Iteration 11, loss = 125108593.24168259\n",
      "Iteration 12, loss = 124769688.03219023\n",
      "Iteration 13, loss = 124615373.37931235\n",
      "Iteration 14, loss = 124536685.39420041\n",
      "Iteration 15, loss = 124523876.00706391\n",
      "Iteration 16, loss = 124290880.49113186\n",
      "Iteration 17, loss = 124342630.51039632\n",
      "Iteration 18, loss = 124276083.13026404\n",
      "Iteration 19, loss = 124180812.54892263\n",
      "Iteration 20, loss = 124150796.95139518\n",
      "Iteration 21, loss = 124280255.52754411\n",
      "Iteration 22, loss = 124152877.61501437\n",
      "Iteration 23, loss = 124229756.30132712\n",
      "Iteration 24, loss = 124181818.01491936\n",
      "Iteration 25, loss = 124394544.98742497\n",
      "Iteration 26, loss = 124214488.24900115\n",
      "Iteration 27, loss = 124192144.92906721\n",
      "Iteration 28, loss = 124188359.93008389\n",
      "Iteration 29, loss = 124090256.90635271\n",
      "Iteration 30, loss = 124360650.21378818\n",
      "Iteration 31, loss = 124060911.33225822\n",
      "Iteration 32, loss = 124100301.15732233\n",
      "Iteration 33, loss = 124134908.88890821\n",
      "Iteration 34, loss = 124073704.04676987\n",
      "Iteration 35, loss = 123995908.49688567\n",
      "Iteration 36, loss = 124173807.69110273\n",
      "Iteration 37, loss = 124168779.36510289\n",
      "Iteration 38, loss = 123977954.92301172\n",
      "Iteration 39, loss = 124082171.83354321\n",
      "Iteration 40, loss = 123986156.13921110\n",
      "Iteration 41, loss = 124014460.54359268\n",
      "Iteration 42, loss = 124198313.13459241\n",
      "Iteration 43, loss = 124030730.86152618\n",
      "Iteration 44, loss = 124184436.86268146\n",
      "Iteration 45, loss = 123992177.15826619\n",
      "Iteration 46, loss = 124030172.21588428\n",
      "Iteration 47, loss = 124246331.63717541\n",
      "Iteration 48, loss = 124103929.01602288\n",
      "Iteration 49, loss = 123967676.78125571\n",
      "Iteration 50, loss = 124012497.08170474\n",
      "Iteration 51, loss = 124033141.26131767\n",
      "Iteration 52, loss = 123942704.05192278\n",
      "Iteration 53, loss = 124034818.15428826\n",
      "Iteration 54, loss = 124140394.80227883\n",
      "Iteration 55, loss = 124020496.03503513\n",
      "Iteration 56, loss = 124090587.38123041\n",
      "Iteration 57, loss = 124090201.65541585\n",
      "Iteration 58, loss = 124206371.32471330\n",
      "Iteration 59, loss = 123937509.28691496\n",
      "Iteration 60, loss = 123969130.59843811\n",
      "Iteration 61, loss = 123890175.50114654\n",
      "Iteration 62, loss = 123812196.84076978\n",
      "Iteration 63, loss = 123894082.20800379\n",
      "Iteration 64, loss = 124175671.95053162\n",
      "Iteration 65, loss = 124027300.67014734\n",
      "Iteration 66, loss = 124192892.58598374\n",
      "Iteration 67, loss = 123817872.60766582\n",
      "Iteration 68, loss = 123856782.73038638\n",
      "Iteration 69, loss = 123899321.95069800\n",
      "Iteration 70, loss = 123881410.25489876\n",
      "Iteration 71, loss = 124067969.10477366\n",
      "Iteration 72, loss = 123862658.25582469\n",
      "Iteration 73, loss = 123980685.92996372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1680793729.65706062\n",
      "Iteration 2, loss = 797329265.26952016\n",
      "Iteration 3, loss = 418536697.87073690\n",
      "Iteration 4, loss = 302832124.49172342\n",
      "Iteration 5, loss = 282568058.26177073\n",
      "Iteration 6, loss = 278504799.71747571\n",
      "Iteration 7, loss = 275537629.67334098\n",
      "Iteration 8, loss = 272812824.50690413\n",
      "Iteration 9, loss = 270415098.92911112\n",
      "Iteration 10, loss = 268281277.26992521\n",
      "Iteration 11, loss = 266441374.74091861\n",
      "Iteration 12, loss = 264923911.05294234\n",
      "Iteration 13, loss = 263688443.08798862\n",
      "Iteration 14, loss = 262697573.45613965\n",
      "Iteration 15, loss = 261876464.88056797\n",
      "Iteration 16, loss = 261195162.80882525\n",
      "Iteration 17, loss = 260639308.50912949\n",
      "Iteration 18, loss = 260196145.96587804\n",
      "Iteration 19, loss = 259798225.85385233\n",
      "Iteration 20, loss = 259498361.91589570\n",
      "Iteration 21, loss = 259277377.95650917\n",
      "Iteration 22, loss = 259093748.20001188\n",
      "Iteration 23, loss = 258988230.85640231\n",
      "Iteration 24, loss = 258889893.39848745\n",
      "Iteration 25, loss = 258807054.61759079\n",
      "Iteration 26, loss = 258776253.32667333\n",
      "Iteration 27, loss = 258732704.45010293\n",
      "Iteration 28, loss = 258698896.06219536\n",
      "Iteration 29, loss = 258710815.68379423\n",
      "Iteration 30, loss = 258689461.99393016\n",
      "Iteration 31, loss = 258641318.21877763\n",
      "Iteration 32, loss = 258571526.97591576\n",
      "Iteration 33, loss = 258462994.95377958\n",
      "Iteration 34, loss = 258475215.03759596\n",
      "Iteration 35, loss = 258430649.44263452\n",
      "Iteration 36, loss = 258349998.33284014\n",
      "Iteration 37, loss = 258411542.73769742\n",
      "Iteration 38, loss = 258360065.76300263\n",
      "Iteration 39, loss = 258370820.89675087\n",
      "Iteration 40, loss = 258439756.27215934\n",
      "Iteration 41, loss = 258298597.88476744\n",
      "Iteration 42, loss = 258260341.80262807\n",
      "Iteration 43, loss = 258269936.32271075\n",
      "Iteration 44, loss = 258440004.32222798\n",
      "Iteration 45, loss = 258300641.47024781\n",
      "Iteration 46, loss = 258209713.57704479\n",
      "Iteration 47, loss = 258343469.46967179\n",
      "Iteration 48, loss = 258324561.32700875\n",
      "Iteration 49, loss = 258205756.70115146\n",
      "Iteration 50, loss = 258288679.00305820\n",
      "Iteration 51, loss = 258229574.24413443\n",
      "Iteration 52, loss = 258242820.17439154\n",
      "Iteration 53, loss = 258257089.35341710\n",
      "Iteration 54, loss = 258187686.55457452\n",
      "Iteration 55, loss = 258234877.39482772\n",
      "Iteration 56, loss = 258273682.26576978\n",
      "Iteration 57, loss = 258249446.11025712\n",
      "Iteration 58, loss = 258171632.60706231\n",
      "Iteration 59, loss = 258306926.60184282\n",
      "Iteration 60, loss = 258256136.16728705\n",
      "Iteration 61, loss = 258218053.05864659\n",
      "Iteration 62, loss = 258327139.84572223\n",
      "Iteration 63, loss = 258174398.80919698\n",
      "Iteration 64, loss = 258201023.59232464\n",
      "Iteration 65, loss = 258215363.49949896\n",
      "Iteration 66, loss = 258169814.01169452\n",
      "Iteration 67, loss = 258210257.52147052\n",
      "Iteration 68, loss = 258215553.54634267\n",
      "Iteration 69, loss = 258190917.95122701\n",
      "Iteration 70, loss = 258459936.12257487\n",
      "Iteration 71, loss = 258180482.90056181\n",
      "Iteration 72, loss = 258218006.77576700\n",
      "Iteration 73, loss = 258183817.81602734\n",
      "Iteration 74, loss = 258283407.61501157\n",
      "Iteration 75, loss = 258181905.14070094\n",
      "Iteration 76, loss = 258163423.76497543\n",
      "Iteration 77, loss = 258250409.86076489\n",
      "Iteration 78, loss = 258232791.85750335\n",
      "Iteration 79, loss = 258271911.15226972\n",
      "Iteration 80, loss = 258116790.90861505\n",
      "Iteration 81, loss = 258150632.55669257\n",
      "Iteration 82, loss = 258151793.19216681\n",
      "Iteration 83, loss = 258100561.94640371\n",
      "Iteration 84, loss = 258110591.53749609\n",
      "Iteration 85, loss = 258222354.25358692\n",
      "Iteration 86, loss = 258429767.97152916\n",
      "Iteration 87, loss = 258016791.86710313\n",
      "Iteration 88, loss = 258233996.18533480\n",
      "Iteration 89, loss = 258024435.27588394\n",
      "Iteration 90, loss = 258004621.03236830\n",
      "Iteration 91, loss = 258486538.29719198\n",
      "Iteration 92, loss = 258089402.63494787\n",
      "Iteration 93, loss = 258028520.67490825\n",
      "Iteration 94, loss = 258111806.29451182\n",
      "Iteration 95, loss = 258069537.23972628\n",
      "Iteration 96, loss = 258051759.74996251\n",
      "Iteration 97, loss = 258123588.10797897\n",
      "Iteration 98, loss = 258162611.28291813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99, loss = 258141584.23359215\n",
      "Iteration 100, loss = 258050483.24150255\n",
      "Iteration 101, loss = 258052367.38378930\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3482860901.44715929\n",
      "Iteration 2, loss = 2442953295.80401421\n",
      "Iteration 3, loss = 1804129750.05234838\n",
      "Iteration 4, loss = 1398858890.92329025\n",
      "Iteration 5, loss = 1129421999.21105933\n",
      "Iteration 6, loss = 949323624.95652580\n",
      "Iteration 7, loss = 830292748.48854828\n",
      "Iteration 8, loss = 748588520.45527422\n",
      "Iteration 9, loss = 686284401.99699306\n",
      "Iteration 10, loss = 635206650.29833150\n",
      "Iteration 11, loss = 591912050.41637981\n",
      "Iteration 12, loss = 553854961.19803762\n",
      "Iteration 13, loss = 519282633.58600128\n",
      "Iteration 14, loss = 486824691.07626551\n",
      "Iteration 15, loss = 455744209.46261692\n",
      "Iteration 16, loss = 425513181.87693977\n",
      "Iteration 17, loss = 396478390.22806454\n",
      "Iteration 18, loss = 368851367.89630032\n",
      "Iteration 19, loss = 344346171.73858988\n",
      "Iteration 20, loss = 324181071.54816580\n",
      "Iteration 21, loss = 309118594.86910284\n",
      "Iteration 22, loss = 298997493.77776784\n",
      "Iteration 23, loss = 292705896.14869010\n",
      "Iteration 24, loss = 289196627.10443193\n",
      "Iteration 25, loss = 287379352.94617003\n",
      "Iteration 26, loss = 286513651.33792043\n",
      "Iteration 27, loss = 286082280.99618995\n",
      "Iteration 28, loss = 285866531.31191248\n",
      "Iteration 29, loss = 285730115.99628288\n",
      "Iteration 30, loss = 285627676.40438318\n",
      "Iteration 31, loss = 285521140.74722844\n",
      "Iteration 32, loss = 285443794.45154411\n",
      "Iteration 33, loss = 285350399.24168938\n",
      "Iteration 34, loss = 285269681.26530540\n",
      "Iteration 35, loss = 285191934.06415403\n",
      "Iteration 36, loss = 285103540.16210461\n",
      "Iteration 37, loss = 285030192.95468140\n",
      "Iteration 38, loss = 284977176.21094036\n",
      "Iteration 39, loss = 284837388.98828834\n",
      "Iteration 40, loss = 284718619.28589267\n",
      "Iteration 41, loss = 284597140.60916644\n",
      "Iteration 42, loss = 284499516.04041672\n",
      "Iteration 43, loss = 284371425.41212982\n",
      "Iteration 44, loss = 284284229.11553484\n",
      "Iteration 45, loss = 284228750.45022362\n",
      "Iteration 46, loss = 284151570.28962564\n",
      "Iteration 47, loss = 284054462.59191447\n",
      "Iteration 48, loss = 283954071.12244391\n",
      "Iteration 49, loss = 283893335.69896346\n",
      "Iteration 50, loss = 283840750.38540256\n",
      "Iteration 51, loss = 283781655.03146976\n",
      "Iteration 52, loss = 283800213.24090672\n",
      "Iteration 53, loss = 283750895.84771663\n",
      "Iteration 54, loss = 283765370.98741210\n",
      "Iteration 55, loss = 283782221.93821102\n",
      "Iteration 56, loss = 283699264.05825472\n",
      "Iteration 57, loss = 283659609.54697442\n",
      "Iteration 58, loss = 283614615.97936261\n",
      "Iteration 59, loss = 283605558.51475358\n",
      "Iteration 60, loss = 283584066.11165488\n",
      "Iteration 61, loss = 283587043.68810409\n",
      "Iteration 62, loss = 283549230.87753928\n",
      "Iteration 63, loss = 283564446.22600275\n",
      "Iteration 64, loss = 283561528.16526741\n",
      "Iteration 65, loss = 283579505.44251090\n",
      "Iteration 66, loss = 283538325.51082790\n",
      "Iteration 67, loss = 283535236.77527720\n",
      "Iteration 68, loss = 283499235.90439755\n",
      "Iteration 69, loss = 283514563.17903942\n",
      "Iteration 70, loss = 283486947.09567744\n",
      "Iteration 71, loss = 283498064.77148068\n",
      "Iteration 72, loss = 283481078.71432137\n",
      "Iteration 73, loss = 283497075.78917354\n",
      "Iteration 74, loss = 283510264.68548375\n",
      "Iteration 75, loss = 283536065.96305442\n",
      "Iteration 76, loss = 283498468.32147503\n",
      "Iteration 77, loss = 283512312.65375721\n",
      "Iteration 78, loss = 283462126.93728787\n",
      "Iteration 79, loss = 283450156.72360075\n",
      "Iteration 80, loss = 283494266.10691404\n",
      "Iteration 81, loss = 283438950.21697640\n",
      "Iteration 82, loss = 283456036.16776592\n",
      "Iteration 83, loss = 283564901.47530115\n",
      "Iteration 84, loss = 283502990.86604077\n",
      "Iteration 85, loss = 283477489.46956414\n",
      "Iteration 86, loss = 283482959.70507056\n",
      "Iteration 87, loss = 283525875.58437818\n",
      "Iteration 88, loss = 283495329.92799628\n",
      "Iteration 89, loss = 283409616.66718060\n",
      "Iteration 90, loss = 283517222.34228766\n",
      "Iteration 91, loss = 283376265.83457834\n",
      "Iteration 92, loss = 283418575.33281958\n",
      "Iteration 93, loss = 283501850.99665618\n",
      "Iteration 94, loss = 283384375.59277552\n",
      "Iteration 95, loss = 283447502.05124968\n",
      "Iteration 96, loss = 283404675.40136600\n",
      "Iteration 97, loss = 283421873.27358347\n",
      "Iteration 98, loss = 283374020.31696677\n",
      "Iteration 99, loss = 283338475.11240250\n",
      "Iteration 100, loss = 283373787.56576198\n",
      "Iteration 101, loss = 283353805.34856832\n",
      "Iteration 102, loss = 283331336.87535304\n",
      "Iteration 103, loss = 283327007.67083395\n",
      "Iteration 104, loss = 283378285.11472380\n",
      "Iteration 105, loss = 283340959.14464766\n",
      "Iteration 106, loss = 283496076.48531634\n",
      "Iteration 107, loss = 283319188.12072176\n",
      "Iteration 108, loss = 283387808.60673112\n",
      "Iteration 109, loss = 283345581.00974196\n",
      "Iteration 110, loss = 283395255.12416297\n",
      "Iteration 111, loss = 283380237.52331483\n",
      "Iteration 112, loss = 283274404.42110133\n",
      "Iteration 113, loss = 283336086.25121111\n",
      "Iteration 114, loss = 283401034.39413553\n",
      "Iteration 115, loss = 283299705.01692796\n",
      "Iteration 116, loss = 283325848.58011049\n",
      "Iteration 117, loss = 283306149.18970263\n",
      "Iteration 118, loss = 283285636.39137435\n",
      "Iteration 119, loss = 283326639.49829561\n",
      "Iteration 120, loss = 283292790.13750100\n",
      "Iteration 121, loss = 283453792.73198438\n",
      "Iteration 122, loss = 283327958.47604638\n",
      "Iteration 123, loss = 283463818.49797440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1133031393.79541683\n",
      "Iteration 2, loss = 440375338.97013241\n",
      "Iteration 3, loss = 322759790.11626643\n",
      "Iteration 4, loss = 309726708.08112884\n",
      "Iteration 5, loss = 301699073.99163467\n",
      "Iteration 6, loss = 295527387.20770198\n",
      "Iteration 7, loss = 290687485.58481580\n",
      "Iteration 8, loss = 287015027.94839406\n",
      "Iteration 9, loss = 284247120.51783520\n",
      "Iteration 10, loss = 282273490.04502112\n",
      "Iteration 11, loss = 280714119.37893242\n",
      "Iteration 12, loss = 279656727.60132188\n",
      "Iteration 13, loss = 278836932.52197748\n",
      "Iteration 14, loss = 278291918.79905134\n",
      "Iteration 15, loss = 277879410.31361949\n",
      "Iteration 16, loss = 277596364.58307797\n",
      "Iteration 17, loss = 277405761.08362854\n",
      "Iteration 18, loss = 277428938.14125878\n",
      "Iteration 19, loss = 277282683.97771806\n",
      "Iteration 20, loss = 277167363.40958697\n",
      "Iteration 21, loss = 277193342.11230350\n",
      "Iteration 22, loss = 277144814.95929009\n",
      "Iteration 23, loss = 277113459.24601144\n",
      "Iteration 24, loss = 277062884.30263197\n",
      "Iteration 25, loss = 277260854.90489471\n",
      "Iteration 26, loss = 277166536.25902510\n",
      "Iteration 27, loss = 277050403.66334075\n",
      "Iteration 28, loss = 277110484.46785343\n",
      "Iteration 29, loss = 277159240.90085149\n",
      "Iteration 30, loss = 277169545.57723129\n",
      "Iteration 31, loss = 277099985.98424286\n",
      "Iteration 32, loss = 277109663.96147323\n",
      "Iteration 33, loss = 277190953.34094667\n",
      "Iteration 34, loss = 277181171.99461967\n",
      "Iteration 35, loss = 277084166.55914885\n",
      "Iteration 36, loss = 277196966.05359679\n",
      "Iteration 37, loss = 277108380.68850940\n",
      "Iteration 38, loss = 277097041.46389592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1577573343.95549369\n",
      "Iteration 2, loss = 854511787.54253542\n",
      "Iteration 3, loss = 478503224.37279707\n",
      "Iteration 4, loss = 292207836.03844494\n",
      "Iteration 5, loss = 210334260.04134914\n",
      "Iteration 6, loss = 179620244.67150316\n",
      "Iteration 7, loss = 170133353.61487457\n",
      "Iteration 8, loss = 167724035.09469765\n",
      "Iteration 9, loss = 167168465.32482213\n",
      "Iteration 10, loss = 166983940.84034675\n",
      "Iteration 11, loss = 166891428.43380821\n",
      "Iteration 12, loss = 166814115.45444581\n",
      "Iteration 13, loss = 166747666.82740468\n",
      "Iteration 14, loss = 166683305.11768842\n",
      "Iteration 15, loss = 166601493.60304919\n",
      "Iteration 16, loss = 166543556.47946504\n",
      "Iteration 17, loss = 166472047.24730328\n",
      "Iteration 18, loss = 166427493.27985200\n",
      "Iteration 19, loss = 166370755.70485383\n",
      "Iteration 20, loss = 166335975.54180476\n",
      "Iteration 21, loss = 166265838.89937085\n",
      "Iteration 22, loss = 166180915.47247496\n",
      "Iteration 23, loss = 166086159.42507860\n",
      "Iteration 24, loss = 166057134.87419954\n",
      "Iteration 25, loss = 166028797.59606260\n",
      "Iteration 26, loss = 166029409.58979410\n",
      "Iteration 27, loss = 165970306.96023816\n",
      "Iteration 28, loss = 166093623.05893880\n",
      "Iteration 29, loss = 165970720.21850097\n",
      "Iteration 30, loss = 165924876.62726301\n",
      "Iteration 31, loss = 165929111.30872864\n",
      "Iteration 32, loss = 165925784.60999152\n",
      "Iteration 33, loss = 165928187.82279217\n",
      "Iteration 34, loss = 165907690.18255353\n",
      "Iteration 35, loss = 165919231.15600881\n",
      "Iteration 36, loss = 165908156.17069054\n",
      "Iteration 37, loss = 165885957.76665139\n",
      "Iteration 38, loss = 165879116.59360343\n",
      "Iteration 39, loss = 165870537.19605848\n",
      "Iteration 40, loss = 165877676.70059556\n",
      "Iteration 41, loss = 165869535.82905555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 165882148.69203678\n",
      "Iteration 43, loss = 165873296.97598410\n",
      "Iteration 44, loss = 165862226.39422965\n",
      "Iteration 45, loss = 165856891.47204742\n",
      "Iteration 46, loss = 165846260.81606576\n",
      "Iteration 47, loss = 165929566.87253395\n",
      "Iteration 48, loss = 165966890.97451738\n",
      "Iteration 49, loss = 165859922.72171620\n",
      "Iteration 50, loss = 165864681.38802010\n",
      "Iteration 51, loss = 165855148.97910121\n",
      "Iteration 52, loss = 165845384.26308492\n",
      "Iteration 53, loss = 165829230.95356783\n",
      "Iteration 54, loss = 165837222.62691915\n",
      "Iteration 55, loss = 165842129.18805730\n",
      "Iteration 56, loss = 165799068.00483066\n",
      "Iteration 57, loss = 165944052.89605948\n",
      "Iteration 58, loss = 165829826.47515860\n",
      "Iteration 59, loss = 165807419.16921923\n",
      "Iteration 60, loss = 165838475.04859793\n",
      "Iteration 61, loss = 165786794.93159112\n",
      "Iteration 62, loss = 165933640.82522494\n",
      "Iteration 63, loss = 165855183.86078212\n",
      "Iteration 64, loss = 165838839.86070287\n",
      "Iteration 65, loss = 165816021.78947997\n",
      "Iteration 66, loss = 165836020.11292657\n",
      "Iteration 67, loss = 165951899.77336991\n",
      "Iteration 68, loss = 165785786.44147167\n",
      "Iteration 69, loss = 165880614.43600073\n",
      "Iteration 70, loss = 165824471.42127487\n",
      "Iteration 71, loss = 165772830.26045394\n",
      "Iteration 72, loss = 165839472.05820790\n",
      "Iteration 73, loss = 165778771.09799275\n",
      "Iteration 74, loss = 165765466.03214067\n",
      "Iteration 75, loss = 165770722.21000245\n",
      "Iteration 76, loss = 165756479.01399586\n",
      "Iteration 77, loss = 165754741.90636188\n",
      "Iteration 78, loss = 165787091.35061178\n",
      "Iteration 79, loss = 165747127.65069064\n",
      "Iteration 80, loss = 165766791.18057692\n",
      "Iteration 81, loss = 165744501.00402296\n",
      "Iteration 82, loss = 165783418.86343244\n",
      "Iteration 83, loss = 165728792.71329063\n",
      "Iteration 84, loss = 165788239.40564278\n",
      "Iteration 85, loss = 165805135.88354957\n",
      "Iteration 86, loss = 165727881.51920679\n",
      "Iteration 87, loss = 165822496.33638242\n",
      "Iteration 88, loss = 165795912.98109052\n",
      "Iteration 89, loss = 165686240.83077201\n",
      "Iteration 90, loss = 165697250.33457544\n",
      "Iteration 91, loss = 165762612.31910926\n",
      "Iteration 92, loss = 165683850.49621585\n",
      "Iteration 93, loss = 165683923.54937890\n",
      "Iteration 94, loss = 165705962.17043930\n",
      "Iteration 95, loss = 165875347.84646276\n",
      "Iteration 96, loss = 165733989.77857140\n",
      "Iteration 97, loss = 165758747.79677364\n",
      "Iteration 98, loss = 165671942.31128842\n",
      "Iteration 99, loss = 165703112.51282176\n",
      "Iteration 100, loss = 165741368.13122487\n",
      "Iteration 101, loss = 165771185.29026613\n",
      "Iteration 102, loss = 165724961.67583317\n",
      "Iteration 103, loss = 165635902.52395076\n",
      "Iteration 104, loss = 165703441.37370154\n",
      "Iteration 105, loss = 165619170.60052180\n",
      "Iteration 106, loss = 165649790.32325777\n",
      "Iteration 107, loss = 165680749.58021113\n",
      "Iteration 108, loss = 165663872.48175043\n",
      "Iteration 109, loss = 165812347.43012518\n",
      "Iteration 110, loss = 165678368.68509743\n",
      "Iteration 111, loss = 165613753.76303574\n",
      "Iteration 112, loss = 165725538.56565198\n",
      "Iteration 113, loss = 165648614.60596853\n",
      "Iteration 114, loss = 165573742.49455699\n",
      "Iteration 115, loss = 165583197.57363120\n",
      "Iteration 116, loss = 165607387.28563800\n",
      "Iteration 117, loss = 165540630.21801227\n",
      "Iteration 118, loss = 165621615.38406637\n",
      "Iteration 119, loss = 165566846.65124041\n",
      "Iteration 120, loss = 165591855.60668996\n",
      "Iteration 121, loss = 165521490.96097901\n",
      "Iteration 122, loss = 165760073.12550429\n",
      "Iteration 123, loss = 165581934.50187999\n",
      "Iteration 124, loss = 165587611.48207045\n",
      "Iteration 125, loss = 165591498.75336936\n",
      "Iteration 126, loss = 165659715.71819279\n",
      "Iteration 127, loss = 165488924.60705802\n",
      "Iteration 128, loss = 165598481.57933962\n",
      "Iteration 129, loss = 165507303.46430328\n",
      "Iteration 130, loss = 165541274.64755294\n",
      "Iteration 131, loss = 165587893.03341937\n",
      "Iteration 132, loss = 165594041.35019198\n",
      "Iteration 133, loss = 165552352.50991297\n",
      "Iteration 134, loss = 165569788.86405709\n",
      "Iteration 135, loss = 165516347.60241234\n",
      "Iteration 136, loss = 165700011.30149931\n",
      "Iteration 137, loss = 165538620.06098863\n",
      "Iteration 138, loss = 165436404.91450927\n",
      "Iteration 139, loss = 165514396.57800972\n",
      "Iteration 140, loss = 165501744.98500806\n",
      "Iteration 141, loss = 165403609.78137064\n",
      "Iteration 142, loss = 165472053.34430206\n",
      "Iteration 143, loss = 165487253.70850575\n",
      "Iteration 144, loss = 165438541.55276468\n",
      "Iteration 145, loss = 165516462.28103700\n",
      "Iteration 146, loss = 165438471.71438882\n",
      "Iteration 147, loss = 165530672.05977985\n",
      "Iteration 148, loss = 165425810.49802983\n",
      "Iteration 149, loss = 165424007.66185883\n",
      "Iteration 150, loss = 165435925.45751244\n",
      "Iteration 151, loss = 165413546.73449883\n",
      "Iteration 152, loss = 165335407.98270106\n",
      "Iteration 153, loss = 165504574.78027305\n",
      "Iteration 154, loss = 165400057.13324752\n",
      "Iteration 155, loss = 165333837.30735767\n",
      "Iteration 156, loss = 165316224.74081877\n",
      "Iteration 157, loss = 165327668.08569449\n",
      "Iteration 158, loss = 165352147.25072673\n",
      "Iteration 159, loss = 165259190.63580686\n",
      "Iteration 160, loss = 165327534.83751577\n",
      "Iteration 161, loss = 165551728.72884345\n",
      "Iteration 162, loss = 165419066.51571417\n",
      "Iteration 163, loss = 165345826.52438498\n",
      "Iteration 164, loss = 165269963.17292613\n",
      "Iteration 165, loss = 165370739.46199900\n",
      "Iteration 166, loss = 165369170.58891949\n",
      "Iteration 167, loss = 165336501.19235978\n",
      "Iteration 168, loss = 165178456.07929134\n",
      "Iteration 169, loss = 165174986.19259048\n",
      "Iteration 170, loss = 165211321.87823108\n",
      "Iteration 171, loss = 165227893.79782224\n",
      "Iteration 172, loss = 165476034.46963683\n",
      "Iteration 173, loss = 165297931.47965622\n",
      "Iteration 174, loss = 165229504.29149395\n",
      "Iteration 175, loss = 165196202.75793657\n",
      "Iteration 176, loss = 165451919.00536913\n",
      "Iteration 177, loss = 165208919.31572732\n",
      "Iteration 178, loss = 165327484.32546186\n",
      "Iteration 179, loss = 165139001.00486434\n",
      "Iteration 180, loss = 165080578.63472179\n",
      "Iteration 181, loss = 165253404.88877651\n",
      "Iteration 182, loss = 165111460.53300506\n",
      "Iteration 183, loss = 165182092.59444007\n",
      "Iteration 184, loss = 165127306.90100291\n",
      "Iteration 185, loss = 165136271.50982913\n",
      "Iteration 186, loss = 165073025.39523941\n",
      "Iteration 187, loss = 165110923.50532559\n",
      "Iteration 188, loss = 165231667.67625856\n",
      "Iteration 189, loss = 165163742.31800583\n",
      "Iteration 190, loss = 165191041.89743200\n",
      "Iteration 191, loss = 165078881.94234061\n",
      "Iteration 192, loss = 165113228.44540030\n",
      "Iteration 193, loss = 165085705.58940154\n",
      "Iteration 194, loss = 165159109.62437353\n",
      "Iteration 195, loss = 165183333.66904625\n",
      "Iteration 196, loss = 165088401.65421623\n",
      "Iteration 197, loss = 165122762.42023826\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 227141912.20124149\n",
      "Iteration 2, loss = 223095388.57081532\n",
      "Iteration 3, loss = 222448583.47977427\n",
      "Iteration 4, loss = 222190676.59512484\n",
      "Iteration 5, loss = 222051248.73614118\n",
      "Iteration 6, loss = 221964073.71620318\n",
      "Iteration 7, loss = 221926188.90498495\n",
      "Iteration 8, loss = 222003198.12010923\n",
      "Iteration 9, loss = 222032420.75203413\n",
      "Iteration 10, loss = 221941516.78908876\n",
      "Iteration 11, loss = 221861991.99444902\n",
      "Iteration 12, loss = 221811004.44838437\n",
      "Iteration 13, loss = 221928398.80918798\n",
      "Iteration 14, loss = 221803627.62626171\n",
      "Iteration 15, loss = 221799358.03687963\n",
      "Iteration 16, loss = 221794007.21085396\n",
      "Iteration 17, loss = 221787570.14772543\n",
      "Iteration 18, loss = 221819809.96645245\n",
      "Iteration 19, loss = 221848499.64926729\n",
      "Iteration 20, loss = 221778650.39531639\n",
      "Iteration 21, loss = 221869029.41277358\n",
      "Iteration 22, loss = 221786566.14699715\n",
      "Iteration 23, loss = 221791782.66313452\n",
      "Iteration 24, loss = 221874959.30653578\n",
      "Iteration 25, loss = 221854756.97459161\n",
      "Iteration 26, loss = 221915093.78900284\n",
      "Iteration 27, loss = 221702784.86920631\n",
      "Iteration 28, loss = 221750062.82748041\n",
      "Iteration 29, loss = 221784416.41475537\n",
      "Iteration 30, loss = 221676965.97000918\n",
      "Iteration 31, loss = 221875981.26751825\n",
      "Iteration 32, loss = 221786374.94550234\n",
      "Iteration 33, loss = 221689769.56419498\n",
      "Iteration 34, loss = 221801733.59438688\n",
      "Iteration 35, loss = 221731690.12326300\n",
      "Iteration 36, loss = 221658593.15815538\n",
      "Iteration 37, loss = 221702762.03345552\n",
      "Iteration 38, loss = 221685076.96523812\n",
      "Iteration 39, loss = 221668057.42349747\n",
      "Iteration 40, loss = 221653754.84054336\n",
      "Iteration 41, loss = 221753304.24569494\n",
      "Iteration 42, loss = 221651366.70415074\n",
      "Iteration 43, loss = 221724490.22695220\n",
      "Iteration 44, loss = 221730455.03661636\n",
      "Iteration 45, loss = 221883838.59852284\n",
      "Iteration 46, loss = 221628810.85389298\n",
      "Iteration 47, loss = 221635467.57541049\n",
      "Iteration 48, loss = 221766922.80333653\n",
      "Iteration 49, loss = 221651927.89533433\n",
      "Iteration 50, loss = 221663261.78765243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51, loss = 221586359.76819974\n",
      "Iteration 52, loss = 221665263.96339116\n",
      "Iteration 53, loss = 221584875.74778271\n",
      "Iteration 54, loss = 221585123.50155273\n",
      "Iteration 55, loss = 221579779.29986039\n",
      "Iteration 56, loss = 221616830.63441327\n",
      "Iteration 57, loss = 221622733.32372129\n",
      "Iteration 58, loss = 221570135.72194174\n",
      "Iteration 59, loss = 221654087.88303301\n",
      "Iteration 60, loss = 221579843.57057446\n",
      "Iteration 61, loss = 221548842.51792085\n",
      "Iteration 62, loss = 221537627.86373273\n",
      "Iteration 63, loss = 221522228.11117148\n",
      "Iteration 64, loss = 221642490.11520541\n",
      "Iteration 65, loss = 221624180.13867128\n",
      "Iteration 66, loss = 221585787.70166454\n",
      "Iteration 67, loss = 221592948.34450445\n",
      "Iteration 68, loss = 221468130.56840819\n",
      "Iteration 69, loss = 221540945.70273200\n",
      "Iteration 70, loss = 221516953.90286186\n",
      "Iteration 71, loss = 221435580.33539146\n",
      "Iteration 72, loss = 221689061.79707232\n",
      "Iteration 73, loss = 221502002.31815836\n",
      "Iteration 74, loss = 221544009.72221518\n",
      "Iteration 75, loss = 221440401.93320268\n",
      "Iteration 76, loss = 221473969.23859394\n",
      "Iteration 77, loss = 221544719.30591395\n",
      "Iteration 78, loss = 221572020.34010395\n",
      "Iteration 79, loss = 221472200.36649302\n",
      "Iteration 80, loss = 221483776.58604097\n",
      "Iteration 81, loss = 221423881.27716425\n",
      "Iteration 82, loss = 221567335.74630919\n",
      "Iteration 83, loss = 221513231.23480123\n",
      "Iteration 84, loss = 221390805.88787788\n",
      "Iteration 85, loss = 221407571.97453940\n",
      "Iteration 86, loss = 221556656.65919131\n",
      "Iteration 87, loss = 221390003.33874887\n",
      "Iteration 88, loss = 221331658.84634107\n",
      "Iteration 89, loss = 221618594.79597872\n",
      "Iteration 90, loss = 221446690.98827031\n",
      "Iteration 91, loss = 221460223.80574101\n",
      "Iteration 92, loss = 221307948.71848443\n",
      "Iteration 93, loss = 221346390.45116484\n",
      "Iteration 94, loss = 221334870.60080180\n",
      "Iteration 95, loss = 221296490.00038639\n",
      "Iteration 96, loss = 221361155.26299649\n",
      "Iteration 97, loss = 221353978.68934634\n",
      "Iteration 98, loss = 221322543.29571915\n",
      "Iteration 99, loss = 221389942.78134108\n",
      "Iteration 100, loss = 221213718.06985158\n",
      "Iteration 101, loss = 221300564.11389497\n",
      "Iteration 102, loss = 221239918.47970864\n",
      "Iteration 103, loss = 221241697.15347803\n",
      "Iteration 104, loss = 221210694.15059230\n",
      "Iteration 105, loss = 221405175.16875470\n",
      "Iteration 106, loss = 221409872.42338949\n",
      "Iteration 107, loss = 221242199.84664911\n",
      "Iteration 108, loss = 221301746.24616727\n",
      "Iteration 109, loss = 221190281.12389526\n",
      "Iteration 110, loss = 221173929.92861313\n",
      "Iteration 111, loss = 221206128.30117500\n",
      "Iteration 112, loss = 221232173.30534199\n",
      "Iteration 113, loss = 221293409.59305644\n",
      "Iteration 114, loss = 221189972.05230683\n",
      "Iteration 115, loss = 221253932.74716884\n",
      "Iteration 116, loss = 221115133.19744888\n",
      "Iteration 117, loss = 221167298.43511078\n",
      "Iteration 118, loss = 221139959.06139770\n",
      "Iteration 119, loss = 221206241.69915128\n",
      "Iteration 120, loss = 221158898.11883569\n",
      "Iteration 121, loss = 221146276.22894776\n",
      "Iteration 122, loss = 221136466.25143829\n",
      "Iteration 123, loss = 221158664.73294023\n",
      "Iteration 124, loss = 221044169.57253349\n",
      "Iteration 125, loss = 221051647.39788789\n",
      "Iteration 126, loss = 221177195.54659718\n",
      "Iteration 127, loss = 221072052.75376567\n",
      "Iteration 128, loss = 221078323.02103946\n",
      "Iteration 129, loss = 221042531.15201035\n",
      "Iteration 130, loss = 221021151.88444164\n",
      "Iteration 131, loss = 221027127.39100865\n",
      "Iteration 132, loss = 221103842.04758799\n",
      "Iteration 133, loss = 221079830.24818707\n",
      "Iteration 134, loss = 220984020.64129013\n",
      "Iteration 135, loss = 220953385.33694437\n",
      "Iteration 136, loss = 221088677.22605118\n",
      "Iteration 137, loss = 220997422.33149859\n",
      "Iteration 138, loss = 220879391.05564898\n",
      "Iteration 139, loss = 220957032.80138856\n",
      "Iteration 140, loss = 221179423.41867921\n",
      "Iteration 141, loss = 221106144.37174067\n",
      "Iteration 142, loss = 220928760.07706466\n",
      "Iteration 143, loss = 220997545.00285390\n",
      "Iteration 144, loss = 220924039.91391161\n",
      "Iteration 145, loss = 220934344.89351860\n",
      "Iteration 146, loss = 220844845.04885778\n",
      "Iteration 147, loss = 220918230.53965953\n",
      "Iteration 148, loss = 220810862.58764961\n",
      "Iteration 149, loss = 220963295.34265733\n",
      "Iteration 150, loss = 220884899.87989843\n",
      "Iteration 151, loss = 220949550.83875760\n",
      "Iteration 152, loss = 220810350.74541372\n",
      "Iteration 153, loss = 220777251.12416515\n",
      "Iteration 154, loss = 220834582.29725561\n",
      "Iteration 155, loss = 220918339.93722895\n",
      "Iteration 156, loss = 220986563.39870107\n",
      "Iteration 157, loss = 220745092.20056245\n",
      "Iteration 158, loss = 220861481.00028330\n",
      "Iteration 159, loss = 220664263.52982408\n",
      "Iteration 160, loss = 220844709.04829600\n",
      "Iteration 161, loss = 220768961.41229889\n",
      "Iteration 162, loss = 220664279.54865414\n",
      "Iteration 163, loss = 220726334.64902881\n",
      "Iteration 164, loss = 220722058.10864136\n",
      "Iteration 165, loss = 220890037.19414985\n",
      "Iteration 166, loss = 220650851.52878082\n",
      "Iteration 167, loss = 220610085.97388399\n",
      "Iteration 168, loss = 220618563.77560523\n",
      "Iteration 169, loss = 220601805.60884228\n",
      "Iteration 170, loss = 220645271.88224462\n",
      "Iteration 171, loss = 220622383.83767095\n",
      "Iteration 172, loss = 220540982.04755571\n",
      "Iteration 173, loss = 220685695.46557924\n",
      "Iteration 174, loss = 220613086.34365204\n",
      "Iteration 175, loss = 220591618.67424989\n",
      "Iteration 176, loss = 220699122.83326724\n",
      "Iteration 177, loss = 220524021.83687270\n",
      "Iteration 178, loss = 220536999.73300776\n",
      "Iteration 179, loss = 220486340.94937417\n",
      "Iteration 180, loss = 220448721.46120480\n",
      "Iteration 181, loss = 220527610.91195548\n",
      "Iteration 182, loss = 220556179.80227125\n",
      "Iteration 183, loss = 220570839.97335243\n",
      "Iteration 184, loss = 220440976.51347187\n",
      "Iteration 185, loss = 220431737.38077953\n",
      "Iteration 186, loss = 220463333.91358471\n",
      "Iteration 187, loss = 220362650.75727099\n",
      "Iteration 188, loss = 220337836.10825136\n",
      "Iteration 189, loss = 220319818.19374487\n",
      "Iteration 190, loss = 220314532.00172397\n",
      "Iteration 191, loss = 220318073.22455499\n",
      "Iteration 192, loss = 220407064.60294673\n",
      "Iteration 193, loss = 220345456.47278818\n",
      "Iteration 194, loss = 220396724.46287623\n",
      "Iteration 195, loss = 220289976.91819337\n",
      "Iteration 196, loss = 220423025.80767801\n",
      "Iteration 197, loss = 220250624.18592617\n",
      "Iteration 198, loss = 220365559.14765170\n",
      "Iteration 199, loss = 220245510.25599986\n",
      "Iteration 200, loss = 220188456.18175420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 190925154.36357683\n",
      "Iteration 2, loss = 189246507.58549809\n",
      "Iteration 3, loss = 188955397.43652886\n",
      "Iteration 4, loss = 188849743.04863009\n",
      "Iteration 5, loss = 188780507.39876893\n",
      "Iteration 6, loss = 188698644.40051025\n",
      "Iteration 7, loss = 188642894.05095255\n",
      "Iteration 8, loss = 188659820.73031530\n",
      "Iteration 9, loss = 188766356.67808741\n",
      "Iteration 10, loss = 188691148.96073309\n",
      "Iteration 11, loss = 188843591.89428911\n",
      "Iteration 12, loss = 188764478.51165789\n",
      "Iteration 13, loss = 188467532.73395965\n",
      "Iteration 14, loss = 188679972.12384593\n",
      "Iteration 15, loss = 188613971.44091198\n",
      "Iteration 16, loss = 188627742.22859982\n",
      "Iteration 17, loss = 188548803.57740250\n",
      "Iteration 18, loss = 188662818.28470013\n",
      "Iteration 19, loss = 188674910.35032099\n",
      "Iteration 20, loss = 188539534.02244788\n",
      "Iteration 21, loss = 188568400.18022212\n",
      "Iteration 22, loss = 188532581.35303366\n",
      "Iteration 23, loss = 188535394.71579874\n",
      "Iteration 24, loss = 188585620.92211628\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 229172015.75977001\n",
      "Iteration 2, loss = 169848968.82837138\n",
      "Iteration 3, loss = 163409275.38840628\n",
      "Iteration 4, loss = 162809346.09423891\n",
      "Iteration 5, loss = 162580220.65775713\n",
      "Iteration 6, loss = 162395988.60230282\n",
      "Iteration 7, loss = 162103401.07853734\n",
      "Iteration 8, loss = 161881168.96665502\n",
      "Iteration 9, loss = 161659803.54139420\n",
      "Iteration 10, loss = 161677993.90225127\n",
      "Iteration 11, loss = 161398631.17307311\n",
      "Iteration 12, loss = 161149088.44445482\n",
      "Iteration 13, loss = 161286421.73088607\n",
      "Iteration 14, loss = 161076674.64264247\n",
      "Iteration 15, loss = 160868812.49075618\n",
      "Iteration 16, loss = 160802919.48402786\n",
      "Iteration 17, loss = 160764605.76512420\n",
      "Iteration 18, loss = 160748690.75172624\n",
      "Iteration 19, loss = 160687376.69188741\n",
      "Iteration 20, loss = 160489563.21204543\n",
      "Iteration 21, loss = 160557138.71415961\n",
      "Iteration 22, loss = 160292029.60362619\n",
      "Iteration 23, loss = 160420310.36376089\n",
      "Iteration 24, loss = 160191675.62010214\n",
      "Iteration 25, loss = 160040152.68657431\n",
      "Iteration 26, loss = 159971460.96423158\n",
      "Iteration 27, loss = 159880371.28900552\n",
      "Iteration 28, loss = 159875485.02320516\n",
      "Iteration 29, loss = 159830762.84392571\n",
      "Iteration 30, loss = 159726895.97769761\n",
      "Iteration 31, loss = 159685822.62891179\n",
      "Iteration 32, loss = 159509954.00088033\n",
      "Iteration 33, loss = 159532523.99226344\n",
      "Iteration 34, loss = 159278443.89797091\n",
      "Iteration 35, loss = 159137704.33589360\n",
      "Iteration 36, loss = 159028955.67586246\n",
      "Iteration 37, loss = 159061597.20236644\n",
      "Iteration 38, loss = 158993481.14804584\n",
      "Iteration 39, loss = 158799776.52470252\n",
      "Iteration 40, loss = 158716602.14549413\n",
      "Iteration 41, loss = 158511936.63299099\n",
      "Iteration 42, loss = 158452665.04637241\n",
      "Iteration 43, loss = 158268362.60907516\n",
      "Iteration 44, loss = 158099893.93187550\n",
      "Iteration 45, loss = 158058169.91289899\n",
      "Iteration 46, loss = 158098832.71340832\n",
      "Iteration 47, loss = 157761994.63094306\n",
      "Iteration 48, loss = 157643227.89875790\n",
      "Iteration 49, loss = 157510943.35078889\n",
      "Iteration 50, loss = 157392151.06847304\n",
      "Iteration 51, loss = 157382234.56099200\n",
      "Iteration 52, loss = 157134886.76719350\n",
      "Iteration 53, loss = 157044097.59319988\n",
      "Iteration 54, loss = 156859006.54684091\n",
      "Iteration 55, loss = 156994053.12366873\n",
      "Iteration 56, loss = 156724215.58249059\n",
      "Iteration 57, loss = 156530297.87661093\n",
      "Iteration 58, loss = 156448687.79124618\n",
      "Iteration 59, loss = 156386221.56210494\n",
      "Iteration 60, loss = 156218662.90330216\n",
      "Iteration 61, loss = 156220720.05800593\n",
      "Iteration 62, loss = 155938603.36179268\n",
      "Iteration 63, loss = 155919084.85063154\n",
      "Iteration 64, loss = 155670611.80493790\n",
      "Iteration 65, loss = 155810314.99003911\n",
      "Iteration 66, loss = 155498359.60299605\n",
      "Iteration 67, loss = 155491150.62823057\n",
      "Iteration 68, loss = 155205175.80258334\n",
      "Iteration 69, loss = 154825394.25524458\n",
      "Iteration 70, loss = 154714321.28896815\n",
      "Iteration 71, loss = 155165083.12705967\n",
      "Iteration 72, loss = 155691223.06174874\n",
      "Iteration 73, loss = 154530056.01104122\n",
      "Iteration 74, loss = 154282628.64071554\n",
      "Iteration 75, loss = 154237059.72500762\n",
      "Iteration 76, loss = 154009594.68894431\n",
      "Iteration 77, loss = 154178562.38562122\n",
      "Iteration 78, loss = 154031133.24444735\n",
      "Iteration 79, loss = 154423874.15335923\n",
      "Iteration 80, loss = 154071778.04527158\n",
      "Iteration 81, loss = 153967424.31302410\n",
      "Iteration 82, loss = 154050552.02687153\n",
      "Iteration 83, loss = 153643512.33039120\n",
      "Iteration 84, loss = 153767770.81488067\n",
      "Iteration 85, loss = 153552346.83065557\n",
      "Iteration 86, loss = 153763855.37447137\n",
      "Iteration 87, loss = 153415884.37611854\n",
      "Iteration 88, loss = 153322351.47271022\n",
      "Iteration 89, loss = 153238726.57520097\n",
      "Iteration 90, loss = 153387498.53338242\n",
      "Iteration 91, loss = 153120427.30810592\n",
      "Iteration 92, loss = 152925538.01053533\n",
      "Iteration 93, loss = 153485226.44748664\n",
      "Iteration 94, loss = 152995278.63637424\n",
      "Iteration 95, loss = 153056095.93883291\n",
      "Iteration 96, loss = 153133264.59739029\n",
      "Iteration 97, loss = 152996075.28455353\n",
      "Iteration 98, loss = 152772495.13747379\n",
      "Iteration 99, loss = 152790859.02132148\n",
      "Iteration 100, loss = 152869775.00213376\n",
      "Iteration 101, loss = 152869069.46670368\n",
      "Iteration 102, loss = 152844258.04076689\n",
      "Iteration 103, loss = 152557111.89447355\n",
      "Iteration 104, loss = 152658766.47001186\n",
      "Iteration 105, loss = 152534209.95631358\n",
      "Iteration 106, loss = 152496245.73783690\n",
      "Iteration 107, loss = 152513751.35686398\n",
      "Iteration 108, loss = 153014301.77924201\n",
      "Iteration 109, loss = 152541548.08234629\n",
      "Iteration 110, loss = 152455153.72868791\n",
      "Iteration 111, loss = 152272951.34894413\n",
      "Iteration 112, loss = 153009126.62465352\n",
      "Iteration 113, loss = 152560195.98425829\n",
      "Iteration 114, loss = 152622228.97298673\n",
      "Iteration 115, loss = 152160550.86040601\n",
      "Iteration 116, loss = 152365116.23726726\n",
      "Iteration 117, loss = 152105321.05980223\n",
      "Iteration 118, loss = 152185923.83348757\n",
      "Iteration 119, loss = 152232015.39131352\n",
      "Iteration 120, loss = 152409366.58443719\n",
      "Iteration 121, loss = 152032448.76873699\n",
      "Iteration 122, loss = 152050868.00243506\n",
      "Iteration 123, loss = 152434469.80123192\n",
      "Iteration 124, loss = 151716684.82942420\n",
      "Iteration 125, loss = 151955600.53627288\n",
      "Iteration 126, loss = 152987726.31999475\n",
      "Iteration 127, loss = 152130513.70305419\n",
      "Iteration 128, loss = 151976608.82522485\n",
      "Iteration 129, loss = 151784834.41912982\n",
      "Iteration 130, loss = 151973564.14757836\n",
      "Iteration 131, loss = 151792966.72703317\n",
      "Iteration 132, loss = 151974933.09407550\n",
      "Iteration 133, loss = 151815973.27336776\n",
      "Iteration 134, loss = 151974714.51227006\n",
      "Iteration 135, loss = 151726113.77507088\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 711028139.35392213\n",
      "Iteration 2, loss = 322933893.64724100\n",
      "Iteration 3, loss = 213915956.03070799\n",
      "Iteration 4, loss = 197959845.50591210\n",
      "Iteration 5, loss = 196600915.13435587\n",
      "Iteration 6, loss = 196198405.73692676\n",
      "Iteration 7, loss = 195842861.69730645\n",
      "Iteration 8, loss = 195508066.32136542\n",
      "Iteration 9, loss = 195212050.06740606\n",
      "Iteration 10, loss = 194972903.01951239\n",
      "Iteration 11, loss = 194735669.46192402\n",
      "Iteration 12, loss = 194540367.98320046\n",
      "Iteration 13, loss = 194401125.69948933\n",
      "Iteration 14, loss = 194241759.56416070\n",
      "Iteration 15, loss = 194137258.82481346\n",
      "Iteration 16, loss = 194013598.72483987\n",
      "Iteration 17, loss = 194027146.36833450\n",
      "Iteration 18, loss = 193910425.80222401\n",
      "Iteration 19, loss = 193745835.16808048\n",
      "Iteration 20, loss = 193707673.77424267\n",
      "Iteration 21, loss = 193628274.98091304\n",
      "Iteration 22, loss = 193529636.51084596\n",
      "Iteration 23, loss = 193452446.05386817\n",
      "Iteration 24, loss = 193405703.69521895\n",
      "Iteration 25, loss = 193437317.99717838\n",
      "Iteration 26, loss = 193301947.11661074\n",
      "Iteration 27, loss = 193205759.92580074\n",
      "Iteration 28, loss = 193175563.10884181\n",
      "Iteration 29, loss = 193103424.72027522\n",
      "Iteration 30, loss = 192955622.31099290\n",
      "Iteration 31, loss = 193189523.14873835\n",
      "Iteration 32, loss = 192860920.85976884\n",
      "Iteration 33, loss = 192897240.10705587\n",
      "Iteration 34, loss = 192726650.75928876\n",
      "Iteration 35, loss = 192642451.54303002\n",
      "Iteration 36, loss = 192560136.82611719\n",
      "Iteration 37, loss = 192574850.67471051\n",
      "Iteration 38, loss = 192539456.66520497\n",
      "Iteration 39, loss = 192312210.99229425\n",
      "Iteration 40, loss = 192316407.53888080\n",
      "Iteration 41, loss = 192372853.49578819\n",
      "Iteration 42, loss = 192183676.87456128\n",
      "Iteration 43, loss = 192134100.77046320\n",
      "Iteration 44, loss = 191982953.69751877\n",
      "Iteration 45, loss = 191842761.30189139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 191850925.73473433\n",
      "Iteration 47, loss = 191777832.42379034\n",
      "Iteration 48, loss = 191769475.88187510\n",
      "Iteration 49, loss = 191638902.07618421\n",
      "Iteration 50, loss = 191533767.26985800\n",
      "Iteration 51, loss = 191470221.94088042\n",
      "Iteration 52, loss = 191324725.57050943\n",
      "Iteration 53, loss = 191181055.96149418\n",
      "Iteration 54, loss = 191140351.60682428\n",
      "Iteration 55, loss = 191054816.22484750\n",
      "Iteration 56, loss = 191133634.36053050\n",
      "Iteration 57, loss = 191001848.62387177\n",
      "Iteration 58, loss = 190819436.84839007\n",
      "Iteration 59, loss = 190837586.70783833\n",
      "Iteration 60, loss = 190769880.43981686\n",
      "Iteration 61, loss = 190614876.09102824\n",
      "Iteration 62, loss = 190605059.62622425\n",
      "Iteration 63, loss = 190476455.39680257\n",
      "Iteration 64, loss = 190306377.06452689\n",
      "Iteration 65, loss = 190321759.22505772\n",
      "Iteration 66, loss = 190149884.84457034\n",
      "Iteration 67, loss = 189972610.93307269\n",
      "Iteration 68, loss = 189938798.76165980\n",
      "Iteration 69, loss = 189939069.00645587\n",
      "Iteration 70, loss = 189825177.65458456\n",
      "Iteration 71, loss = 189647531.69733381\n",
      "Iteration 72, loss = 189551347.83703458\n",
      "Iteration 73, loss = 189499206.08952254\n",
      "Iteration 74, loss = 189464701.86767715\n",
      "Iteration 75, loss = 189209639.73828301\n",
      "Iteration 76, loss = 189150600.18370789\n",
      "Iteration 77, loss = 188914155.55505100\n",
      "Iteration 78, loss = 189546785.33976507\n",
      "Iteration 79, loss = 188897598.35765716\n",
      "Iteration 80, loss = 188710575.59383371\n",
      "Iteration 81, loss = 188690600.89176178\n",
      "Iteration 82, loss = 188607607.59606054\n",
      "Iteration 83, loss = 188537047.40212405\n",
      "Iteration 84, loss = 188455935.76954755\n",
      "Iteration 85, loss = 188203220.37565106\n",
      "Iteration 86, loss = 188042339.34294567\n",
      "Iteration 87, loss = 187954862.96595833\n",
      "Iteration 88, loss = 187782350.57520631\n",
      "Iteration 89, loss = 187908614.53752211\n",
      "Iteration 90, loss = 188102798.57764924\n",
      "Iteration 91, loss = 187609576.41627139\n",
      "Iteration 92, loss = 187447287.79671639\n",
      "Iteration 93, loss = 187250480.61330089\n",
      "Iteration 94, loss = 187189977.90906268\n",
      "Iteration 95, loss = 187155427.87400767\n",
      "Iteration 96, loss = 187053254.93719512\n",
      "Iteration 97, loss = 186881273.62006050\n",
      "Iteration 98, loss = 186798779.86489537\n",
      "Iteration 99, loss = 186630616.93798873\n",
      "Iteration 100, loss = 186741200.81530002\n",
      "Iteration 101, loss = 186333120.23692086\n",
      "Iteration 102, loss = 186276591.14149556\n",
      "Iteration 103, loss = 186130760.96402910\n",
      "Iteration 104, loss = 185963169.79749927\n",
      "Iteration 105, loss = 185905466.11680976\n",
      "Iteration 106, loss = 185864049.74474522\n",
      "Iteration 107, loss = 185806423.15710968\n",
      "Iteration 108, loss = 185631968.83648941\n",
      "Iteration 109, loss = 185488474.60389555\n",
      "Iteration 110, loss = 185346467.25954849\n",
      "Iteration 111, loss = 185149610.21146899\n",
      "Iteration 112, loss = 185236327.11095861\n",
      "Iteration 113, loss = 184982602.17040771\n",
      "Iteration 114, loss = 184902077.29640949\n",
      "Iteration 115, loss = 184860595.62741008\n",
      "Iteration 116, loss = 184972847.79392412\n",
      "Iteration 117, loss = 184655552.49331552\n",
      "Iteration 118, loss = 184566650.10011715\n",
      "Iteration 119, loss = 184239737.49288824\n",
      "Iteration 120, loss = 184430489.44634590\n",
      "Iteration 121, loss = 184515441.44330248\n",
      "Iteration 122, loss = 184222412.97822171\n",
      "Iteration 123, loss = 183883331.88887396\n",
      "Iteration 124, loss = 183667454.79514331\n",
      "Iteration 125, loss = 183635094.83117607\n",
      "Iteration 126, loss = 183715147.39629957\n",
      "Iteration 127, loss = 183732589.34294617\n",
      "Iteration 128, loss = 183430981.92953885\n",
      "Iteration 129, loss = 183139562.99707618\n",
      "Iteration 130, loss = 183000803.55580074\n",
      "Iteration 131, loss = 182962721.48406339\n",
      "Iteration 132, loss = 182864634.44760564\n",
      "Iteration 133, loss = 182949244.39677611\n",
      "Iteration 134, loss = 182706312.06026143\n",
      "Iteration 135, loss = 182728950.96143731\n",
      "Iteration 136, loss = 182602582.62921301\n",
      "Iteration 137, loss = 182391152.81426257\n",
      "Iteration 138, loss = 182489261.74117211\n",
      "Iteration 139, loss = 182415176.69195876\n",
      "Iteration 140, loss = 182121648.50272444\n",
      "Iteration 141, loss = 182033161.36072519\n",
      "Iteration 142, loss = 181899162.51780850\n",
      "Iteration 143, loss = 182000395.13677305\n",
      "Iteration 144, loss = 181892697.04829466\n",
      "Iteration 145, loss = 181757174.21906373\n",
      "Iteration 146, loss = 181823028.30788800\n",
      "Iteration 147, loss = 181847975.29233947\n",
      "Iteration 148, loss = 181279416.87959066\n",
      "Iteration 149, loss = 181280037.88593906\n",
      "Iteration 150, loss = 181231101.09284818\n",
      "Iteration 151, loss = 181108462.65030876\n",
      "Iteration 152, loss = 181413766.89457750\n",
      "Iteration 153, loss = 180980643.62085584\n",
      "Iteration 154, loss = 181036328.70250812\n",
      "Iteration 155, loss = 181040948.26648402\n",
      "Iteration 156, loss = 180765936.37339830\n",
      "Iteration 157, loss = 180788083.71929902\n",
      "Iteration 158, loss = 180841872.68107522\n",
      "Iteration 159, loss = 180932787.11254483\n",
      "Iteration 160, loss = 180466266.88265294\n",
      "Iteration 161, loss = 180332807.19199097\n",
      "Iteration 162, loss = 180525978.17468029\n",
      "Iteration 163, loss = 180253665.03264260\n",
      "Iteration 164, loss = 180133552.49764329\n",
      "Iteration 165, loss = 180186202.82929009\n",
      "Iteration 166, loss = 180103313.13667727\n",
      "Iteration 167, loss = 180091829.17423698\n",
      "Iteration 168, loss = 180002089.82452771\n",
      "Iteration 169, loss = 180179004.19029161\n",
      "Iteration 170, loss = 179795756.89028871\n",
      "Iteration 171, loss = 179857300.55599293\n",
      "Iteration 172, loss = 179656932.33112389\n",
      "Iteration 173, loss = 179618036.62003216\n",
      "Iteration 174, loss = 179568160.86899975\n",
      "Iteration 175, loss = 179401071.25702721\n",
      "Iteration 176, loss = 180133892.81164584\n",
      "Iteration 177, loss = 179535429.36614788\n",
      "Iteration 178, loss = 179270547.74870294\n",
      "Iteration 179, loss = 179278617.85068980\n",
      "Iteration 180, loss = 179451760.85630530\n",
      "Iteration 181, loss = 179183489.26913923\n",
      "Iteration 182, loss = 179148778.48370835\n",
      "Iteration 183, loss = 178935715.71843475\n",
      "Iteration 184, loss = 178979705.43743917\n",
      "Iteration 185, loss = 178889271.23082942\n",
      "Iteration 186, loss = 178986746.85276017\n",
      "Iteration 187, loss = 178890071.68197000\n",
      "Iteration 188, loss = 178727285.43935490\n",
      "Iteration 189, loss = 178728708.30221698\n",
      "Iteration 190, loss = 178723511.44008410\n",
      "Iteration 191, loss = 178705201.80096659\n",
      "Iteration 192, loss = 178764231.30159444\n",
      "Iteration 193, loss = 178481209.51736602\n",
      "Iteration 194, loss = 178298156.20582759\n",
      "Iteration 195, loss = 178389936.76471186\n",
      "Iteration 196, loss = 178501352.33213365\n",
      "Iteration 197, loss = 178250381.96538353\n",
      "Iteration 198, loss = 178223316.67602968\n",
      "Iteration 199, loss = 178285687.65200645\n",
      "Iteration 200, loss = 178730613.61340633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  16273.161075601049\n",
      "MSE:  289636989.96278536\n",
      "RMSE:  17018.724686732123\n",
      "R2:  -136224.14219608117\n",
      "2 16273.161075601049\n",
      "(103416, 15) (23700, 15)\n",
      "Iteration 1, loss = 969988258.93758714\n",
      "Iteration 2, loss = 615045884.41903532\n",
      "Iteration 3, loss = 472674703.40889072\n",
      "Iteration 4, loss = 431639595.12084848\n",
      "Iteration 5, loss = 422447347.46105051\n",
      "Iteration 6, loss = 419891655.80505067\n",
      "Iteration 7, loss = 418350786.32643294\n",
      "Iteration 8, loss = 417249686.56867456\n",
      "Iteration 9, loss = 416602256.63370764\n",
      "Iteration 10, loss = 416171983.12998778\n",
      "Iteration 11, loss = 415950916.75529170\n",
      "Iteration 12, loss = 415971612.32311147\n",
      "Iteration 13, loss = 415895851.69055498\n",
      "Iteration 14, loss = 415693732.75107771\n",
      "Iteration 15, loss = 415602295.62657279\n",
      "Iteration 16, loss = 415682318.08372122\n",
      "Iteration 17, loss = 415661270.08861506\n",
      "Iteration 18, loss = 415551434.75442171\n",
      "Iteration 19, loss = 415629890.68465507\n",
      "Iteration 20, loss = 415596611.61209053\n",
      "Iteration 21, loss = 415614877.25223213\n",
      "Iteration 22, loss = 415645606.71314144\n",
      "Iteration 23, loss = 415538130.81431282\n",
      "Iteration 24, loss = 415731214.33409804\n",
      "Iteration 25, loss = 415442646.53284973\n",
      "Iteration 26, loss = 415498281.49198550\n",
      "Iteration 27, loss = 415583246.64617270\n",
      "Iteration 28, loss = 415516620.66321957\n",
      "Iteration 29, loss = 415616887.59896737\n",
      "Iteration 30, loss = 415543226.47817701\n",
      "Iteration 31, loss = 415458952.10727012\n",
      "Iteration 32, loss = 415558882.76946950\n",
      "Iteration 33, loss = 415461790.66277802\n",
      "Iteration 34, loss = 415637081.23115838\n",
      "Iteration 35, loss = 415462108.53121954\n",
      "Iteration 36, loss = 415435233.10177761\n",
      "Iteration 37, loss = 415415952.15638995\n",
      "Iteration 38, loss = 415305639.15112895\n",
      "Iteration 39, loss = 415472169.56696540\n",
      "Iteration 40, loss = 415490294.77587116\n",
      "Iteration 41, loss = 415553173.96350187\n",
      "Iteration 42, loss = 415522288.84253961\n",
      "Iteration 43, loss = 415452185.46328992\n",
      "Iteration 44, loss = 415431562.84943098\n",
      "Iteration 45, loss = 415406636.79899848\n",
      "Iteration 46, loss = 415366952.53904945\n",
      "Iteration 47, loss = 415355507.16656744\n",
      "Iteration 48, loss = 415480146.33940500\n",
      "Iteration 49, loss = 415362873.86348116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 375455405.70563382\n",
      "Iteration 2, loss = 126259539.70792471\n",
      "Iteration 3, loss = 121429833.51694755\n",
      "Iteration 4, loss = 120575270.89454973\n",
      "Iteration 5, loss = 119909300.99672918\n",
      "Iteration 6, loss = 119437364.49951118\n",
      "Iteration 7, loss = 119121322.29876605\n",
      "Iteration 8, loss = 118912421.83966994\n",
      "Iteration 9, loss = 118833242.73789893\n",
      "Iteration 10, loss = 118757011.03578213\n",
      "Iteration 11, loss = 118737206.65165271\n",
      "Iteration 12, loss = 118666088.90272281\n",
      "Iteration 13, loss = 118615227.86879626\n",
      "Iteration 14, loss = 118643570.12086517\n",
      "Iteration 15, loss = 118736353.53131822\n",
      "Iteration 16, loss = 118543724.56437235\n",
      "Iteration 17, loss = 118717793.66550753\n",
      "Iteration 18, loss = 118584844.51404263\n",
      "Iteration 19, loss = 118633582.34184326\n",
      "Iteration 20, loss = 118675536.58921216\n",
      "Iteration 21, loss = 118633726.01635490\n",
      "Iteration 22, loss = 118622912.39760411\n",
      "Iteration 23, loss = 118557975.76217456\n",
      "Iteration 24, loss = 118786572.26985554\n",
      "Iteration 25, loss = 118604958.25627078\n",
      "Iteration 26, loss = 118616688.84301394\n",
      "Iteration 27, loss = 118609061.37625754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1614288065.17794871\n",
      "Iteration 2, loss = 878650268.34397864\n",
      "Iteration 3, loss = 521935526.49283367\n",
      "Iteration 4, loss = 355674641.46138638\n",
      "Iteration 5, loss = 280377397.52623677\n",
      "Iteration 6, loss = 253746280.68939540\n",
      "Iteration 7, loss = 247173459.75786591\n",
      "Iteration 8, loss = 245889187.42665836\n",
      "Iteration 9, loss = 245708792.30825880\n",
      "Iteration 10, loss = 245666767.16141632\n",
      "Iteration 11, loss = 245652669.83259127\n",
      "Iteration 12, loss = 245641299.82746592\n",
      "Iteration 13, loss = 245630302.65329805\n",
      "Iteration 14, loss = 245618790.63301298\n",
      "Iteration 15, loss = 245613386.50120875\n",
      "Iteration 16, loss = 245648372.11182025\n",
      "Iteration 17, loss = 245594996.15315807\n",
      "Iteration 18, loss = 245617336.91739151\n",
      "Iteration 19, loss = 245592844.33469790\n",
      "Iteration 20, loss = 245580190.99609977\n",
      "Iteration 21, loss = 245554745.86762753\n",
      "Iteration 22, loss = 245569402.04997870\n",
      "Iteration 23, loss = 245585187.82519355\n",
      "Iteration 24, loss = 245560205.12654230\n",
      "Iteration 25, loss = 245534417.58823639\n",
      "Iteration 26, loss = 245552974.19576573\n",
      "Iteration 27, loss = 245509034.69392818\n",
      "Iteration 28, loss = 245526176.19153100\n",
      "Iteration 29, loss = 245498054.19282481\n",
      "Iteration 30, loss = 245506295.26733842\n",
      "Iteration 31, loss = 245495567.52572611\n",
      "Iteration 32, loss = 245507473.08081141\n",
      "Iteration 33, loss = 245535883.03121373\n",
      "Iteration 34, loss = 245504408.47298184\n",
      "Iteration 35, loss = 245460377.43995756\n",
      "Iteration 36, loss = 245465953.62839863\n",
      "Iteration 37, loss = 245461602.19900048\n",
      "Iteration 38, loss = 245460946.86255533\n",
      "Iteration 39, loss = 245473722.97330835\n",
      "Iteration 40, loss = 245471942.46157545\n",
      "Iteration 41, loss = 245465473.84027016\n",
      "Iteration 42, loss = 245452059.19461480\n",
      "Iteration 43, loss = 245505225.27230391\n",
      "Iteration 44, loss = 245431742.63647440\n",
      "Iteration 45, loss = 245457197.08019018\n",
      "Iteration 46, loss = 245439130.48164847\n",
      "Iteration 47, loss = 245408771.18791518\n",
      "Iteration 48, loss = 245418057.36391509\n",
      "Iteration 49, loss = 245384621.25781474\n",
      "Iteration 50, loss = 245515219.30907691\n",
      "Iteration 51, loss = 245454332.63023037\n",
      "Iteration 52, loss = 245447896.26559547\n",
      "Iteration 53, loss = 245376403.63685045\n",
      "Iteration 54, loss = 245391709.46946025\n",
      "Iteration 55, loss = 245376054.44328594\n",
      "Iteration 56, loss = 245381303.09233236\n",
      "Iteration 57, loss = 245375938.79803446\n",
      "Iteration 58, loss = 245364759.03846875\n",
      "Iteration 59, loss = 245348320.19930145\n",
      "Iteration 60, loss = 245351087.88504884\n",
      "Iteration 61, loss = 245366603.71680689\n",
      "Iteration 62, loss = 245336555.94867611\n",
      "Iteration 63, loss = 245308152.54192501\n",
      "Iteration 64, loss = 245350569.15680924\n",
      "Iteration 65, loss = 245410367.98493022\n",
      "Iteration 66, loss = 245266341.92768344\n",
      "Iteration 67, loss = 245276824.88365299\n",
      "Iteration 68, loss = 245271417.24645820\n",
      "Iteration 69, loss = 245295201.33215240\n",
      "Iteration 70, loss = 245324818.17233437\n",
      "Iteration 71, loss = 245255797.96705851\n",
      "Iteration 72, loss = 245246390.82789469\n",
      "Iteration 73, loss = 245288740.83336625\n",
      "Iteration 74, loss = 245248468.42621332\n",
      "Iteration 75, loss = 245304023.85822713\n",
      "Iteration 76, loss = 245286777.69198188\n",
      "Iteration 77, loss = 245235494.33250141\n",
      "Iteration 78, loss = 245452652.57211411\n",
      "Iteration 79, loss = 245386784.05118412\n",
      "Iteration 80, loss = 245330889.46752182\n",
      "Iteration 81, loss = 245146928.14872181\n",
      "Iteration 82, loss = 245229607.89719829\n",
      "Iteration 83, loss = 245187450.90219706\n",
      "Iteration 84, loss = 245198631.53075099\n",
      "Iteration 85, loss = 245250568.16889009\n",
      "Iteration 86, loss = 245245002.27875826\n",
      "Iteration 87, loss = 245199615.63518554\n",
      "Iteration 88, loss = 245131924.65655443\n",
      "Iteration 89, loss = 245127634.82137406\n",
      "Iteration 90, loss = 245244065.24536133\n",
      "Iteration 91, loss = 245192706.56552470\n",
      "Iteration 92, loss = 245169445.36387455\n",
      "Iteration 93, loss = 245118845.76235533\n",
      "Iteration 94, loss = 245146641.19805121\n",
      "Iteration 95, loss = 245053852.50853881\n",
      "Iteration 96, loss = 245009652.70782793\n",
      "Iteration 97, loss = 245138460.75155282\n",
      "Iteration 98, loss = 245210527.12294209\n",
      "Iteration 99, loss = 245091200.23071772\n",
      "Iteration 100, loss = 245003787.95100781\n",
      "Iteration 101, loss = 245198343.75053379\n",
      "Iteration 102, loss = 245079064.36033139\n",
      "Iteration 103, loss = 245044160.20795444\n",
      "Iteration 104, loss = 245122211.79022470\n",
      "Iteration 105, loss = 245303410.86599103\n",
      "Iteration 106, loss = 245047806.21186870\n",
      "Iteration 107, loss = 245087708.03507408\n",
      "Iteration 108, loss = 245109192.50155982\n",
      "Iteration 109, loss = 245040980.93669054\n",
      "Iteration 110, loss = 244964948.41873184\n",
      "Iteration 111, loss = 245044037.00030172\n",
      "Iteration 112, loss = 245005897.63112059\n",
      "Iteration 113, loss = 245083961.49718711\n",
      "Iteration 114, loss = 245081724.39976737\n",
      "Iteration 115, loss = 244965502.69949064\n",
      "Iteration 116, loss = 245031378.37979332\n",
      "Iteration 117, loss = 244995272.27436128\n",
      "Iteration 118, loss = 244951003.50457594\n",
      "Iteration 119, loss = 245037000.28178164\n",
      "Iteration 120, loss = 245084126.74721280\n",
      "Iteration 121, loss = 244924814.40494785\n",
      "Iteration 122, loss = 244984454.38922855\n",
      "Iteration 123, loss = 245036079.95631176\n",
      "Iteration 124, loss = 245033455.94586700\n",
      "Iteration 125, loss = 244996589.29148233\n",
      "Iteration 126, loss = 244848135.31482974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 127, loss = 245164810.07994252\n",
      "Iteration 128, loss = 245000770.38637203\n",
      "Iteration 129, loss = 244990476.56674525\n",
      "Iteration 130, loss = 244943938.03222978\n",
      "Iteration 131, loss = 244873279.06492114\n",
      "Iteration 132, loss = 245066535.32820043\n",
      "Iteration 133, loss = 244905393.50079584\n",
      "Iteration 134, loss = 244911745.73348683\n",
      "Iteration 135, loss = 244933513.55576906\n",
      "Iteration 136, loss = 244919555.34487852\n",
      "Iteration 137, loss = 244863675.87576368\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 328159920.80597889\n",
      "Iteration 2, loss = 315995744.58925736\n",
      "Iteration 3, loss = 313774094.68328881\n",
      "Iteration 4, loss = 314403065.07326293\n",
      "Iteration 5, loss = 313049871.51656282\n",
      "Iteration 6, loss = 312959260.63086057\n",
      "Iteration 7, loss = 313205307.34697318\n",
      "Iteration 8, loss = 313831268.72609192\n",
      "Iteration 9, loss = 312835608.36316246\n",
      "Iteration 10, loss = 313200704.99655706\n",
      "Iteration 11, loss = 313324969.31197894\n",
      "Iteration 12, loss = 312791064.56410986\n",
      "Iteration 13, loss = 312897118.58580023\n",
      "Iteration 14, loss = 313655040.88384014\n",
      "Iteration 15, loss = 313418352.58919817\n",
      "Iteration 16, loss = 312816088.29400271\n",
      "Iteration 17, loss = 313004980.34613508\n",
      "Iteration 18, loss = 312639845.68705356\n",
      "Iteration 19, loss = 313062588.80984432\n",
      "Iteration 20, loss = 312794024.58201402\n",
      "Iteration 21, loss = 313257357.87778014\n",
      "Iteration 22, loss = 313197353.03829032\n",
      "Iteration 23, loss = 313033986.14112139\n",
      "Iteration 24, loss = 312871560.77730554\n",
      "Iteration 25, loss = 312793140.24246085\n",
      "Iteration 26, loss = 313070071.25258583\n",
      "Iteration 27, loss = 313188811.58117813\n",
      "Iteration 28, loss = 313689059.32639092\n",
      "Iteration 29, loss = 312953225.76313305\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3300155440.93573189\n",
      "Iteration 2, loss = 2085361449.44692469\n",
      "Iteration 3, loss = 1217184951.76117587\n",
      "Iteration 4, loss = 600397114.97620094\n",
      "Iteration 5, loss = 404260980.61940390\n",
      "Iteration 6, loss = 364700290.03558356\n",
      "Iteration 7, loss = 337977971.03419143\n",
      "Iteration 8, loss = 315594140.26093811\n",
      "Iteration 9, loss = 304451930.70635337\n",
      "Iteration 10, loss = 298498244.27173245\n",
      "Iteration 11, loss = 295892053.38025606\n",
      "Iteration 12, loss = 294601899.86290139\n",
      "Iteration 13, loss = 294101054.33547330\n",
      "Iteration 14, loss = 293824314.00427991\n",
      "Iteration 15, loss = 293874159.10971206\n",
      "Iteration 16, loss = 294301820.57979965\n",
      "Iteration 17, loss = 293810617.15574843\n",
      "Iteration 18, loss = 293790345.69506896\n",
      "Iteration 19, loss = 293700078.69547915\n",
      "Iteration 20, loss = 293695191.70894402\n",
      "Iteration 21, loss = 293811015.33363855\n",
      "Iteration 22, loss = 293723662.64176476\n",
      "Iteration 23, loss = 293930649.78677571\n",
      "Iteration 24, loss = 293779899.33064508\n",
      "Iteration 25, loss = 293977580.36650705\n",
      "Iteration 26, loss = 293945259.25325984\n",
      "Iteration 27, loss = 293927295.79387236\n",
      "Iteration 28, loss = 293698677.02295154\n",
      "Iteration 29, loss = 293684754.56861722\n",
      "Iteration 30, loss = 293707874.31297374\n",
      "Iteration 31, loss = 293756325.60817564\n",
      "Iteration 32, loss = 293840322.54324275\n",
      "Iteration 33, loss = 293602542.26966184\n",
      "Iteration 34, loss = 293716821.44453514\n",
      "Iteration 35, loss = 293697393.24190485\n",
      "Iteration 36, loss = 293622995.47482824\n",
      "Iteration 37, loss = 293874441.77381396\n",
      "Iteration 38, loss = 293612283.18822312\n",
      "Iteration 39, loss = 293639628.32755488\n",
      "Iteration 40, loss = 293776736.73839015\n",
      "Iteration 41, loss = 293590654.52579498\n",
      "Iteration 42, loss = 293712656.27693665\n",
      "Iteration 43, loss = 293534560.51126838\n",
      "Iteration 44, loss = 293697492.65962142\n",
      "Iteration 45, loss = 293574314.20328170\n",
      "Iteration 46, loss = 293638931.80552834\n",
      "Iteration 47, loss = 293721089.80118489\n",
      "Iteration 48, loss = 293505157.84032959\n",
      "Iteration 49, loss = 293547717.87631339\n",
      "Iteration 50, loss = 293612116.38968718\n",
      "Iteration 51, loss = 293617309.27276653\n",
      "Iteration 52, loss = 293545822.52626920\n",
      "Iteration 53, loss = 293558150.60445553\n",
      "Iteration 54, loss = 293648594.37479091\n",
      "Iteration 55, loss = 293595439.38915581\n",
      "Iteration 56, loss = 293573903.67764109\n",
      "Iteration 57, loss = 293577136.83075923\n",
      "Iteration 58, loss = 293629726.84708118\n",
      "Iteration 59, loss = 293581434.91603810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 249452118.35890004\n",
      "Iteration 2, loss = 175755655.38127846\n",
      "Iteration 3, loss = 175552338.70381168\n",
      "Iteration 4, loss = 175516975.68315217\n",
      "Iteration 5, loss = 175433811.87156609\n",
      "Iteration 6, loss = 175384915.47268736\n",
      "Iteration 7, loss = 175374356.71093506\n",
      "Iteration 8, loss = 175370322.29740727\n",
      "Iteration 9, loss = 175244190.41296232\n",
      "Iteration 10, loss = 175243336.52921942\n",
      "Iteration 11, loss = 175212867.43536839\n",
      "Iteration 12, loss = 175172697.56623161\n",
      "Iteration 13, loss = 175120552.99547690\n",
      "Iteration 14, loss = 175277152.28117976\n",
      "Iteration 15, loss = 175077988.62340954\n",
      "Iteration 16, loss = 175013572.23652554\n",
      "Iteration 17, loss = 175145299.12333915\n",
      "Iteration 18, loss = 175143563.80510876\n",
      "Iteration 19, loss = 174991119.56038058\n",
      "Iteration 20, loss = 175030769.85624191\n",
      "Iteration 21, loss = 175001127.68994138\n",
      "Iteration 22, loss = 175065846.92836598\n",
      "Iteration 23, loss = 174979925.10877788\n",
      "Iteration 24, loss = 174959915.28537172\n",
      "Iteration 25, loss = 175206780.01085407\n",
      "Iteration 26, loss = 174988700.23886436\n",
      "Iteration 27, loss = 175016406.21620530\n",
      "Iteration 28, loss = 175023749.72798732\n",
      "Iteration 29, loss = 174963865.66854820\n",
      "Iteration 30, loss = 174963256.22041598\n",
      "Iteration 31, loss = 175051361.38832176\n",
      "Iteration 32, loss = 175006398.87280220\n",
      "Iteration 33, loss = 174958493.83374661\n",
      "Iteration 34, loss = 175307976.23411018\n",
      "Iteration 35, loss = 175130553.15177965\n",
      "Iteration 36, loss = 174916990.01289657\n",
      "Iteration 37, loss = 174963509.31312588\n",
      "Iteration 38, loss = 174959431.56604257\n",
      "Iteration 39, loss = 174952358.33058193\n",
      "Iteration 40, loss = 174995413.86644140\n",
      "Iteration 41, loss = 175037059.90400973\n",
      "Iteration 42, loss = 174857099.75433564\n",
      "Iteration 43, loss = 175015030.98906553\n",
      "Iteration 44, loss = 174856016.47600245\n",
      "Iteration 45, loss = 174874105.59714824\n",
      "Iteration 46, loss = 174732357.99044117\n",
      "Iteration 47, loss = 175105913.94594666\n",
      "Iteration 48, loss = 175042052.56276286\n",
      "Iteration 49, loss = 174872098.31914443\n",
      "Iteration 50, loss = 174841159.65364897\n",
      "Iteration 51, loss = 174947455.57561713\n",
      "Iteration 52, loss = 175036413.70585942\n",
      "Iteration 53, loss = 174971923.53462523\n",
      "Iteration 54, loss = 174962666.15550900\n",
      "Iteration 55, loss = 174819816.90111408\n",
      "Iteration 56, loss = 174879412.09667179\n",
      "Iteration 57, loss = 174775856.78556839\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3950093844.20208597\n",
      "Iteration 2, loss = 2047483028.83637309\n",
      "Iteration 3, loss = 1127973198.04889655\n",
      "Iteration 4, loss = 645237035.69716692\n",
      "Iteration 5, loss = 401392910.13899028\n",
      "Iteration 6, loss = 289058917.17194837\n",
      "Iteration 7, loss = 244673596.30488092\n",
      "Iteration 8, loss = 229734350.24642035\n",
      "Iteration 9, loss = 225398245.85877550\n",
      "Iteration 10, loss = 224221625.77613452\n",
      "Iteration 11, loss = 223829242.33060917\n",
      "Iteration 12, loss = 223617759.43233058\n",
      "Iteration 13, loss = 223392100.89093751\n",
      "Iteration 14, loss = 223213189.56113681\n",
      "Iteration 15, loss = 223040744.70190638\n",
      "Iteration 16, loss = 222892207.69114476\n",
      "Iteration 17, loss = 222728453.29673120\n",
      "Iteration 18, loss = 222602257.82437494\n",
      "Iteration 19, loss = 222470076.51440391\n",
      "Iteration 20, loss = 222346308.95142549\n",
      "Iteration 21, loss = 222251609.74606332\n",
      "Iteration 22, loss = 222150908.65174103\n",
      "Iteration 23, loss = 222081521.89420307\n",
      "Iteration 24, loss = 222017635.60284615\n",
      "Iteration 25, loss = 221966348.31049502\n",
      "Iteration 26, loss = 221894329.72464985\n",
      "Iteration 27, loss = 221867731.08248463\n",
      "Iteration 28, loss = 221822026.78052500\n",
      "Iteration 29, loss = 221783098.38074547\n",
      "Iteration 30, loss = 221782565.41833282\n",
      "Iteration 31, loss = 221733034.67848575\n",
      "Iteration 32, loss = 221726370.73120826\n",
      "Iteration 33, loss = 221681258.20671982\n",
      "Iteration 34, loss = 221671081.89231929\n",
      "Iteration 35, loss = 221690715.67384481\n",
      "Iteration 36, loss = 221661582.95194852\n",
      "Iteration 37, loss = 221685063.39927962\n",
      "Iteration 38, loss = 221623924.31445098\n",
      "Iteration 39, loss = 221636079.89785859\n",
      "Iteration 40, loss = 221648607.12738502\n",
      "Iteration 41, loss = 221621246.67088082\n",
      "Iteration 42, loss = 221642648.41897568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 221613095.52072948\n",
      "Iteration 44, loss = 221585754.74533677\n",
      "Iteration 45, loss = 221661917.53835052\n",
      "Iteration 46, loss = 221559916.54737881\n",
      "Iteration 47, loss = 221611881.79685336\n",
      "Iteration 48, loss = 221650933.96056885\n",
      "Iteration 49, loss = 221616726.14090317\n",
      "Iteration 50, loss = 221547341.59293339\n",
      "Iteration 51, loss = 221572619.46035293\n",
      "Iteration 52, loss = 221530325.32977363\n",
      "Iteration 53, loss = 221590360.91436484\n",
      "Iteration 54, loss = 221539416.24032730\n",
      "Iteration 55, loss = 221560946.94724467\n",
      "Iteration 56, loss = 221523428.47877890\n",
      "Iteration 57, loss = 221539782.93806398\n",
      "Iteration 58, loss = 221517667.07577911\n",
      "Iteration 59, loss = 221503938.16245836\n",
      "Iteration 60, loss = 221523843.65003771\n",
      "Iteration 61, loss = 221478611.74813485\n",
      "Iteration 62, loss = 221529082.40859815\n",
      "Iteration 63, loss = 221487555.29414079\n",
      "Iteration 64, loss = 221524010.59739137\n",
      "Iteration 65, loss = 221492836.18189698\n",
      "Iteration 66, loss = 221492084.81634650\n",
      "Iteration 67, loss = 221498417.28691572\n",
      "Iteration 68, loss = 221480209.27831855\n",
      "Iteration 69, loss = 221477960.25174898\n",
      "Iteration 70, loss = 221495117.55297062\n",
      "Iteration 71, loss = 221421052.73880213\n",
      "Iteration 72, loss = 221441571.41021186\n",
      "Iteration 73, loss = 221481333.68159512\n",
      "Iteration 74, loss = 221425764.39401734\n",
      "Iteration 75, loss = 221703091.00137031\n",
      "Iteration 76, loss = 221440437.40997905\n",
      "Iteration 77, loss = 221390012.59679717\n",
      "Iteration 78, loss = 221477177.76328292\n",
      "Iteration 79, loss = 221423333.33737567\n",
      "Iteration 80, loss = 221413745.22033307\n",
      "Iteration 81, loss = 221440523.93139932\n",
      "Iteration 82, loss = 221424623.63302743\n",
      "Iteration 83, loss = 221403281.06371519\n",
      "Iteration 84, loss = 221356599.78169844\n",
      "Iteration 85, loss = 221655655.25633314\n",
      "Iteration 86, loss = 221341736.50704315\n",
      "Iteration 87, loss = 221315989.60606962\n",
      "Iteration 88, loss = 221342502.17540845\n",
      "Iteration 89, loss = 221320260.10122618\n",
      "Iteration 90, loss = 221359516.84269390\n",
      "Iteration 91, loss = 221399386.49541441\n",
      "Iteration 92, loss = 221298594.14039594\n",
      "Iteration 93, loss = 221314032.63941172\n",
      "Iteration 94, loss = 221372544.01631805\n",
      "Iteration 95, loss = 221251747.97241837\n",
      "Iteration 96, loss = 221475465.42219967\n",
      "Iteration 97, loss = 221311553.65246797\n",
      "Iteration 98, loss = 221314533.13890195\n",
      "Iteration 99, loss = 221323749.70594701\n",
      "Iteration 100, loss = 221281578.56683663\n",
      "Iteration 101, loss = 221281637.92259341\n",
      "Iteration 102, loss = 221260855.83631632\n",
      "Iteration 103, loss = 221218662.15727779\n",
      "Iteration 104, loss = 221180307.69205379\n",
      "Iteration 105, loss = 221216779.56045148\n",
      "Iteration 106, loss = 221376624.15860492\n",
      "Iteration 107, loss = 221197565.35694376\n",
      "Iteration 108, loss = 221188870.44117114\n",
      "Iteration 109, loss = 221094719.75510630\n",
      "Iteration 110, loss = 221356151.73736030\n",
      "Iteration 111, loss = 221170239.67731968\n",
      "Iteration 112, loss = 221183280.40848413\n",
      "Iteration 113, loss = 221310319.31073213\n",
      "Iteration 114, loss = 221350396.68901798\n",
      "Iteration 115, loss = 221130550.67753637\n",
      "Iteration 116, loss = 221111568.41243204\n",
      "Iteration 117, loss = 221119022.34676412\n",
      "Iteration 118, loss = 221056632.10653260\n",
      "Iteration 119, loss = 221244261.04111812\n",
      "Iteration 120, loss = 221168254.04736465\n",
      "Iteration 121, loss = 221024367.68194932\n",
      "Iteration 122, loss = 221234328.25078106\n",
      "Iteration 123, loss = 221099540.24989864\n",
      "Iteration 124, loss = 221118648.38274083\n",
      "Iteration 125, loss = 221099096.49480957\n",
      "Iteration 126, loss = 221218374.50068828\n",
      "Iteration 127, loss = 221471499.21281710\n",
      "Iteration 128, loss = 221056818.80943179\n",
      "Iteration 129, loss = 221149220.73880419\n",
      "Iteration 130, loss = 221079829.74629328\n",
      "Iteration 131, loss = 221067048.23873135\n",
      "Iteration 132, loss = 221001792.20765004\n",
      "Iteration 133, loss = 221020778.26808307\n",
      "Iteration 134, loss = 221064614.69986993\n",
      "Iteration 135, loss = 221067835.21836314\n",
      "Iteration 136, loss = 221040106.33078876\n",
      "Iteration 137, loss = 220957622.81998065\n",
      "Iteration 138, loss = 221111879.74163398\n",
      "Iteration 139, loss = 220990372.08137172\n",
      "Iteration 140, loss = 221111693.38480082\n",
      "Iteration 141, loss = 220959210.67869326\n",
      "Iteration 142, loss = 220965552.59502339\n",
      "Iteration 143, loss = 220895504.04149565\n",
      "Iteration 144, loss = 220950584.28869623\n",
      "Iteration 145, loss = 221080693.27100000\n",
      "Iteration 146, loss = 220877070.53983730\n",
      "Iteration 147, loss = 220943739.57054830\n",
      "Iteration 148, loss = 220817395.42896947\n",
      "Iteration 149, loss = 220868238.25153369\n",
      "Iteration 150, loss = 221040453.38917714\n",
      "Iteration 151, loss = 220831272.00270697\n",
      "Iteration 152, loss = 220889926.74011704\n",
      "Iteration 153, loss = 220805308.38906410\n",
      "Iteration 154, loss = 220900868.64550397\n",
      "Iteration 155, loss = 220900297.22873133\n",
      "Iteration 156, loss = 220789339.48005062\n",
      "Iteration 157, loss = 220896443.08993414\n",
      "Iteration 158, loss = 220903463.96198609\n",
      "Iteration 159, loss = 220793348.62752640\n",
      "Iteration 160, loss = 220738585.46928388\n",
      "Iteration 161, loss = 220727714.22710133\n",
      "Iteration 162, loss = 220801837.75384170\n",
      "Iteration 163, loss = 220814918.29243529\n",
      "Iteration 164, loss = 220663406.57819176\n",
      "Iteration 165, loss = 220834506.18466017\n",
      "Iteration 166, loss = 220751214.38484555\n",
      "Iteration 167, loss = 221363238.42170635\n",
      "Iteration 168, loss = 220642186.10195524\n",
      "Iteration 169, loss = 220663119.81191739\n",
      "Iteration 170, loss = 220809715.22499248\n",
      "Iteration 171, loss = 220639402.22509545\n",
      "Iteration 172, loss = 220729349.11143237\n",
      "Iteration 173, loss = 220876904.32644838\n",
      "Iteration 174, loss = 220934760.53098795\n",
      "Iteration 175, loss = 220535890.31496161\n",
      "Iteration 176, loss = 220865532.32006943\n",
      "Iteration 177, loss = 220586386.43087006\n",
      "Iteration 178, loss = 220703650.81567788\n",
      "Iteration 179, loss = 220502469.56359565\n",
      "Iteration 180, loss = 220512318.59014028\n",
      "Iteration 181, loss = 220691840.26040503\n",
      "Iteration 182, loss = 220608641.44983110\n",
      "Iteration 183, loss = 220432287.37019059\n",
      "Iteration 184, loss = 220925435.79732069\n",
      "Iteration 185, loss = 220557282.91001186\n",
      "Iteration 186, loss = 220645377.97240084\n",
      "Iteration 187, loss = 220485711.30572486\n",
      "Iteration 188, loss = 220632940.67151278\n",
      "Iteration 189, loss = 220411779.61279884\n",
      "Iteration 190, loss = 220592506.15341705\n",
      "Iteration 191, loss = 220506427.74102515\n",
      "Iteration 192, loss = 220457210.72335431\n",
      "Iteration 193, loss = 220365670.28902236\n",
      "Iteration 194, loss = 220343846.13736957\n",
      "Iteration 195, loss = 220576815.41578448\n",
      "Iteration 196, loss = 220348223.04640552\n",
      "Iteration 197, loss = 220445650.06298736\n",
      "Iteration 198, loss = 220348434.52806690\n",
      "Iteration 199, loss = 220458218.68137118\n",
      "Iteration 200, loss = 220398605.34337547\n",
      "Iteration 1, loss = 187514242.48536238\n",
      "Iteration 2, loss = 186704234.49050689\n",
      "Iteration 3, loss = 186815483.04202440\n",
      "Iteration 4, loss = 186648169.25386781\n",
      "Iteration 5, loss = 186704882.94920248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 186574301.56122833\n",
      "Iteration 7, loss = 186506183.83023441\n",
      "Iteration 8, loss = 186466939.15294787\n",
      "Iteration 9, loss = 186443372.30617175\n",
      "Iteration 10, loss = 186660582.51090661\n",
      "Iteration 11, loss = 186419598.59664974\n",
      "Iteration 12, loss = 186576412.90368342\n",
      "Iteration 13, loss = 186664319.95498019\n",
      "Iteration 14, loss = 186362758.96205485\n",
      "Iteration 15, loss = 186428466.86854994\n",
      "Iteration 16, loss = 186291189.71258333\n",
      "Iteration 17, loss = 186154504.06577533\n",
      "Iteration 18, loss = 186131069.83785003\n",
      "Iteration 19, loss = 186337157.83630776\n",
      "Iteration 20, loss = 186084878.90266913\n",
      "Iteration 21, loss = 186301984.05786434\n",
      "Iteration 22, loss = 185947871.70361236\n",
      "Iteration 23, loss = 186223133.41763985\n",
      "Iteration 24, loss = 186146647.09610632\n",
      "Iteration 25, loss = 185867948.89303195\n",
      "Iteration 26, loss = 185967935.91089755\n",
      "Iteration 27, loss = 185824173.00526801\n",
      "Iteration 28, loss = 185884234.73918688\n",
      "Iteration 29, loss = 185873169.15137383\n",
      "Iteration 30, loss = 185709087.16189733\n",
      "Iteration 31, loss = 185747234.54024071\n",
      "Iteration 32, loss = 185883471.58462861\n",
      "Iteration 33, loss = 185781977.29761583\n",
      "Iteration 34, loss = 185910859.23711529\n",
      "Iteration 35, loss = 185734772.81403369\n",
      "Iteration 36, loss = 185628037.05564642\n",
      "Iteration 37, loss = 185869365.88366801\n",
      "Iteration 38, loss = 185694582.52556804\n",
      "Iteration 39, loss = 185671376.83278659\n",
      "Iteration 40, loss = 185487937.42424595\n",
      "Iteration 41, loss = 185354314.04057899\n",
      "Iteration 42, loss = 185444272.23171714\n",
      "Iteration 43, loss = 185448076.42808467\n",
      "Iteration 44, loss = 185394592.35937956\n",
      "Iteration 45, loss = 185415054.86203352\n",
      "Iteration 46, loss = 185295932.60033390\n",
      "Iteration 47, loss = 185358712.76727271\n",
      "Iteration 48, loss = 185229235.45681325\n",
      "Iteration 49, loss = 185101747.33056381\n",
      "Iteration 50, loss = 185141583.93717459\n",
      "Iteration 51, loss = 185193734.41350335\n",
      "Iteration 52, loss = 185312413.46587643\n",
      "Iteration 53, loss = 185297790.34691814\n",
      "Iteration 54, loss = 185208738.75174817\n",
      "Iteration 55, loss = 185063340.27162638\n",
      "Iteration 56, loss = 185080386.89261729\n",
      "Iteration 57, loss = 184931810.94277290\n",
      "Iteration 58, loss = 184846157.38236454\n",
      "Iteration 59, loss = 184945145.43807328\n",
      "Iteration 60, loss = 184840981.57144749\n",
      "Iteration 61, loss = 184910378.72077873\n",
      "Iteration 62, loss = 185080573.28301671\n",
      "Iteration 63, loss = 184923114.20528337\n",
      "Iteration 64, loss = 184756616.25344327\n",
      "Iteration 65, loss = 184772800.09374613\n",
      "Iteration 66, loss = 184601229.84641808\n",
      "Iteration 67, loss = 184626788.94618934\n",
      "Iteration 68, loss = 184717868.61635238\n",
      "Iteration 69, loss = 184514826.96370989\n",
      "Iteration 70, loss = 184507961.20339093\n",
      "Iteration 71, loss = 184610566.80653122\n",
      "Iteration 72, loss = 184407434.48945686\n",
      "Iteration 73, loss = 184306601.92112559\n",
      "Iteration 74, loss = 184525917.55700582\n",
      "Iteration 75, loss = 184501176.21576187\n",
      "Iteration 76, loss = 184533706.23851061\n",
      "Iteration 77, loss = 184215947.45359877\n",
      "Iteration 78, loss = 184284774.92100418\n",
      "Iteration 79, loss = 184258120.46510199\n",
      "Iteration 80, loss = 184523002.41578102\n",
      "Iteration 81, loss = 184273865.79433736\n",
      "Iteration 82, loss = 184265193.74561337\n",
      "Iteration 83, loss = 184055259.53092918\n",
      "Iteration 84, loss = 183969353.44886354\n",
      "Iteration 85, loss = 184063928.91481861\n",
      "Iteration 86, loss = 183840195.88003370\n",
      "Iteration 87, loss = 183949200.46854880\n",
      "Iteration 88, loss = 184054129.48786563\n",
      "Iteration 89, loss = 183760130.14655897\n",
      "Iteration 90, loss = 183777579.72422311\n",
      "Iteration 91, loss = 183779274.87269193\n",
      "Iteration 92, loss = 183901057.45404258\n",
      "Iteration 93, loss = 183746007.20101702\n",
      "Iteration 94, loss = 183655129.74463576\n",
      "Iteration 95, loss = 183686467.50837699\n",
      "Iteration 96, loss = 183802598.64623317\n",
      "Iteration 97, loss = 183696225.02868053\n",
      "Iteration 98, loss = 183453238.97101656\n",
      "Iteration 99, loss = 183566648.81934059\n",
      "Iteration 100, loss = 183463068.14033705\n",
      "Iteration 101, loss = 183419860.74037266\n",
      "Iteration 102, loss = 183882932.14219570\n",
      "Iteration 103, loss = 183266868.62121215\n",
      "Iteration 104, loss = 183426470.69162887\n",
      "Iteration 105, loss = 183270623.53144720\n",
      "Iteration 106, loss = 183134663.77229074\n",
      "Iteration 107, loss = 183677975.23081413\n",
      "Iteration 108, loss = 183028941.12489286\n",
      "Iteration 109, loss = 183088838.71488163\n",
      "Iteration 110, loss = 183175167.75742313\n",
      "Iteration 111, loss = 182964056.47417274\n",
      "Iteration 112, loss = 183371024.69623515\n",
      "Iteration 113, loss = 182946901.95396084\n",
      "Iteration 114, loss = 182978138.73421896\n",
      "Iteration 115, loss = 183000520.59340915\n",
      "Iteration 116, loss = 182830083.11429676\n",
      "Iteration 117, loss = 182743842.22731090\n",
      "Iteration 118, loss = 182817313.05439404\n",
      "Iteration 119, loss = 182727622.70886937\n",
      "Iteration 120, loss = 182826629.09290060\n",
      "Iteration 121, loss = 182830853.13464761\n",
      "Iteration 122, loss = 182392750.19563314\n",
      "Iteration 123, loss = 182572174.53151339\n",
      "Iteration 124, loss = 182436748.64974090\n",
      "Iteration 125, loss = 182756765.99329126\n",
      "Iteration 126, loss = 182875123.31891024\n",
      "Iteration 127, loss = 182435472.31319982\n",
      "Iteration 128, loss = 182608242.05414227\n",
      "Iteration 129, loss = 182486015.10688841\n",
      "Iteration 130, loss = 182292591.52118200\n",
      "Iteration 131, loss = 182258935.79154247\n",
      "Iteration 132, loss = 182364741.72742295\n",
      "Iteration 133, loss = 182429679.45921412\n",
      "Iteration 134, loss = 182619521.44695830\n",
      "Iteration 135, loss = 182100750.99316269\n",
      "Iteration 136, loss = 182384582.63123494\n",
      "Iteration 137, loss = 181949588.17680728\n",
      "Iteration 138, loss = 181883819.53777483\n",
      "Iteration 139, loss = 182249622.53236896\n",
      "Iteration 140, loss = 181735140.79426393\n",
      "Iteration 141, loss = 181827999.13305646\n",
      "Iteration 142, loss = 181889381.76485685\n",
      "Iteration 143, loss = 181681312.42602479\n",
      "Iteration 144, loss = 181999014.22811693\n",
      "Iteration 145, loss = 181947547.55906683\n",
      "Iteration 146, loss = 181776544.17279908\n",
      "Iteration 147, loss = 181638243.28117993\n",
      "Iteration 148, loss = 181571043.41331986\n",
      "Iteration 149, loss = 181725096.07150254\n",
      "Iteration 150, loss = 181609380.50410295\n",
      "Iteration 151, loss = 181661658.24388358\n",
      "Iteration 152, loss = 181540746.07551000\n",
      "Iteration 153, loss = 181454983.83446583\n",
      "Iteration 154, loss = 181315582.74443313\n",
      "Iteration 155, loss = 181405499.53966522\n",
      "Iteration 156, loss = 181506593.68252444\n",
      "Iteration 157, loss = 181420984.81677952\n",
      "Iteration 158, loss = 181234711.07106659\n",
      "Iteration 159, loss = 181727443.61304620\n",
      "Iteration 160, loss = 181158622.28963122\n",
      "Iteration 161, loss = 181092874.87841067\n",
      "Iteration 162, loss = 180893845.08199713\n",
      "Iteration 163, loss = 181005527.76992705\n",
      "Iteration 164, loss = 181013977.02650651\n",
      "Iteration 165, loss = 180960373.76710498\n",
      "Iteration 166, loss = 181139933.04121676\n",
      "Iteration 167, loss = 180857000.85670531\n",
      "Iteration 168, loss = 181102421.67450038\n",
      "Iteration 169, loss = 180942167.10970992\n",
      "Iteration 170, loss = 180884698.99439374\n",
      "Iteration 171, loss = 180761055.87380517\n",
      "Iteration 172, loss = 180792277.26034841\n",
      "Iteration 173, loss = 180791325.77804205\n",
      "Iteration 174, loss = 180813832.13275841\n",
      "Iteration 175, loss = 180563912.49920243\n",
      "Iteration 176, loss = 180608911.38059506\n",
      "Iteration 177, loss = 180808103.68168581\n",
      "Iteration 178, loss = 180559789.11485898\n",
      "Iteration 179, loss = 180426551.11846536\n",
      "Iteration 180, loss = 180486374.15696129\n",
      "Iteration 181, loss = 180573555.61589688\n",
      "Iteration 182, loss = 180517376.85196698\n",
      "Iteration 183, loss = 180311451.90009269\n",
      "Iteration 184, loss = 180530022.11475426\n",
      "Iteration 185, loss = 180446783.38684684\n",
      "Iteration 186, loss = 180407278.85860121\n",
      "Iteration 187, loss = 180218372.94488758\n",
      "Iteration 188, loss = 180306780.07186568\n",
      "Iteration 189, loss = 180120405.84599298\n",
      "Iteration 190, loss = 180565197.66955921\n",
      "Iteration 191, loss = 180034611.78334868\n",
      "Iteration 192, loss = 180213571.30366406\n",
      "Iteration 193, loss = 180039859.64605302\n",
      "Iteration 194, loss = 179827866.17736039\n",
      "Iteration 195, loss = 179911504.61757061\n",
      "Iteration 196, loss = 180126941.29309663\n",
      "Iteration 197, loss = 180039668.68569899\n",
      "Iteration 198, loss = 180294200.83284327\n",
      "Iteration 199, loss = 180816406.27923700\n",
      "Iteration 200, loss = 180425866.59771046\n",
      "Iteration 1, loss = 248361212.23588580\n",
      "Iteration 2, loss = 146529597.10456949\n",
      "Iteration 3, loss = 128710402.46543227\n",
      "Iteration 4, loss = 127943639.91282000\n",
      "Iteration 5, loss = 127327735.42686340\n",
      "Iteration 6, loss = 126951358.10533969\n",
      "Iteration 7, loss = 126664179.40349264\n",
      "Iteration 8, loss = 126486007.09704196\n",
      "Iteration 9, loss = 126344109.24623430\n",
      "Iteration 10, loss = 126227409.09558584\n",
      "Iteration 11, loss = 126145912.03686546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 126043684.64452371\n",
      "Iteration 13, loss = 125982245.90820251\n",
      "Iteration 14, loss = 125958965.85660073\n",
      "Iteration 15, loss = 125899116.19269624\n",
      "Iteration 16, loss = 125865818.57896985\n",
      "Iteration 17, loss = 125826799.82648332\n",
      "Iteration 18, loss = 125767197.34208055\n",
      "Iteration 19, loss = 125702467.42107382\n",
      "Iteration 20, loss = 125671957.64611542\n",
      "Iteration 21, loss = 125603851.67409785\n",
      "Iteration 22, loss = 125580061.30741614\n",
      "Iteration 23, loss = 125540883.96371841\n",
      "Iteration 24, loss = 125530138.96421172\n",
      "Iteration 25, loss = 125464824.39270650\n",
      "Iteration 26, loss = 125406953.48369053\n",
      "Iteration 27, loss = 125368872.78596297\n",
      "Iteration 28, loss = 125432036.25422226\n",
      "Iteration 29, loss = 125328848.39269422\n",
      "Iteration 30, loss = 125302312.46641475\n",
      "Iteration 31, loss = 125216735.94396268\n",
      "Iteration 32, loss = 125195489.53786910\n",
      "Iteration 33, loss = 125190641.46659517\n",
      "Iteration 34, loss = 125116984.36546165\n",
      "Iteration 35, loss = 125058004.53579089\n",
      "Iteration 36, loss = 125073496.44286677\n",
      "Iteration 37, loss = 125065696.40865114\n",
      "Iteration 38, loss = 125051200.35902114\n",
      "Iteration 39, loss = 124942908.20060098\n",
      "Iteration 40, loss = 124935368.11327273\n",
      "Iteration 41, loss = 124864606.92615227\n",
      "Iteration 42, loss = 124874957.82463486\n",
      "Iteration 43, loss = 124909281.05261999\n",
      "Iteration 44, loss = 124742647.33104298\n",
      "Iteration 45, loss = 124752951.49646333\n",
      "Iteration 46, loss = 124680793.79761404\n",
      "Iteration 47, loss = 124614506.97485483\n",
      "Iteration 48, loss = 124566961.98415595\n",
      "Iteration 49, loss = 124528651.86040178\n",
      "Iteration 50, loss = 124508503.66481319\n",
      "Iteration 51, loss = 124384077.99871536\n",
      "Iteration 52, loss = 124380434.37194473\n",
      "Iteration 53, loss = 124238063.05990422\n",
      "Iteration 54, loss = 124248963.43798965\n",
      "Iteration 55, loss = 124092000.30255887\n",
      "Iteration 56, loss = 123982433.10084760\n",
      "Iteration 57, loss = 123996028.63895947\n",
      "Iteration 58, loss = 123845256.33290555\n",
      "Iteration 59, loss = 123788175.65864028\n",
      "Iteration 60, loss = 123724172.02536152\n",
      "Iteration 61, loss = 123580691.96509956\n",
      "Iteration 62, loss = 123498638.32152969\n",
      "Iteration 63, loss = 123374933.42617732\n",
      "Iteration 64, loss = 123352614.98789737\n",
      "Iteration 65, loss = 123300915.25948113\n",
      "Iteration 66, loss = 123083747.86098261\n",
      "Iteration 67, loss = 123073000.62345974\n",
      "Iteration 68, loss = 122966720.82944128\n",
      "Iteration 69, loss = 122882121.15914382\n",
      "Iteration 70, loss = 122724332.52918659\n",
      "Iteration 71, loss = 122667356.28422077\n",
      "Iteration 72, loss = 122536816.88719270\n",
      "Iteration 73, loss = 122490314.90686126\n",
      "Iteration 74, loss = 122298594.70372021\n",
      "Iteration 75, loss = 122235370.18462270\n",
      "Iteration 76, loss = 122238634.10494922\n",
      "Iteration 77, loss = 122181120.21656249\n",
      "Iteration 78, loss = 121955945.96238872\n",
      "Iteration 79, loss = 121855342.08661588\n",
      "Iteration 80, loss = 121881445.78619610\n",
      "Iteration 81, loss = 121780595.89186437\n",
      "Iteration 82, loss = 121668123.98286946\n",
      "Iteration 83, loss = 121668999.74312988\n",
      "Iteration 84, loss = 121499942.04301946\n",
      "Iteration 85, loss = 121463091.10122019\n",
      "Iteration 86, loss = 121327914.21581504\n",
      "Iteration 87, loss = 121267349.81715195\n",
      "Iteration 88, loss = 121186655.67299378\n",
      "Iteration 89, loss = 121030885.43612427\n",
      "Iteration 90, loss = 120950772.85880874\n",
      "Iteration 91, loss = 120863698.94797446\n",
      "Iteration 92, loss = 120653091.93794617\n",
      "Iteration 93, loss = 120672345.02457993\n",
      "Iteration 94, loss = 120561346.26290411\n",
      "Iteration 95, loss = 120491145.97116531\n",
      "Iteration 96, loss = 120505459.96507779\n",
      "Iteration 97, loss = 120411833.43244758\n",
      "Iteration 98, loss = 120448529.77547407\n",
      "Iteration 99, loss = 120339372.97162656\n",
      "Iteration 100, loss = 120089677.50060542\n",
      "Iteration 101, loss = 120377743.34164956\n",
      "Iteration 102, loss = 120122580.26992182\n",
      "Iteration 103, loss = 120096345.93441707\n",
      "Iteration 104, loss = 120048210.76682299\n",
      "Iteration 105, loss = 120047069.51959167\n",
      "Iteration 106, loss = 119996768.17657156\n",
      "Iteration 107, loss = 119965457.96137862\n",
      "Iteration 108, loss = 120008118.95841780\n",
      "Iteration 109, loss = 119917745.59211539\n",
      "Iteration 110, loss = 119846150.03978968\n",
      "Iteration 111, loss = 119683613.69424759\n",
      "Iteration 112, loss = 119667060.24813820\n",
      "Iteration 113, loss = 119801613.34503603\n",
      "Iteration 114, loss = 119652217.06588344\n",
      "Iteration 115, loss = 119550886.66102512\n",
      "Iteration 116, loss = 119468535.48885545\n",
      "Iteration 117, loss = 119657331.80695407\n",
      "Iteration 118, loss = 119563501.48016001\n",
      "Iteration 119, loss = 119425490.47086062\n",
      "Iteration 120, loss = 119366274.14320783\n",
      "Iteration 121, loss = 119417457.92715684\n",
      "Iteration 122, loss = 119409417.11293516\n",
      "Iteration 123, loss = 119303570.38705200\n",
      "Iteration 124, loss = 119354082.85092065\n",
      "Iteration 125, loss = 119423669.63506044\n",
      "Iteration 126, loss = 119317714.29245844\n",
      "Iteration 127, loss = 119241927.25678721\n",
      "Iteration 128, loss = 119144288.48332107\n",
      "Iteration 129, loss = 119259645.80870056\n",
      "Iteration 130, loss = 119049036.48546697\n",
      "Iteration 131, loss = 119036992.72530894\n",
      "Iteration 132, loss = 119142689.96139614\n",
      "Iteration 133, loss = 119013755.59975229\n",
      "Iteration 134, loss = 119017113.36916035\n",
      "Iteration 135, loss = 118983732.07308808\n",
      "Iteration 136, loss = 118955446.35766751\n",
      "Iteration 137, loss = 119209682.96463303\n",
      "Iteration 138, loss = 119023476.92141157\n",
      "Iteration 139, loss = 118787877.40327246\n",
      "Iteration 140, loss = 118865476.56879577\n",
      "Iteration 141, loss = 118800546.16317847\n",
      "Iteration 142, loss = 118727177.82347180\n",
      "Iteration 143, loss = 118797956.40719268\n",
      "Iteration 144, loss = 118876898.80785815\n",
      "Iteration 145, loss = 118658745.77842648\n",
      "Iteration 146, loss = 118818603.98822446\n",
      "Iteration 147, loss = 118600351.78652866\n",
      "Iteration 148, loss = 118553049.06682581\n",
      "Iteration 149, loss = 118526487.72857000\n",
      "Iteration 150, loss = 118479185.15309350\n",
      "Iteration 151, loss = 118451558.35019656\n",
      "Iteration 152, loss = 118528011.16409609\n",
      "Iteration 153, loss = 118385721.31291516\n",
      "Iteration 154, loss = 118436205.64794959\n",
      "Iteration 155, loss = 118479783.22748549\n",
      "Iteration 156, loss = 118269543.80760258\n",
      "Iteration 157, loss = 118401250.72502075\n",
      "Iteration 158, loss = 118273967.81090693\n",
      "Iteration 159, loss = 118171031.69798099\n",
      "Iteration 160, loss = 118296233.15574692\n",
      "Iteration 161, loss = 118184999.81291333\n",
      "Iteration 162, loss = 118142937.04409397\n",
      "Iteration 163, loss = 118302345.21526743\n",
      "Iteration 164, loss = 118048154.80197005\n",
      "Iteration 165, loss = 118180979.70642950\n",
      "Iteration 166, loss = 118066377.66149738\n",
      "Iteration 167, loss = 117963919.53550825\n",
      "Iteration 168, loss = 118042508.11876167\n",
      "Iteration 169, loss = 117945562.36150694\n",
      "Iteration 170, loss = 118103654.55113433\n",
      "Iteration 171, loss = 117890401.98980965\n",
      "Iteration 172, loss = 117862727.08766373\n",
      "Iteration 173, loss = 117780952.91288970\n",
      "Iteration 174, loss = 117732667.81190211\n",
      "Iteration 175, loss = 117654215.98721722\n",
      "Iteration 176, loss = 117661544.34874736\n",
      "Iteration 177, loss = 117634391.88378255\n",
      "Iteration 178, loss = 117735321.35585034\n",
      "Iteration 179, loss = 117682187.63203049\n",
      "Iteration 180, loss = 117634253.39184900\n",
      "Iteration 181, loss = 117640502.34687920\n",
      "Iteration 182, loss = 117530441.75238137\n",
      "Iteration 183, loss = 117456856.78423391\n",
      "Iteration 184, loss = 117711447.85135779\n",
      "Iteration 185, loss = 117452670.94327119\n",
      "Iteration 186, loss = 117440659.64759822\n",
      "Iteration 187, loss = 117289348.79759565\n",
      "Iteration 188, loss = 117544704.23661238\n",
      "Iteration 189, loss = 117345286.32818735\n",
      "Iteration 190, loss = 117212827.06630354\n",
      "Iteration 191, loss = 117250493.55354252\n",
      "Iteration 192, loss = 117170726.96417217\n",
      "Iteration 193, loss = 117369090.46010141\n",
      "Iteration 194, loss = 117207687.53370735\n",
      "Iteration 195, loss = 117091332.15574846\n",
      "Iteration 196, loss = 117030899.76166677\n",
      "Iteration 197, loss = 117170080.35399427\n",
      "Iteration 198, loss = 117045877.67852706\n",
      "Iteration 199, loss = 116955993.57353982\n",
      "Iteration 200, loss = 117139510.92100555\n",
      "Iteration 1, loss = 280600717.07069898\n",
      "Iteration 2, loss = 251652196.32280010\n",
      "Iteration 3, loss = 249715001.54051211\n",
      "Iteration 4, loss = 248036952.12023717\n",
      "Iteration 5, loss = 246646563.98858652\n",
      "Iteration 6, loss = 245589465.40962499\n",
      "Iteration 7, loss = 244649712.39506456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 244074376.50964701\n",
      "Iteration 9, loss = 243587467.09144336\n",
      "Iteration 10, loss = 243135794.25676069\n",
      "Iteration 11, loss = 242937614.81636494\n",
      "Iteration 12, loss = 242780275.50181982\n",
      "Iteration 13, loss = 242629923.09835196\n",
      "Iteration 14, loss = 242328284.56064692\n",
      "Iteration 15, loss = 242291169.80800319\n",
      "Iteration 16, loss = 242156920.44984293\n",
      "Iteration 17, loss = 242174035.25379357\n",
      "Iteration 18, loss = 242129084.40640450\n",
      "Iteration 19, loss = 242073521.63648716\n",
      "Iteration 20, loss = 242019183.19103017\n",
      "Iteration 21, loss = 241752040.99532983\n",
      "Iteration 22, loss = 241829132.47016162\n",
      "Iteration 23, loss = 241612401.44490537\n",
      "Iteration 24, loss = 241586333.24624518\n",
      "Iteration 25, loss = 241527419.99726251\n",
      "Iteration 26, loss = 241494936.72896037\n",
      "Iteration 27, loss = 241385426.78815705\n",
      "Iteration 28, loss = 241554996.93665737\n",
      "Iteration 29, loss = 241326724.52783754\n",
      "Iteration 30, loss = 241157730.55663100\n",
      "Iteration 31, loss = 241078486.18386039\n",
      "Iteration 32, loss = 240932792.16988814\n",
      "Iteration 33, loss = 241056750.97098491\n",
      "Iteration 34, loss = 240991249.49261901\n",
      "Iteration 35, loss = 240768612.37434098\n",
      "Iteration 36, loss = 240752188.49062359\n",
      "Iteration 37, loss = 240664111.45221385\n",
      "Iteration 38, loss = 240686776.85838804\n",
      "Iteration 39, loss = 240571990.23557633\n",
      "Iteration 40, loss = 240440713.30432925\n",
      "Iteration 41, loss = 240459643.64401940\n",
      "Iteration 42, loss = 240426271.86684039\n",
      "Iteration 43, loss = 240243794.58786568\n",
      "Iteration 44, loss = 240208121.79140830\n",
      "Iteration 45, loss = 240047002.25225326\n",
      "Iteration 46, loss = 239970721.20845684\n",
      "Iteration 47, loss = 239932932.67232183\n",
      "Iteration 48, loss = 239980022.88759726\n",
      "Iteration 49, loss = 239818659.45546293\n",
      "Iteration 50, loss = 239726150.22502524\n",
      "Iteration 51, loss = 239564637.80581865\n",
      "Iteration 52, loss = 239571751.77109918\n",
      "Iteration 53, loss = 239393612.19908315\n",
      "Iteration 54, loss = 239375943.60460487\n",
      "Iteration 55, loss = 239313785.87128589\n",
      "Iteration 56, loss = 239466734.99059555\n",
      "Iteration 57, loss = 239199058.65522003\n",
      "Iteration 58, loss = 239195462.41828349\n",
      "Iteration 59, loss = 239497700.31720385\n",
      "Iteration 60, loss = 238844759.07493609\n",
      "Iteration 61, loss = 239117256.24153072\n",
      "Iteration 62, loss = 238659690.84304029\n",
      "Iteration 63, loss = 238523738.09903643\n",
      "Iteration 64, loss = 238460773.39463735\n",
      "Iteration 65, loss = 238381193.35948300\n",
      "Iteration 66, loss = 238479556.23814818\n",
      "Iteration 67, loss = 238259277.83532017\n",
      "Iteration 68, loss = 238036519.81111470\n",
      "Iteration 69, loss = 238148186.45212838\n",
      "Iteration 70, loss = 237902596.03639752\n",
      "Iteration 71, loss = 238022375.47426903\n",
      "Iteration 72, loss = 237817645.93409926\n",
      "Iteration 73, loss = 237733935.40163353\n",
      "Iteration 74, loss = 237731286.54893216\n",
      "Iteration 75, loss = 237656966.52182817\n",
      "Iteration 76, loss = 237382329.99517143\n",
      "Iteration 77, loss = 237284218.55893642\n",
      "Iteration 78, loss = 237197234.41189337\n",
      "Iteration 79, loss = 237047559.59984273\n",
      "Iteration 80, loss = 237032295.39454702\n",
      "Iteration 81, loss = 236999521.09709057\n",
      "Iteration 82, loss = 236746449.18209970\n",
      "Iteration 83, loss = 236700564.36145470\n",
      "Iteration 84, loss = 236582412.02460778\n",
      "Iteration 85, loss = 236533448.74193329\n",
      "Iteration 86, loss = 236318707.32010937\n",
      "Iteration 87, loss = 236626044.97355375\n",
      "Iteration 88, loss = 236227672.66909960\n",
      "Iteration 89, loss = 236044156.61374125\n",
      "Iteration 90, loss = 236083435.42834857\n",
      "Iteration 91, loss = 235997020.47367150\n",
      "Iteration 92, loss = 235738153.36959326\n",
      "Iteration 93, loss = 235856539.66731626\n",
      "Iteration 94, loss = 235578286.19786504\n",
      "Iteration 95, loss = 235424661.12330142\n",
      "Iteration 96, loss = 235377715.81971598\n",
      "Iteration 97, loss = 235184627.03550416\n",
      "Iteration 98, loss = 235326362.24105030\n",
      "Iteration 99, loss = 235144998.61347294\n",
      "Iteration 100, loss = 234970923.80223721\n",
      "Iteration 101, loss = 234827146.30050713\n",
      "Iteration 102, loss = 234778791.47776800\n",
      "Iteration 103, loss = 234749408.60659429\n",
      "Iteration 104, loss = 234396789.36482868\n",
      "Iteration 105, loss = 234567698.86587963\n",
      "Iteration 106, loss = 234392928.48649341\n",
      "Iteration 107, loss = 234255345.74879575\n",
      "Iteration 108, loss = 234008582.61187071\n",
      "Iteration 109, loss = 233982142.23891503\n",
      "Iteration 110, loss = 233777004.68810421\n",
      "Iteration 111, loss = 233590855.20030415\n",
      "Iteration 112, loss = 233616405.93660083\n",
      "Iteration 113, loss = 233495373.05415660\n",
      "Iteration 114, loss = 233250977.12997988\n",
      "Iteration 115, loss = 233298537.24408600\n",
      "Iteration 116, loss = 233193995.15307412\n",
      "Iteration 117, loss = 232741504.63808715\n",
      "Iteration 118, loss = 233003648.22858402\n",
      "Iteration 119, loss = 232819765.32672709\n",
      "Iteration 120, loss = 232793008.15874633\n",
      "Iteration 121, loss = 232499642.27897471\n",
      "Iteration 122, loss = 232490197.35649225\n",
      "Iteration 123, loss = 232189412.17358705\n",
      "Iteration 124, loss = 232138818.59209043\n",
      "Iteration 125, loss = 231966088.76851898\n",
      "Iteration 126, loss = 232002088.07146171\n",
      "Iteration 127, loss = 232080225.67887476\n",
      "Iteration 128, loss = 231355415.57040268\n",
      "Iteration 129, loss = 231257589.53163877\n",
      "Iteration 130, loss = 231320228.25345099\n",
      "Iteration 131, loss = 231452939.37214550\n",
      "Iteration 132, loss = 231313750.41573173\n",
      "Iteration 133, loss = 231766602.16515517\n",
      "Iteration 134, loss = 231057266.85735816\n",
      "Iteration 135, loss = 230684131.08001596\n",
      "Iteration 136, loss = 230938486.75082460\n",
      "Iteration 137, loss = 230527272.66769823\n",
      "Iteration 138, loss = 230427457.31493956\n",
      "Iteration 139, loss = 230732760.91684681\n",
      "Iteration 140, loss = 230320787.95597246\n",
      "Iteration 141, loss = 230163208.17691928\n",
      "Iteration 142, loss = 229907709.29097843\n",
      "Iteration 143, loss = 230268273.00248936\n",
      "Iteration 144, loss = 229765499.11041260\n",
      "Iteration 145, loss = 229680723.08694571\n",
      "Iteration 146, loss = 229593695.44098911\n",
      "Iteration 147, loss = 229328432.46521372\n",
      "Iteration 148, loss = 229359313.62184232\n",
      "Iteration 149, loss = 229227454.70543203\n",
      "Iteration 150, loss = 229266957.98845449\n",
      "Iteration 151, loss = 229384032.51685226\n",
      "Iteration 152, loss = 228829235.09435010\n",
      "Iteration 153, loss = 228660933.26181337\n",
      "Iteration 154, loss = 229044235.36646649\n",
      "Iteration 155, loss = 228354843.31206366\n",
      "Iteration 156, loss = 228506282.12769908\n",
      "Iteration 157, loss = 228238781.97196943\n",
      "Iteration 158, loss = 228094757.13977152\n",
      "Iteration 159, loss = 228173711.12741134\n",
      "Iteration 160, loss = 227992178.19748077\n",
      "Iteration 161, loss = 228018860.57030782\n",
      "Iteration 162, loss = 227580169.33560809\n",
      "Iteration 163, loss = 227622539.03166288\n",
      "Iteration 164, loss = 227544805.16369662\n",
      "Iteration 165, loss = 227454739.34600616\n",
      "Iteration 166, loss = 227409719.55187270\n",
      "Iteration 167, loss = 227226730.86747330\n",
      "Iteration 168, loss = 227260708.40026137\n",
      "Iteration 169, loss = 226931764.64029819\n",
      "Iteration 170, loss = 227039904.79313847\n",
      "Iteration 171, loss = 227367852.30185512\n",
      "Iteration 172, loss = 226570021.16514647\n",
      "Iteration 173, loss = 226775724.10731021\n",
      "Iteration 174, loss = 226347331.44211888\n",
      "Iteration 175, loss = 226287285.98331845\n",
      "Iteration 176, loss = 226325090.08499214\n",
      "Iteration 177, loss = 226225455.33722016\n",
      "Iteration 178, loss = 226048755.91481861\n",
      "Iteration 179, loss = 226122770.76558217\n",
      "Iteration 180, loss = 225856609.56251895\n",
      "Iteration 181, loss = 225666818.99994281\n",
      "Iteration 182, loss = 225924156.56806123\n",
      "Iteration 183, loss = 225486699.18120673\n",
      "Iteration 184, loss = 225524512.03662980\n",
      "Iteration 185, loss = 225375111.57094395\n",
      "Iteration 186, loss = 225221299.72616407\n",
      "Iteration 187, loss = 225394718.88289711\n",
      "Iteration 188, loss = 225075951.26802900\n",
      "Iteration 189, loss = 225130674.41987574\n",
      "Iteration 190, loss = 224944179.70699838\n",
      "Iteration 191, loss = 225107849.48919547\n",
      "Iteration 192, loss = 224730639.10976949\n",
      "Iteration 193, loss = 224723260.67267603\n",
      "Iteration 194, loss = 224845483.17703140\n",
      "Iteration 195, loss = 224566874.28728086\n",
      "Iteration 196, loss = 224609345.14125445\n",
      "Iteration 197, loss = 224517044.61927897\n",
      "Iteration 198, loss = 224246788.31090307\n",
      "Iteration 199, loss = 224061191.79367372\n",
      "Iteration 200, loss = 223966626.76660630\n",
      "MAE:  13617.713400245164\n",
      "MSE:  196189855.9067907\n",
      "RMSE:  14006.778926890747\n",
      "R2:  -420.1456561410305\n",
      "3 13617.713400245164\n",
      "(103465, 15) (23651, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2992877078.37561607\n",
      "Iteration 2, loss = 1110557385.50988960\n",
      "Iteration 3, loss = 548273232.94860649\n",
      "Iteration 4, loss = 436209906.40987289\n",
      "Iteration 5, loss = 421944425.46478343\n",
      "Iteration 6, loss = 420257911.56383479\n",
      "Iteration 7, loss = 419473605.72057140\n",
      "Iteration 8, loss = 418774258.90798992\n",
      "Iteration 9, loss = 418160717.34474367\n",
      "Iteration 10, loss = 417583812.15648538\n",
      "Iteration 11, loss = 417069269.16221881\n",
      "Iteration 12, loss = 416579075.78967535\n",
      "Iteration 13, loss = 415963193.17806304\n",
      "Iteration 14, loss = 415592158.84284377\n",
      "Iteration 15, loss = 415248121.23513347\n",
      "Iteration 16, loss = 415007739.91748834\n",
      "Iteration 17, loss = 414849136.74294317\n",
      "Iteration 18, loss = 414704648.86151421\n",
      "Iteration 19, loss = 414533626.09917349\n",
      "Iteration 20, loss = 414379658.61973029\n",
      "Iteration 21, loss = 414313365.63762116\n",
      "Iteration 22, loss = 414136319.34165657\n",
      "Iteration 23, loss = 414093871.81093520\n",
      "Iteration 24, loss = 413958922.01004285\n",
      "Iteration 25, loss = 413808910.63544643\n",
      "Iteration 26, loss = 413900371.87743664\n",
      "Iteration 27, loss = 413653720.13555640\n",
      "Iteration 28, loss = 413706265.92429000\n",
      "Iteration 29, loss = 413499563.14434856\n",
      "Iteration 30, loss = 413472893.13821483\n",
      "Iteration 31, loss = 413397469.17434633\n",
      "Iteration 32, loss = 413351041.76956499\n",
      "Iteration 33, loss = 413221386.99774861\n",
      "Iteration 34, loss = 413502978.44089544\n",
      "Iteration 35, loss = 413128451.45874631\n",
      "Iteration 36, loss = 413137106.22602451\n",
      "Iteration 37, loss = 412994281.66898400\n",
      "Iteration 38, loss = 413012755.70350277\n",
      "Iteration 39, loss = 412938015.14077502\n",
      "Iteration 40, loss = 412985509.52561891\n",
      "Iteration 41, loss = 413019695.49104041\n",
      "Iteration 42, loss = 412710945.01453751\n",
      "Iteration 43, loss = 412668435.02575493\n",
      "Iteration 44, loss = 412744900.28110951\n",
      "Iteration 45, loss = 412738505.53119558\n",
      "Iteration 46, loss = 412722443.49690443\n",
      "Iteration 47, loss = 412864506.55495501\n",
      "Iteration 48, loss = 412577748.18106174\n",
      "Iteration 49, loss = 412579980.33011794\n",
      "Iteration 50, loss = 412750202.44669646\n",
      "Iteration 51, loss = 412479206.84275317\n",
      "Iteration 52, loss = 412750889.22675824\n",
      "Iteration 53, loss = 412476249.46732312\n",
      "Iteration 54, loss = 412468968.73109448\n",
      "Iteration 55, loss = 412514850.96298724\n",
      "Iteration 56, loss = 412384113.53302205\n",
      "Iteration 57, loss = 412330088.98283815\n",
      "Iteration 58, loss = 412420461.08810008\n",
      "Iteration 59, loss = 412476082.21741986\n",
      "Iteration 60, loss = 412682458.53567117\n",
      "Iteration 61, loss = 412332792.74008811\n",
      "Iteration 62, loss = 412201161.57186550\n",
      "Iteration 63, loss = 412266193.98749375\n",
      "Iteration 64, loss = 412130332.44080061\n",
      "Iteration 65, loss = 412520853.98686862\n",
      "Iteration 66, loss = 412236465.21912104\n",
      "Iteration 67, loss = 412278359.37297112\n",
      "Iteration 68, loss = 412146708.47271961\n",
      "Iteration 69, loss = 412575932.17225492\n",
      "Iteration 70, loss = 411918547.69827980\n",
      "Iteration 71, loss = 412061309.88555366\n",
      "Iteration 72, loss = 412027437.69195056\n",
      "Iteration 73, loss = 411849709.33219182\n",
      "Iteration 74, loss = 411960457.20964313\n",
      "Iteration 75, loss = 412371855.51568472\n",
      "Iteration 76, loss = 411780807.00302160\n",
      "Iteration 77, loss = 412085346.42663401\n",
      "Iteration 78, loss = 412002648.99463266\n",
      "Iteration 79, loss = 411937441.54696071\n",
      "Iteration 80, loss = 412463932.52958357\n",
      "Iteration 81, loss = 411787924.15104312\n",
      "Iteration 82, loss = 412007657.20854676\n",
      "Iteration 83, loss = 411919475.06855512\n",
      "Iteration 84, loss = 411667417.25763685\n",
      "Iteration 85, loss = 411789891.49792123\n",
      "Iteration 86, loss = 411862968.05255842\n",
      "Iteration 87, loss = 411586778.84403670\n",
      "Iteration 88, loss = 412418913.42752767\n",
      "Iteration 89, loss = 412238249.30545491\n",
      "Iteration 90, loss = 412088944.06623775\n",
      "Iteration 91, loss = 411528116.58066404\n",
      "Iteration 92, loss = 411666110.18405437\n",
      "Iteration 93, loss = 411475388.50099230\n",
      "Iteration 94, loss = 411301992.25683379\n",
      "Iteration 95, loss = 411623431.88622862\n",
      "Iteration 96, loss = 411376256.84947115\n",
      "Iteration 97, loss = 411277862.45299613\n",
      "Iteration 98, loss = 411389522.85944581\n",
      "Iteration 99, loss = 411503009.37600368\n",
      "Iteration 100, loss = 411469300.63447094\n",
      "Iteration 101, loss = 411281419.64398187\n",
      "Iteration 102, loss = 411478930.71053249\n",
      "Iteration 103, loss = 411393822.41420746\n",
      "Iteration 104, loss = 411315827.11827171\n",
      "Iteration 105, loss = 411322932.89901710\n",
      "Iteration 106, loss = 411194464.36704034\n",
      "Iteration 107, loss = 411227251.40161252\n",
      "Iteration 108, loss = 411186436.54951322\n",
      "Iteration 109, loss = 411452819.69583106\n",
      "Iteration 110, loss = 411650645.21890086\n",
      "Iteration 111, loss = 411034597.91844553\n",
      "Iteration 112, loss = 410945148.78929150\n",
      "Iteration 113, loss = 411580263.34617662\n",
      "Iteration 114, loss = 412269429.30728406\n",
      "Iteration 115, loss = 410962901.94828743\n",
      "Iteration 116, loss = 410977892.67756987\n",
      "Iteration 117, loss = 411913707.72459894\n",
      "Iteration 118, loss = 411674339.04151613\n",
      "Iteration 119, loss = 410958808.28389245\n",
      "Iteration 120, loss = 411201389.98635006\n",
      "Iteration 121, loss = 411014550.23619956\n",
      "Iteration 122, loss = 410677694.11572868\n",
      "Iteration 123, loss = 411178425.66181988\n",
      "Iteration 124, loss = 410867675.59002656\n",
      "Iteration 125, loss = 410868847.06219757\n",
      "Iteration 126, loss = 410904583.49574584\n",
      "Iteration 127, loss = 410591716.90837067\n",
      "Iteration 128, loss = 410560132.79418695\n",
      "Iteration 129, loss = 410807600.11216986\n",
      "Iteration 130, loss = 410652865.27620471\n",
      "Iteration 131, loss = 410563430.09111023\n",
      "Iteration 132, loss = 410619261.40492648\n",
      "Iteration 133, loss = 411080507.53454810\n",
      "Iteration 134, loss = 410572650.37721604\n",
      "Iteration 135, loss = 410466818.71699792\n",
      "Iteration 136, loss = 410316704.99156481\n",
      "Iteration 137, loss = 410474979.06494665\n",
      "Iteration 138, loss = 410492738.36579579\n",
      "Iteration 139, loss = 410867301.04529673\n",
      "Iteration 140, loss = 410417515.18389642\n",
      "Iteration 141, loss = 410459077.55072784\n",
      "Iteration 142, loss = 410791054.41694897\n",
      "Iteration 143, loss = 410727564.39515787\n",
      "Iteration 144, loss = 410379225.25955832\n",
      "Iteration 145, loss = 410184477.79545212\n",
      "Iteration 146, loss = 410285404.11372757\n",
      "Iteration 147, loss = 410286440.00390154\n",
      "Iteration 148, loss = 410151028.79082626\n",
      "Iteration 149, loss = 410088582.44838297\n",
      "Iteration 150, loss = 410120972.00355536\n",
      "Iteration 151, loss = 410280513.87637454\n",
      "Iteration 152, loss = 410218895.16530854\n",
      "Iteration 153, loss = 409885092.75508571\n",
      "Iteration 154, loss = 410111916.90309584\n",
      "Iteration 155, loss = 409788420.18713588\n",
      "Iteration 156, loss = 410618616.75585192\n",
      "Iteration 157, loss = 409436649.75150335\n",
      "Iteration 158, loss = 410500650.32664961\n",
      "Iteration 159, loss = 409868787.22548866\n",
      "Iteration 160, loss = 410067987.85669118\n",
      "Iteration 161, loss = 409812291.85119778\n",
      "Iteration 162, loss = 409812778.78198493\n",
      "Iteration 163, loss = 410081933.22716117\n",
      "Iteration 164, loss = 410372096.26013172\n",
      "Iteration 165, loss = 409843640.04117888\n",
      "Iteration 166, loss = 409473070.32557917\n",
      "Iteration 167, loss = 409857508.05891144\n",
      "Iteration 168, loss = 410236015.65957940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 134674494.32538149\n",
      "Iteration 2, loss = 126821522.66145051\n",
      "Iteration 3, loss = 123920762.07452935\n",
      "Iteration 4, loss = 122632408.67292586\n",
      "Iteration 5, loss = 122173310.74082518\n",
      "Iteration 6, loss = 121584254.21798523\n",
      "Iteration 7, loss = 121412417.11732081\n",
      "Iteration 8, loss = 121363241.92206633\n",
      "Iteration 9, loss = 121210528.93018091\n",
      "Iteration 10, loss = 121178172.03923753\n",
      "Iteration 11, loss = 121129490.14554873\n",
      "Iteration 12, loss = 121270289.25192058\n",
      "Iteration 13, loss = 121118715.44201370\n",
      "Iteration 14, loss = 121052287.18797542\n",
      "Iteration 15, loss = 121078260.61161411\n",
      "Iteration 16, loss = 121053691.60919032\n",
      "Iteration 17, loss = 121074418.05122995\n",
      "Iteration 18, loss = 121134917.19537337\n",
      "Iteration 19, loss = 121200031.16345815\n",
      "Iteration 20, loss = 121071863.72883940\n",
      "Iteration 21, loss = 121119369.84093308\n",
      "Iteration 22, loss = 121012779.55052805\n",
      "Iteration 23, loss = 121017060.17424126\n",
      "Iteration 24, loss = 121047943.96376899\n",
      "Iteration 25, loss = 121184795.82101113\n",
      "Iteration 26, loss = 121165647.15800393\n",
      "Iteration 27, loss = 121090211.63361727\n",
      "Iteration 28, loss = 121015388.89918931\n",
      "Iteration 29, loss = 121060824.06616543\n",
      "Iteration 30, loss = 121004902.66139899\n",
      "Iteration 31, loss = 121098453.58385473\n",
      "Iteration 32, loss = 121002848.63206692\n",
      "Iteration 33, loss = 121009550.95909023\n",
      "Iteration 34, loss = 121038855.84994522\n",
      "Iteration 35, loss = 121166361.28179051\n",
      "Iteration 36, loss = 121004042.18058842\n",
      "Iteration 37, loss = 121219787.06706463\n",
      "Iteration 38, loss = 121096424.11679445\n",
      "Iteration 39, loss = 120943854.95016254\n",
      "Iteration 40, loss = 121064180.00340454\n",
      "Iteration 41, loss = 121016995.25779308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 120961383.76731321\n",
      "Iteration 43, loss = 121074001.26561728\n",
      "Iteration 44, loss = 120945064.10894760\n",
      "Iteration 45, loss = 121033047.85633765\n",
      "Iteration 46, loss = 120909809.05966792\n",
      "Iteration 47, loss = 120964939.88517815\n",
      "Iteration 48, loss = 120859874.68169209\n",
      "Iteration 49, loss = 120815819.47636807\n",
      "Iteration 50, loss = 121070914.70755191\n",
      "Iteration 51, loss = 121037107.10577603\n",
      "Iteration 52, loss = 120807808.76543038\n",
      "Iteration 53, loss = 121062711.23145190\n",
      "Iteration 54, loss = 121034119.67360306\n",
      "Iteration 55, loss = 120936865.23566462\n",
      "Iteration 56, loss = 120914334.32419194\n",
      "Iteration 57, loss = 121024968.04206114\n",
      "Iteration 58, loss = 120892056.84996182\n",
      "Iteration 59, loss = 121009900.43225461\n",
      "Iteration 60, loss = 120878078.51177490\n",
      "Iteration 61, loss = 120886872.73793344\n",
      "Iteration 62, loss = 120932023.32110320\n",
      "Iteration 63, loss = 120822621.55635533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 260100843.70044664\n",
      "Iteration 2, loss = 249108575.63699028\n",
      "Iteration 3, loss = 248883625.80836391\n",
      "Iteration 4, loss = 248866698.17537192\n",
      "Iteration 5, loss = 248761398.47417006\n",
      "Iteration 6, loss = 248816047.58010298\n",
      "Iteration 7, loss = 248793000.62286744\n",
      "Iteration 8, loss = 248663303.17265299\n",
      "Iteration 9, loss = 248687031.31579429\n",
      "Iteration 10, loss = 248684286.56054398\n",
      "Iteration 11, loss = 248802675.62526435\n",
      "Iteration 12, loss = 248601399.43823344\n",
      "Iteration 13, loss = 248577766.10999501\n",
      "Iteration 14, loss = 248641199.50802591\n",
      "Iteration 15, loss = 248589347.53300092\n",
      "Iteration 16, loss = 248580589.30258727\n",
      "Iteration 17, loss = 248549194.34115112\n",
      "Iteration 18, loss = 248587979.30313638\n",
      "Iteration 19, loss = 248539718.88487867\n",
      "Iteration 20, loss = 248590296.49716246\n",
      "Iteration 21, loss = 248544431.07884809\n",
      "Iteration 22, loss = 248575631.76291347\n",
      "Iteration 23, loss = 248552550.07229087\n",
      "Iteration 24, loss = 248598570.22020194\n",
      "Iteration 25, loss = 248587901.32083461\n",
      "Iteration 26, loss = 248529785.59334645\n",
      "Iteration 27, loss = 248545437.50895816\n",
      "Iteration 28, loss = 248577329.95052871\n",
      "Iteration 29, loss = 248505881.82370844\n",
      "Iteration 30, loss = 248472864.45936924\n",
      "Iteration 31, loss = 248544086.11512911\n",
      "Iteration 32, loss = 248577645.14636129\n",
      "Iteration 33, loss = 248564007.35844940\n",
      "Iteration 34, loss = 248493555.26138955\n",
      "Iteration 35, loss = 248543524.53141192\n",
      "Iteration 36, loss = 248440153.29548642\n",
      "Iteration 37, loss = 248482512.77799264\n",
      "Iteration 38, loss = 248366812.52072042\n",
      "Iteration 39, loss = 248565826.65849361\n",
      "Iteration 40, loss = 248412916.36117294\n",
      "Iteration 41, loss = 248529967.38862699\n",
      "Iteration 42, loss = 248573976.23204181\n",
      "Iteration 43, loss = 248583523.62242132\n",
      "Iteration 44, loss = 248417233.78089505\n",
      "Iteration 45, loss = 248489779.72656351\n",
      "Iteration 46, loss = 248499596.25294676\n",
      "Iteration 47, loss = 248507068.32977125\n",
      "Iteration 48, loss = 248488769.28651640\n",
      "Iteration 49, loss = 248608423.68396401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 698983292.50592828\n",
      "Iteration 2, loss = 520842374.68750185\n",
      "Iteration 3, loss = 412221191.03564900\n",
      "Iteration 4, loss = 349767733.32934743\n",
      "Iteration 5, loss = 322490547.52742034\n",
      "Iteration 6, loss = 314113488.99060917\n",
      "Iteration 7, loss = 312420570.23489666\n",
      "Iteration 8, loss = 311992939.73326582\n",
      "Iteration 9, loss = 311840275.34531319\n",
      "Iteration 10, loss = 311758612.61796355\n",
      "Iteration 11, loss = 311635409.20201021\n",
      "Iteration 12, loss = 311562639.74060762\n",
      "Iteration 13, loss = 311454917.25396794\n",
      "Iteration 14, loss = 311353130.36993045\n",
      "Iteration 15, loss = 311300538.73669839\n",
      "Iteration 16, loss = 311277751.41060394\n",
      "Iteration 17, loss = 311223030.99448222\n",
      "Iteration 18, loss = 311120215.70581698\n",
      "Iteration 19, loss = 311052921.67656648\n",
      "Iteration 20, loss = 311028483.35322857\n",
      "Iteration 21, loss = 311027066.67087799\n",
      "Iteration 22, loss = 311009051.73820674\n",
      "Iteration 23, loss = 310927789.92521113\n",
      "Iteration 24, loss = 310928345.43314129\n",
      "Iteration 25, loss = 310933498.79518610\n",
      "Iteration 26, loss = 310869414.95149672\n",
      "Iteration 27, loss = 310942417.55506474\n",
      "Iteration 28, loss = 310956295.02310735\n",
      "Iteration 29, loss = 310823774.77823442\n",
      "Iteration 30, loss = 310896223.29378277\n",
      "Iteration 31, loss = 310899336.89753473\n",
      "Iteration 32, loss = 310812262.10996747\n",
      "Iteration 33, loss = 310816664.05397040\n",
      "Iteration 34, loss = 310815349.59662819\n",
      "Iteration 35, loss = 310858322.64035058\n",
      "Iteration 36, loss = 310860010.50676638\n",
      "Iteration 37, loss = 310775900.11466908\n",
      "Iteration 38, loss = 310834200.15508837\n",
      "Iteration 39, loss = 310774800.02469969\n",
      "Iteration 40, loss = 310817042.99172324\n",
      "Iteration 41, loss = 310786611.89589757\n",
      "Iteration 42, loss = 310773136.51714164\n",
      "Iteration 43, loss = 310933102.13014394\n",
      "Iteration 44, loss = 310759373.95093787\n",
      "Iteration 45, loss = 310861743.52576774\n",
      "Iteration 46, loss = 310824920.12455946\n",
      "Iteration 47, loss = 310843638.58349341\n",
      "Iteration 48, loss = 310809789.48771554\n",
      "Iteration 49, loss = 310812494.06472576\n",
      "Iteration 50, loss = 310797311.79748356\n",
      "Iteration 51, loss = 310801492.17167014\n",
      "Iteration 52, loss = 310837417.50968969\n",
      "Iteration 53, loss = 310801534.11889511\n",
      "Iteration 54, loss = 310789721.94495237\n",
      "Iteration 55, loss = 310790861.06387669\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 347901278.62809187\n",
      "Iteration 2, loss = 302127656.72472656\n",
      "Iteration 3, loss = 297507259.74392706\n",
      "Iteration 4, loss = 294848109.20834750\n",
      "Iteration 5, loss = 293153627.82113284\n",
      "Iteration 6, loss = 292294723.24471599\n",
      "Iteration 7, loss = 292015298.32879919\n",
      "Iteration 8, loss = 291711527.49014401\n",
      "Iteration 9, loss = 291730695.26056695\n",
      "Iteration 10, loss = 291964538.64335161\n",
      "Iteration 11, loss = 291671172.62983298\n",
      "Iteration 12, loss = 291597584.40770721\n",
      "Iteration 13, loss = 291557662.75865251\n",
      "Iteration 14, loss = 291860275.67216939\n",
      "Iteration 15, loss = 291456240.33687431\n",
      "Iteration 16, loss = 291759156.51407766\n",
      "Iteration 17, loss = 291859132.33439010\n",
      "Iteration 18, loss = 291977604.77311951\n",
      "Iteration 19, loss = 291804750.80993718\n",
      "Iteration 20, loss = 291653175.72102910\n",
      "Iteration 21, loss = 291587149.80690002\n",
      "Iteration 22, loss = 292116358.30353850\n",
      "Iteration 23, loss = 292211591.38122940\n",
      "Iteration 24, loss = 291628001.22342914\n",
      "Iteration 25, loss = 291554959.57118469\n",
      "Iteration 26, loss = 291513887.40996385\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 180666973.34019279\n",
      "Iteration 2, loss = 178352664.30650580\n",
      "Iteration 3, loss = 177923015.92087311\n",
      "Iteration 4, loss = 177859791.40915719\n",
      "Iteration 5, loss = 177896553.01876840\n",
      "Iteration 6, loss = 177839048.55315819\n",
      "Iteration 7, loss = 177766728.20987901\n",
      "Iteration 8, loss = 177764358.61737567\n",
      "Iteration 9, loss = 177867768.97580245\n",
      "Iteration 10, loss = 177861156.24377486\n",
      "Iteration 11, loss = 177660587.19309384\n",
      "Iteration 12, loss = 177835012.52262813\n",
      "Iteration 13, loss = 177924709.63578209\n",
      "Iteration 14, loss = 177921250.63562730\n",
      "Iteration 15, loss = 177679466.27879459\n",
      "Iteration 16, loss = 177836988.10336557\n",
      "Iteration 17, loss = 177898409.47577721\n",
      "Iteration 18, loss = 177662755.60530066\n",
      "Iteration 19, loss = 177708732.40775466\n",
      "Iteration 20, loss = 177761065.75892711\n",
      "Iteration 21, loss = 177705901.63409188\n",
      "Iteration 22, loss = 177705209.38575804\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1195618224.68300009\n",
      "Iteration 2, loss = 349020929.76773125\n",
      "Iteration 3, loss = 231954652.82977962\n",
      "Iteration 4, loss = 226506611.00672105\n",
      "Iteration 5, loss = 225898456.78156617\n",
      "Iteration 6, loss = 225242211.30879873\n",
      "Iteration 7, loss = 224696605.30236846\n",
      "Iteration 8, loss = 224246267.81592286\n",
      "Iteration 9, loss = 223771845.58057466\n",
      "Iteration 10, loss = 223388849.11057201\n",
      "Iteration 11, loss = 223129136.81820771\n",
      "Iteration 12, loss = 222845545.84914979\n",
      "Iteration 13, loss = 222634828.68329468\n",
      "Iteration 14, loss = 222490047.55226079\n",
      "Iteration 15, loss = 222409969.57888487\n",
      "Iteration 16, loss = 222313778.26207852\n",
      "Iteration 17, loss = 222210097.70979756\n",
      "Iteration 18, loss = 222169569.53310299\n",
      "Iteration 19, loss = 222162692.04716364\n",
      "Iteration 20, loss = 222091338.74852574\n",
      "Iteration 21, loss = 222114593.96661493\n",
      "Iteration 22, loss = 222139898.67730391\n",
      "Iteration 23, loss = 222047485.01563340\n",
      "Iteration 24, loss = 222037380.65139097\n",
      "Iteration 25, loss = 222037212.97166798\n",
      "Iteration 26, loss = 222015999.68141562\n",
      "Iteration 27, loss = 221996793.65203026\n",
      "Iteration 28, loss = 222052166.06620148\n",
      "Iteration 29, loss = 222013453.89381489\n",
      "Iteration 30, loss = 221994155.97197509\n",
      "Iteration 31, loss = 222062891.43038002\n",
      "Iteration 32, loss = 221976683.10378885\n",
      "Iteration 33, loss = 221924071.12608281\n",
      "Iteration 34, loss = 222164132.04887930\n",
      "Iteration 35, loss = 221964901.95732665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 222005065.53181577\n",
      "Iteration 37, loss = 222015249.97312304\n",
      "Iteration 38, loss = 222128799.35797188\n",
      "Iteration 39, loss = 222166719.50702846\n",
      "Iteration 40, loss = 222137328.60469893\n",
      "Iteration 41, loss = 221936511.27265245\n",
      "Iteration 42, loss = 221879817.69465694\n",
      "Iteration 43, loss = 221933863.99373731\n",
      "Iteration 44, loss = 221957820.12435675\n",
      "Iteration 45, loss = 221906904.97041908\n",
      "Iteration 46, loss = 222003285.76719081\n",
      "Iteration 47, loss = 221913345.87010914\n",
      "Iteration 48, loss = 222192837.70602423\n",
      "Iteration 49, loss = 221958743.00307807\n",
      "Iteration 50, loss = 221877151.57528290\n",
      "Iteration 51, loss = 221970752.87254611\n",
      "Iteration 52, loss = 221799230.53786445\n",
      "Iteration 53, loss = 221933452.79924679\n",
      "Iteration 54, loss = 221913034.91549623\n",
      "Iteration 55, loss = 221843419.93583781\n",
      "Iteration 56, loss = 221999220.02796349\n",
      "Iteration 57, loss = 221770028.76125944\n",
      "Iteration 58, loss = 222001827.09604049\n",
      "Iteration 59, loss = 221835769.66067985\n",
      "Iteration 60, loss = 221883120.18183658\n",
      "Iteration 61, loss = 221813807.95123848\n",
      "Iteration 62, loss = 221862155.63343522\n",
      "Iteration 63, loss = 221819823.67910063\n",
      "Iteration 64, loss = 221758549.91631696\n",
      "Iteration 65, loss = 221762581.60108081\n",
      "Iteration 66, loss = 222130024.09808198\n",
      "Iteration 67, loss = 221758976.51611742\n",
      "Iteration 68, loss = 221971295.28766766\n",
      "Iteration 69, loss = 221858801.36549541\n",
      "Iteration 70, loss = 221853855.25972438\n",
      "Iteration 71, loss = 221919596.95136958\n",
      "Iteration 72, loss = 221805404.95646879\n",
      "Iteration 73, loss = 221901808.70716909\n",
      "Iteration 74, loss = 221787116.91335112\n",
      "Iteration 75, loss = 221752644.69613951\n",
      "Iteration 76, loss = 221591704.33675805\n",
      "Iteration 77, loss = 221755297.87114891\n",
      "Iteration 78, loss = 221693789.71241090\n",
      "Iteration 79, loss = 221699104.49699146\n",
      "Iteration 80, loss = 221174456.28952172\n",
      "Iteration 81, loss = 221997118.31025234\n",
      "Iteration 82, loss = 221580046.08823043\n",
      "Iteration 83, loss = 221721376.94196048\n",
      "Iteration 84, loss = 221707373.52908877\n",
      "Iteration 85, loss = 221437863.03805792\n",
      "Iteration 86, loss = 221680351.23219806\n",
      "Iteration 87, loss = 221567876.39023960\n",
      "Iteration 88, loss = 221447848.69176489\n",
      "Iteration 89, loss = 221732360.06644061\n",
      "Iteration 90, loss = 221546638.61118746\n",
      "Iteration 91, loss = 221751819.62946054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1273162957.46352100\n",
      "Iteration 2, loss = 523657953.06740469\n",
      "Iteration 3, loss = 265299038.82144210\n",
      "Iteration 4, loss = 200842365.96162224\n",
      "Iteration 5, loss = 190374141.02560687\n",
      "Iteration 6, loss = 188836846.80538908\n",
      "Iteration 7, loss = 188279954.17805791\n",
      "Iteration 8, loss = 188023921.24911496\n",
      "Iteration 9, loss = 187843285.17581904\n",
      "Iteration 10, loss = 187747255.26733235\n",
      "Iteration 11, loss = 187633588.12868428\n",
      "Iteration 12, loss = 187503597.63617185\n",
      "Iteration 13, loss = 187453430.51489723\n",
      "Iteration 14, loss = 187393260.07267484\n",
      "Iteration 15, loss = 187340730.13742411\n",
      "Iteration 16, loss = 187230664.47521526\n",
      "Iteration 17, loss = 187129518.28459150\n",
      "Iteration 18, loss = 187009546.05102995\n",
      "Iteration 19, loss = 187000699.65556684\n",
      "Iteration 20, loss = 186847791.89816162\n",
      "Iteration 21, loss = 186833520.44831863\n",
      "Iteration 22, loss = 186756814.51682228\n",
      "Iteration 23, loss = 186766460.40716460\n",
      "Iteration 24, loss = 186605149.09501806\n",
      "Iteration 25, loss = 186551435.97081301\n",
      "Iteration 26, loss = 186503020.83693385\n",
      "Iteration 27, loss = 186423317.29424062\n",
      "Iteration 28, loss = 186354234.75135168\n",
      "Iteration 29, loss = 186366532.53103745\n",
      "Iteration 30, loss = 186393463.03425235\n",
      "Iteration 31, loss = 186352301.52595600\n",
      "Iteration 32, loss = 186208885.65993544\n",
      "Iteration 33, loss = 186365960.69585252\n",
      "Iteration 34, loss = 186284441.08716807\n",
      "Iteration 35, loss = 186225746.37342411\n",
      "Iteration 36, loss = 186180968.86362532\n",
      "Iteration 37, loss = 186163300.59406745\n",
      "Iteration 38, loss = 185977379.89103684\n",
      "Iteration 39, loss = 186103673.36138204\n",
      "Iteration 40, loss = 186079801.24071905\n",
      "Iteration 41, loss = 185920291.40843540\n",
      "Iteration 42, loss = 185854464.48199752\n",
      "Iteration 43, loss = 185954777.86707708\n",
      "Iteration 44, loss = 185958792.34792244\n",
      "Iteration 45, loss = 185820222.07593402\n",
      "Iteration 46, loss = 185855627.09217626\n",
      "Iteration 47, loss = 185782208.45630839\n",
      "Iteration 48, loss = 185741655.67967108\n",
      "Iteration 49, loss = 185722977.00613701\n",
      "Iteration 50, loss = 185633777.71267015\n",
      "Iteration 51, loss = 185609295.41097382\n",
      "Iteration 52, loss = 185591113.56905448\n",
      "Iteration 53, loss = 185555103.24599141\n",
      "Iteration 54, loss = 185673767.75084475\n",
      "Iteration 55, loss = 185445774.85727179\n",
      "Iteration 56, loss = 185515591.96445921\n",
      "Iteration 57, loss = 185411411.09103474\n",
      "Iteration 58, loss = 185349840.86496171\n",
      "Iteration 59, loss = 185348252.94845349\n",
      "Iteration 60, loss = 185430940.79193980\n",
      "Iteration 61, loss = 185347635.59948790\n",
      "Iteration 62, loss = 185271122.47314015\n",
      "Iteration 63, loss = 185228688.92106208\n",
      "Iteration 64, loss = 185191081.23092416\n",
      "Iteration 65, loss = 185145114.84381330\n",
      "Iteration 66, loss = 185296208.27733615\n",
      "Iteration 67, loss = 185090479.67357084\n",
      "Iteration 68, loss = 185028152.55624574\n",
      "Iteration 69, loss = 184962114.99656639\n",
      "Iteration 70, loss = 185037714.79039556\n",
      "Iteration 71, loss = 184929297.70223910\n",
      "Iteration 72, loss = 184926948.48944697\n",
      "Iteration 73, loss = 184911472.71120298\n",
      "Iteration 74, loss = 184762235.38424310\n",
      "Iteration 75, loss = 184723952.76001114\n",
      "Iteration 76, loss = 184628506.86034805\n",
      "Iteration 77, loss = 184687004.06692111\n",
      "Iteration 78, loss = 184739288.87614727\n",
      "Iteration 79, loss = 184460401.06870091\n",
      "Iteration 80, loss = 184655134.67402372\n",
      "Iteration 81, loss = 184652044.02139586\n",
      "Iteration 82, loss = 184405816.98485181\n",
      "Iteration 83, loss = 184664625.99578115\n",
      "Iteration 84, loss = 184376177.55010584\n",
      "Iteration 85, loss = 184357358.96829230\n",
      "Iteration 86, loss = 184172877.25522074\n",
      "Iteration 87, loss = 184225152.33952162\n",
      "Iteration 88, loss = 184111308.35111344\n",
      "Iteration 89, loss = 184342288.71843034\n",
      "Iteration 90, loss = 184034389.48772183\n",
      "Iteration 91, loss = 183965862.80455929\n",
      "Iteration 92, loss = 183967230.92489514\n",
      "Iteration 93, loss = 183956266.36570254\n",
      "Iteration 94, loss = 183845897.31200981\n",
      "Iteration 95, loss = 183846962.38846132\n",
      "Iteration 96, loss = 183952529.85809490\n",
      "Iteration 97, loss = 183911720.68641913\n",
      "Iteration 98, loss = 183636788.16851097\n",
      "Iteration 99, loss = 183603230.20869425\n",
      "Iteration 100, loss = 183524644.91095999\n",
      "Iteration 101, loss = 183766952.37837481\n",
      "Iteration 102, loss = 183312089.10733727\n",
      "Iteration 103, loss = 183494080.92484576\n",
      "Iteration 104, loss = 183301638.25531837\n",
      "Iteration 105, loss = 183535290.69060439\n",
      "Iteration 106, loss = 183370041.57989332\n",
      "Iteration 107, loss = 183147716.16764650\n",
      "Iteration 108, loss = 183312509.38260448\n",
      "Iteration 109, loss = 183560447.29974812\n",
      "Iteration 110, loss = 183083592.59966290\n",
      "Iteration 111, loss = 183480103.64940003\n",
      "Iteration 112, loss = 183013435.99117288\n",
      "Iteration 113, loss = 183160701.33888230\n",
      "Iteration 114, loss = 182893769.39530948\n",
      "Iteration 115, loss = 183032336.77223107\n",
      "Iteration 116, loss = 182905065.76975307\n",
      "Iteration 117, loss = 182865842.63817781\n",
      "Iteration 118, loss = 182605309.77493241\n",
      "Iteration 119, loss = 183626363.39940244\n",
      "Iteration 120, loss = 182526466.55976376\n",
      "Iteration 121, loss = 182775708.59116530\n",
      "Iteration 122, loss = 182533829.14641288\n",
      "Iteration 123, loss = 182535898.59174910\n",
      "Iteration 124, loss = 182583043.38665196\n",
      "Iteration 125, loss = 182372333.09144065\n",
      "Iteration 126, loss = 182484438.68613213\n",
      "Iteration 127, loss = 182591886.46305120\n",
      "Iteration 128, loss = 182794308.10896960\n",
      "Iteration 129, loss = 182522882.72349334\n",
      "Iteration 130, loss = 182231263.54061297\n",
      "Iteration 131, loss = 182154519.91809756\n",
      "Iteration 132, loss = 182214138.35344142\n",
      "Iteration 133, loss = 182128316.42948231\n",
      "Iteration 134, loss = 182382090.15162182\n",
      "Iteration 135, loss = 181843206.22159013\n",
      "Iteration 136, loss = 182095014.42687294\n",
      "Iteration 137, loss = 182004821.79099184\n",
      "Iteration 138, loss = 182021110.13907409\n",
      "Iteration 139, loss = 181925270.67540511\n",
      "Iteration 140, loss = 181979649.69611871\n",
      "Iteration 141, loss = 181693588.69167966\n",
      "Iteration 142, loss = 181831055.06838304\n",
      "Iteration 143, loss = 181525621.92989379\n",
      "Iteration 144, loss = 181809669.48909131\n",
      "Iteration 145, loss = 181623508.15115067\n",
      "Iteration 146, loss = 181554780.68070391\n",
      "Iteration 147, loss = 181466416.77058673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 148, loss = 181610501.80167603\n",
      "Iteration 149, loss = 181506211.71481061\n",
      "Iteration 150, loss = 181159798.75701872\n",
      "Iteration 151, loss = 181432771.33940184\n",
      "Iteration 152, loss = 181309564.49920738\n",
      "Iteration 153, loss = 181335234.40492660\n",
      "Iteration 154, loss = 181259923.23862186\n",
      "Iteration 155, loss = 181124089.05534750\n",
      "Iteration 156, loss = 181076719.24644619\n",
      "Iteration 157, loss = 181299307.46969366\n",
      "Iteration 158, loss = 181397253.55353180\n",
      "Iteration 159, loss = 181319561.55990270\n",
      "Iteration 160, loss = 181187535.38165140\n",
      "Iteration 161, loss = 181006743.61722004\n",
      "Iteration 162, loss = 180753174.75137910\n",
      "Iteration 163, loss = 180783625.85287693\n",
      "Iteration 164, loss = 181114490.22281080\n",
      "Iteration 165, loss = 180765292.89632541\n",
      "Iteration 166, loss = 180832664.61601430\n",
      "Iteration 167, loss = 180729663.14736578\n",
      "Iteration 168, loss = 180622590.98191246\n",
      "Iteration 169, loss = 180580470.82269087\n",
      "Iteration 170, loss = 180990579.20663095\n",
      "Iteration 171, loss = 180729193.55564278\n",
      "Iteration 172, loss = 180703786.03784376\n",
      "Iteration 173, loss = 180589800.07032922\n",
      "Iteration 174, loss = 180798070.63270071\n",
      "Iteration 175, loss = 180789822.37553149\n",
      "Iteration 176, loss = 180867443.53618589\n",
      "Iteration 177, loss = 180572213.29358011\n",
      "Iteration 178, loss = 180578531.16800827\n",
      "Iteration 179, loss = 180350404.99325582\n",
      "Iteration 180, loss = 180353534.47738191\n",
      "Iteration 181, loss = 180245068.78466004\n",
      "Iteration 182, loss = 180155258.84938422\n",
      "Iteration 183, loss = 180188384.57229048\n",
      "Iteration 184, loss = 180369359.82364878\n",
      "Iteration 185, loss = 180443508.34145674\n",
      "Iteration 186, loss = 179992906.29828459\n",
      "Iteration 187, loss = 180065998.13334271\n",
      "Iteration 188, loss = 179988061.26193240\n",
      "Iteration 189, loss = 179950184.50088894\n",
      "Iteration 190, loss = 180015858.87720364\n",
      "Iteration 191, loss = 179858342.96686253\n",
      "Iteration 192, loss = 180171650.04095671\n",
      "Iteration 193, loss = 180044439.19104928\n",
      "Iteration 194, loss = 180116583.70794618\n",
      "Iteration 195, loss = 180253572.98822099\n",
      "Iteration 196, loss = 179708893.45691326\n",
      "Iteration 197, loss = 179906309.73082545\n",
      "Iteration 198, loss = 179882372.61714727\n",
      "Iteration 199, loss = 180215274.94939518\n",
      "Iteration 200, loss = 179667248.87539437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 227851353.02548301\n",
      "Iteration 2, loss = 145699478.08982742\n",
      "Iteration 3, loss = 131855909.87937492\n",
      "Iteration 4, loss = 128758398.70308530\n",
      "Iteration 5, loss = 128390375.62955803\n",
      "Iteration 6, loss = 128197187.59400883\n",
      "Iteration 7, loss = 127890567.75075823\n",
      "Iteration 8, loss = 127656813.64983155\n",
      "Iteration 9, loss = 127474520.95875373\n",
      "Iteration 10, loss = 127304170.46247576\n",
      "Iteration 11, loss = 127177176.58536926\n",
      "Iteration 12, loss = 127031005.18972771\n",
      "Iteration 13, loss = 126885960.99574395\n",
      "Iteration 14, loss = 126796233.09769529\n",
      "Iteration 15, loss = 126649156.41238545\n",
      "Iteration 16, loss = 126582850.77794880\n",
      "Iteration 17, loss = 126459309.19892569\n",
      "Iteration 18, loss = 126395402.67099874\n",
      "Iteration 19, loss = 126301488.00558901\n",
      "Iteration 20, loss = 126182421.35911138\n",
      "Iteration 21, loss = 126145571.02730677\n",
      "Iteration 22, loss = 126055884.12164070\n",
      "Iteration 23, loss = 126015685.96443731\n",
      "Iteration 24, loss = 125944255.62239629\n",
      "Iteration 25, loss = 125858465.48536582\n",
      "Iteration 26, loss = 125958944.64736211\n",
      "Iteration 27, loss = 125725855.90631436\n",
      "Iteration 28, loss = 125701458.40846421\n",
      "Iteration 29, loss = 125637821.94967027\n",
      "Iteration 30, loss = 125659855.74794595\n",
      "Iteration 31, loss = 125628018.72943383\n",
      "Iteration 32, loss = 125691779.02395970\n",
      "Iteration 33, loss = 125525908.73660220\n",
      "Iteration 34, loss = 125447430.65122619\n",
      "Iteration 35, loss = 125405886.30481941\n",
      "Iteration 36, loss = 125488402.79416348\n",
      "Iteration 37, loss = 125407934.90664378\n",
      "Iteration 38, loss = 125318117.81422129\n",
      "Iteration 39, loss = 125307112.43867886\n",
      "Iteration 40, loss = 125308486.24108565\n",
      "Iteration 41, loss = 125330386.71052103\n",
      "Iteration 42, loss = 125432299.08519448\n",
      "Iteration 43, loss = 125174945.66253787\n",
      "Iteration 44, loss = 125167511.40690555\n",
      "Iteration 45, loss = 125046250.59814288\n",
      "Iteration 46, loss = 124982727.27867776\n",
      "Iteration 47, loss = 125068679.85314497\n",
      "Iteration 48, loss = 125246337.65481636\n",
      "Iteration 49, loss = 125024403.67041934\n",
      "Iteration 50, loss = 124880717.22356850\n",
      "Iteration 51, loss = 124901061.80668360\n",
      "Iteration 52, loss = 124872446.69253406\n",
      "Iteration 53, loss = 124862776.54168338\n",
      "Iteration 54, loss = 124811197.43681428\n",
      "Iteration 55, loss = 124707912.22281267\n",
      "Iteration 56, loss = 124692785.81191878\n",
      "Iteration 57, loss = 124720258.00663486\n",
      "Iteration 58, loss = 124644839.26392388\n",
      "Iteration 59, loss = 124569519.37524851\n",
      "Iteration 60, loss = 124664902.43752976\n",
      "Iteration 61, loss = 124452028.24603215\n",
      "Iteration 62, loss = 124505877.55375879\n",
      "Iteration 63, loss = 124602908.59006682\n",
      "Iteration 64, loss = 124342378.47703621\n",
      "Iteration 65, loss = 124432324.84119560\n",
      "Iteration 66, loss = 124213215.08537224\n",
      "Iteration 67, loss = 124292147.83828950\n",
      "Iteration 68, loss = 124215146.24498042\n",
      "Iteration 69, loss = 124134230.21724485\n",
      "Iteration 70, loss = 124097285.98864815\n",
      "Iteration 71, loss = 124055715.35948677\n",
      "Iteration 72, loss = 124089184.13441685\n",
      "Iteration 73, loss = 124034281.02908508\n",
      "Iteration 74, loss = 123992409.52458869\n",
      "Iteration 75, loss = 123885625.09301697\n",
      "Iteration 76, loss = 123902479.76992966\n",
      "Iteration 77, loss = 123820018.77876890\n",
      "Iteration 78, loss = 123863498.41307537\n",
      "Iteration 79, loss = 123892479.28144932\n",
      "Iteration 80, loss = 123752485.17143047\n",
      "Iteration 81, loss = 123775475.29508828\n",
      "Iteration 82, loss = 123621678.40464258\n",
      "Iteration 83, loss = 123591018.97027966\n",
      "Iteration 84, loss = 123590829.35640867\n",
      "Iteration 85, loss = 123498245.09497869\n",
      "Iteration 86, loss = 123423452.08947271\n",
      "Iteration 87, loss = 123523558.91518743\n",
      "Iteration 88, loss = 123611046.10872220\n",
      "Iteration 89, loss = 123348259.89933677\n",
      "Iteration 90, loss = 123711547.26297277\n",
      "Iteration 91, loss = 123273774.27393109\n",
      "Iteration 92, loss = 123294345.50939673\n",
      "Iteration 93, loss = 123245575.08552632\n",
      "Iteration 94, loss = 123317078.55060656\n",
      "Iteration 95, loss = 123189590.28687659\n",
      "Iteration 96, loss = 123188268.21861516\n",
      "Iteration 97, loss = 123207760.16423355\n",
      "Iteration 98, loss = 123058880.08659707\n",
      "Iteration 99, loss = 123132275.59131287\n",
      "Iteration 100, loss = 123070373.73427497\n",
      "Iteration 101, loss = 123052825.97270295\n",
      "Iteration 102, loss = 123177320.65379046\n",
      "Iteration 103, loss = 123050670.34870954\n",
      "Iteration 104, loss = 122930595.55987304\n",
      "Iteration 105, loss = 122914263.99724592\n",
      "Iteration 106, loss = 123074556.64016663\n",
      "Iteration 107, loss = 122734052.26355764\n",
      "Iteration 108, loss = 122776926.63318318\n",
      "Iteration 109, loss = 122774828.61049509\n",
      "Iteration 110, loss = 122656342.58729216\n",
      "Iteration 111, loss = 122625163.93407634\n",
      "Iteration 112, loss = 122614168.69788443\n",
      "Iteration 113, loss = 122648020.75640865\n",
      "Iteration 114, loss = 122567924.21535502\n",
      "Iteration 115, loss = 122609219.23910479\n",
      "Iteration 116, loss = 122456883.04065584\n",
      "Iteration 117, loss = 122615472.99949111\n",
      "Iteration 118, loss = 122568489.92912418\n",
      "Iteration 119, loss = 122484186.69207625\n",
      "Iteration 120, loss = 122370764.98130798\n",
      "Iteration 121, loss = 122343371.77574667\n",
      "Iteration 122, loss = 122302622.43884683\n",
      "Iteration 123, loss = 122339681.23823673\n",
      "Iteration 124, loss = 122300581.25145188\n",
      "Iteration 125, loss = 122256547.62749338\n",
      "Iteration 126, loss = 122213657.71085478\n",
      "Iteration 127, loss = 122077886.87873492\n",
      "Iteration 128, loss = 122175580.82243599\n",
      "Iteration 129, loss = 122441759.13477011\n",
      "Iteration 130, loss = 121901699.12893245\n",
      "Iteration 131, loss = 122027594.69041607\n",
      "Iteration 132, loss = 122112100.95800452\n",
      "Iteration 133, loss = 122043871.26726392\n",
      "Iteration 134, loss = 122134463.71671423\n",
      "Iteration 135, loss = 121955765.25178321\n",
      "Iteration 136, loss = 121791146.63826895\n",
      "Iteration 137, loss = 122392014.65947470\n",
      "Iteration 138, loss = 121921215.11965716\n",
      "Iteration 139, loss = 121813386.18863201\n",
      "Iteration 140, loss = 121831559.12771462\n",
      "Iteration 141, loss = 121791213.79904488\n",
      "Iteration 142, loss = 121734708.32276873\n",
      "Iteration 143, loss = 121641314.26171480\n",
      "Iteration 144, loss = 121639787.04168445\n",
      "Iteration 145, loss = 121723869.52105319\n",
      "Iteration 146, loss = 121648256.63118328\n",
      "Iteration 147, loss = 121691732.94381055\n",
      "Iteration 148, loss = 121572485.27075104\n",
      "Iteration 149, loss = 121578300.94189648\n",
      "Iteration 150, loss = 121519129.19575112\n",
      "Iteration 151, loss = 121609850.40554057\n",
      "Iteration 152, loss = 121530017.91376573\n",
      "Iteration 153, loss = 121437546.16678816\n",
      "Iteration 154, loss = 121539584.56534843\n",
      "Iteration 155, loss = 121547759.33677757\n",
      "Iteration 156, loss = 121548959.85469422\n",
      "Iteration 157, loss = 121311385.31697205\n",
      "Iteration 158, loss = 121361690.74967000\n",
      "Iteration 159, loss = 121286774.25918940\n",
      "Iteration 160, loss = 121318406.06278379\n",
      "Iteration 161, loss = 121206114.13450997\n",
      "Iteration 162, loss = 121331044.86955974\n",
      "Iteration 163, loss = 121253478.58445752\n",
      "Iteration 164, loss = 121158287.23156992\n",
      "Iteration 165, loss = 121230627.47373304\n",
      "Iteration 166, loss = 121234742.37110576\n",
      "Iteration 167, loss = 121171430.86232893\n",
      "Iteration 168, loss = 121189329.52915037\n",
      "Iteration 169, loss = 121100822.48719844\n",
      "Iteration 170, loss = 121323309.46187288\n",
      "Iteration 171, loss = 121197530.97469351\n",
      "Iteration 172, loss = 121068106.45805527\n",
      "Iteration 173, loss = 121177936.72205305\n",
      "Iteration 174, loss = 121147652.98494191\n",
      "Iteration 175, loss = 120924957.99805318\n",
      "Iteration 176, loss = 120870118.96665354\n",
      "Iteration 177, loss = 121019368.04734990\n",
      "Iteration 178, loss = 120931475.95587651\n",
      "Iteration 179, loss = 120921480.23601592\n",
      "Iteration 180, loss = 121005844.46733208\n",
      "Iteration 181, loss = 120834242.04521424\n",
      "Iteration 182, loss = 120862243.55916877\n",
      "Iteration 183, loss = 120996708.82303530\n",
      "Iteration 184, loss = 120910291.35741097\n",
      "Iteration 185, loss = 120856528.27261062\n",
      "Iteration 186, loss = 120747681.74000721\n",
      "Iteration 187, loss = 120849952.81553127\n",
      "Iteration 188, loss = 120775834.34004314\n",
      "Iteration 189, loss = 120747021.85893758\n",
      "Iteration 190, loss = 120786153.82742542\n",
      "Iteration 191, loss = 120697247.08190949\n",
      "Iteration 192, loss = 120727283.40476474\n",
      "Iteration 193, loss = 120694597.93831494\n",
      "Iteration 194, loss = 120701432.41873071\n",
      "Iteration 195, loss = 120541347.00316863\n",
      "Iteration 196, loss = 120700473.15545315\n",
      "Iteration 197, loss = 120578946.04284286\n",
      "Iteration 198, loss = 120598616.24256128\n",
      "Iteration 199, loss = 120806237.82504486\n",
      "Iteration 200, loss = 120974244.43718547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 906668646.23364389\n",
      "Iteration 2, loss = 542174404.39643729\n",
      "Iteration 3, loss = 354780806.23289651\n",
      "Iteration 4, loss = 259104554.65477428\n",
      "Iteration 5, loss = 244517510.50047129\n",
      "Iteration 6, loss = 243865690.85553026\n",
      "Iteration 7, loss = 243504200.42820936\n",
      "Iteration 8, loss = 243423265.73434365\n",
      "Iteration 9, loss = 243304655.08720663\n",
      "Iteration 10, loss = 243212864.04943153\n",
      "Iteration 11, loss = 243126719.46602070\n",
      "Iteration 12, loss = 243072315.74222416\n",
      "Iteration 13, loss = 243073129.39115870\n",
      "Iteration 14, loss = 243018981.10285401\n",
      "Iteration 15, loss = 242958730.10360923\n",
      "Iteration 16, loss = 242926805.99223080\n",
      "Iteration 17, loss = 242974513.55371290\n",
      "Iteration 18, loss = 242938725.21466166\n",
      "Iteration 19, loss = 242822894.84057856\n",
      "Iteration 20, loss = 242816559.25519520\n",
      "Iteration 21, loss = 242749832.81571230\n",
      "Iteration 22, loss = 242751720.73746583\n",
      "Iteration 23, loss = 242709589.70429781\n",
      "Iteration 24, loss = 242692280.91234091\n",
      "Iteration 25, loss = 242689198.68552858\n",
      "Iteration 26, loss = 242588013.46994746\n",
      "Iteration 27, loss = 242541419.22902289\n",
      "Iteration 28, loss = 242520781.18587819\n",
      "Iteration 29, loss = 242621758.15528408\n",
      "Iteration 30, loss = 242411177.75224659\n",
      "Iteration 31, loss = 242382934.40210727\n",
      "Iteration 32, loss = 242293652.59562731\n",
      "Iteration 33, loss = 242241881.50901046\n",
      "Iteration 34, loss = 242272326.27896622\n",
      "Iteration 35, loss = 242185806.91878858\n",
      "Iteration 36, loss = 242246654.76326934\n",
      "Iteration 37, loss = 242070699.36359721\n",
      "Iteration 38, loss = 242164500.47282603\n",
      "Iteration 39, loss = 242166467.94008902\n",
      "Iteration 40, loss = 241979300.02611792\n",
      "Iteration 41, loss = 241876141.92123047\n",
      "Iteration 42, loss = 241787859.97095758\n",
      "Iteration 43, loss = 241772005.28594381\n",
      "Iteration 44, loss = 241787936.66077936\n",
      "Iteration 45, loss = 241757252.24251026\n",
      "Iteration 46, loss = 241617064.04326767\n",
      "Iteration 47, loss = 241665244.28850660\n",
      "Iteration 48, loss = 241503598.19413075\n",
      "Iteration 49, loss = 241429077.86290562\n",
      "Iteration 50, loss = 241622853.50399274\n",
      "Iteration 51, loss = 241391219.03279179\n",
      "Iteration 52, loss = 241388491.25411966\n",
      "Iteration 53, loss = 241238499.80194804\n",
      "Iteration 54, loss = 241126272.71136358\n",
      "Iteration 55, loss = 241190122.10268173\n",
      "Iteration 56, loss = 241117719.82134232\n",
      "Iteration 57, loss = 240977184.72309121\n",
      "Iteration 58, loss = 240906721.88215739\n",
      "Iteration 59, loss = 240978458.06956875\n",
      "Iteration 60, loss = 240951764.41651499\n",
      "Iteration 61, loss = 240800043.64024657\n",
      "Iteration 62, loss = 240691915.67563149\n",
      "Iteration 63, loss = 240435349.83715394\n",
      "Iteration 64, loss = 240683366.30881050\n",
      "Iteration 65, loss = 240461059.33486089\n",
      "Iteration 66, loss = 240369296.21332431\n",
      "Iteration 67, loss = 240512397.55628636\n",
      "Iteration 68, loss = 240338497.30126664\n",
      "Iteration 69, loss = 240270804.85596520\n",
      "Iteration 70, loss = 240378861.09421527\n",
      "Iteration 71, loss = 240121175.41073769\n",
      "Iteration 72, loss = 240028715.46572274\n",
      "Iteration 73, loss = 240055276.79976010\n",
      "Iteration 74, loss = 239880072.20515987\n",
      "Iteration 75, loss = 239779063.20959297\n",
      "Iteration 76, loss = 239692652.10077378\n",
      "Iteration 77, loss = 239850460.96897733\n",
      "Iteration 78, loss = 239550558.20325717\n",
      "Iteration 79, loss = 239584659.30509806\n",
      "Iteration 80, loss = 239467261.55355054\n",
      "Iteration 81, loss = 239286068.80623847\n",
      "Iteration 82, loss = 239292864.78900138\n",
      "Iteration 83, loss = 239094760.15879396\n",
      "Iteration 84, loss = 239062593.11063510\n",
      "Iteration 85, loss = 238964889.38141322\n",
      "Iteration 86, loss = 238903427.15352178\n",
      "Iteration 87, loss = 238888789.62215039\n",
      "Iteration 88, loss = 238585757.78040645\n",
      "Iteration 89, loss = 238389518.86918041\n",
      "Iteration 90, loss = 238735736.11808383\n",
      "Iteration 91, loss = 238465929.78069672\n",
      "Iteration 92, loss = 238461053.99344918\n",
      "Iteration 93, loss = 238364551.71654540\n",
      "Iteration 94, loss = 238183606.04658544\n",
      "Iteration 95, loss = 238113719.61775181\n",
      "Iteration 96, loss = 238013040.63427261\n",
      "Iteration 97, loss = 237927848.43527153\n",
      "Iteration 98, loss = 237777347.49961382\n",
      "Iteration 99, loss = 237823843.38170299\n",
      "Iteration 100, loss = 237561693.27242011\n",
      "Iteration 101, loss = 237445160.85730675\n",
      "Iteration 102, loss = 237462705.17580834\n",
      "Iteration 103, loss = 237286569.07105079\n",
      "Iteration 104, loss = 237047692.93596205\n",
      "Iteration 105, loss = 237254763.95359766\n",
      "Iteration 106, loss = 236909543.84422284\n",
      "Iteration 107, loss = 236928768.74352539\n",
      "Iteration 108, loss = 236922121.69753647\n",
      "Iteration 109, loss = 236688368.71105194\n",
      "Iteration 110, loss = 236558468.49874535\n",
      "Iteration 111, loss = 236502610.95432803\n",
      "Iteration 112, loss = 236246442.13254091\n",
      "Iteration 113, loss = 236153715.42414057\n",
      "Iteration 114, loss = 235995332.41626957\n",
      "Iteration 115, loss = 235866009.03142738\n",
      "Iteration 116, loss = 235801605.90300131\n",
      "Iteration 117, loss = 235653578.02784222\n",
      "Iteration 118, loss = 235562041.33647558\n",
      "Iteration 119, loss = 235410439.67280024\n",
      "Iteration 120, loss = 235333255.65126869\n",
      "Iteration 121, loss = 235476216.87637797\n",
      "Iteration 122, loss = 235273202.79418555\n",
      "Iteration 123, loss = 234986045.64433691\n",
      "Iteration 124, loss = 234947338.07347101\n",
      "Iteration 125, loss = 234787205.82558906\n",
      "Iteration 126, loss = 234616538.91253266\n",
      "Iteration 127, loss = 234478378.54690668\n",
      "Iteration 128, loss = 234322584.80099458\n",
      "Iteration 129, loss = 234332530.41152987\n",
      "Iteration 130, loss = 234022776.87920806\n",
      "Iteration 131, loss = 234060177.20853862\n",
      "Iteration 132, loss = 233993005.80658725\n",
      "Iteration 133, loss = 233829364.36997324\n",
      "Iteration 134, loss = 233696020.22471002\n",
      "Iteration 135, loss = 233536571.52773726\n",
      "Iteration 136, loss = 233547092.15361515\n",
      "Iteration 137, loss = 233086882.39699468\n",
      "Iteration 138, loss = 233017955.47541997\n",
      "Iteration 139, loss = 233086162.52162412\n",
      "Iteration 140, loss = 232807831.47502378\n",
      "Iteration 141, loss = 232845680.91090107\n",
      "Iteration 142, loss = 232505405.31164694\n",
      "Iteration 143, loss = 232449406.33333212\n",
      "Iteration 144, loss = 232352351.63645878\n",
      "Iteration 145, loss = 232133489.59403315\n",
      "Iteration 146, loss = 231999895.82173780\n",
      "Iteration 147, loss = 231865027.94603124\n",
      "Iteration 148, loss = 231984472.72830078\n",
      "Iteration 149, loss = 231509627.74101797\n",
      "Iteration 150, loss = 231705062.97920093\n",
      "Iteration 151, loss = 232314719.02460894\n",
      "Iteration 152, loss = 231187172.50318667\n",
      "Iteration 153, loss = 231219840.20992202\n",
      "Iteration 154, loss = 231026255.59062490\n",
      "Iteration 155, loss = 230924105.42917466\n",
      "Iteration 156, loss = 230800503.88463837\n",
      "Iteration 157, loss = 230644538.32037646\n",
      "Iteration 158, loss = 230537033.97877124\n",
      "Iteration 159, loss = 230407533.31510678\n",
      "Iteration 160, loss = 230467422.63435712\n",
      "Iteration 161, loss = 230389325.17966977\n",
      "Iteration 162, loss = 230086003.69976875\n",
      "Iteration 163, loss = 229887201.63332388\n",
      "Iteration 164, loss = 230088911.31159487\n",
      "Iteration 165, loss = 229733409.37284473\n",
      "Iteration 166, loss = 229537004.86332798\n",
      "Iteration 167, loss = 229385971.94503585\n",
      "Iteration 168, loss = 229368519.47207066\n",
      "Iteration 169, loss = 229237627.78489444\n",
      "Iteration 170, loss = 229220915.99286768\n",
      "Iteration 171, loss = 228754543.91769660\n",
      "Iteration 172, loss = 228770423.52306601\n",
      "Iteration 173, loss = 228833318.16924825\n",
      "Iteration 174, loss = 228641138.15258139\n",
      "Iteration 175, loss = 228547071.37237275\n",
      "Iteration 176, loss = 228445394.39415672\n",
      "Iteration 177, loss = 228566479.15038168\n",
      "Iteration 178, loss = 228034078.34772357\n",
      "Iteration 179, loss = 228124194.09382680\n",
      "Iteration 180, loss = 227973398.97784570\n",
      "Iteration 181, loss = 227939612.52813217\n",
      "Iteration 182, loss = 227944020.40599081\n",
      "Iteration 183, loss = 227601969.36947832\n",
      "Iteration 184, loss = 227550362.10480455\n",
      "Iteration 185, loss = 227438440.63506141\n",
      "Iteration 186, loss = 227298850.15450981\n",
      "Iteration 187, loss = 227448672.67165011\n",
      "Iteration 188, loss = 227084366.01694191\n",
      "Iteration 189, loss = 226953818.96576700\n",
      "Iteration 190, loss = 226682619.63323370\n",
      "Iteration 191, loss = 227009508.70581385\n",
      "Iteration 192, loss = 226759970.29246205\n",
      "Iteration 193, loss = 226435864.43497935\n",
      "Iteration 194, loss = 226391678.68551925\n",
      "Iteration 195, loss = 226522752.29609922\n",
      "Iteration 196, loss = 226303514.29367653\n",
      "Iteration 197, loss = 226296581.13333055\n",
      "Iteration 198, loss = 226094458.63926855\n",
      "Iteration 199, loss = 225857691.40948394\n",
      "Iteration 200, loss = 226161402.42691141\n",
      "MAE:  15253.74025108633\n",
      "MSE:  520443694.14043266\n",
      "RMSE:  22813.23506520793\n",
      "R2:  0.05965454246082991\n",
      "4 15253.74025108633\n",
      "13860.848977740849 9818.704465978451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# nn\n",
    "\n",
    "best_model = None\n",
    "error_cv = 0\n",
    "best_error = np.iinfo(np.int32).max\n",
    "for fold in range(5):\n",
    "    dataset_train = splits.loc[splits['fold'] != fold]\n",
    "    dataset_test = splits.loc[splits['fold'] == fold]\n",
    "    \n",
    "    train_y = dataset_train['Weekly_Sales']\n",
    "    train_x = dataset_train.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    test_y = dataset_test['Weekly_Sales']\n",
    "    test_x = dataset_test.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    print(dataset_train.shape, dataset_test.shape)\n",
    "    dataset_train = None\n",
    "    dataset_test = None\n",
    "    gc.collect()\n",
    "    predicted, model = train_and_predict(train_x, train_y, test_x,nn)\n",
    "    weights = test_x['IsHoliday'].replace(True, 5).replace(False, 1)\n",
    "    error = calculate_error(test_y, predicted, weights)\n",
    "    error_cv += error\n",
    "    print(fold, error)\n",
    "    if error < best_error:\n",
    "        print('Find best model')\n",
    "        best_error = error\n",
    "        best_model = model\n",
    "error_cv /= 5\n",
    "\n",
    "print(error_cv, best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:31:32.140076Z",
     "start_time": "2021-01-09T20:31:29.794270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100454, 15) (26662, 15)\n",
      "MAE:  51632.93834587601\n",
      "MSE:  3699399632.7157407\n",
      "RMSE:  60822.6901140992\n",
      "R2:  -5.193074163809001\n",
      "0 51632.93834587601\n",
      "Find best model\n",
      "(100524, 15) (26592, 15)\n",
      "MAE:  40751.156985069036\n",
      "MSE:  2526633756.1529217\n",
      "RMSE:  50265.631958157275\n",
      "R2:  -2797.771453626747\n",
      "1 40751.156985069036\n",
      "Find best model\n",
      "(100605, 15) (26511, 15)\n",
      "MAE:  51539.78197522198\n",
      "MSE:  3753878991.386281\n",
      "RMSE:  61268.907215538624\n",
      "R2:  -1765563.1962519453\n",
      "2 51539.78197522198\n",
      "(103416, 15) (23700, 15)\n",
      "MAE:  33403.005146295676\n",
      "MSE:  1549289137.255143\n",
      "RMSE:  39361.01036883\n",
      "R2:  -3324.7396884550344\n",
      "3 33403.005146295676\n",
      "Find best model\n",
      "(103465, 15) (23651, 15)\n",
      "MAE:  22829.53098061574\n",
      "MSE:  776817471.2855248\n",
      "RMSE:  27871.445446648886\n",
      "R2:  -0.403565435962995\n",
      "4 22829.53098061574\n",
      "Find best model\n",
      "40031.28268661569 22829.53098061574\n"
     ]
    }
   ],
   "source": [
    "# linear_reg\n",
    "\n",
    "best_model = None\n",
    "error_cv = 0\n",
    "best_error = np.iinfo(np.int32).max\n",
    "for fold in range(5):\n",
    "    dataset_train = splits.loc[splits['fold'] != fold]\n",
    "    dataset_test = splits.loc[splits['fold'] == fold]\n",
    "    \n",
    "    train_y = dataset_train['Weekly_Sales']\n",
    "    train_x = dataset_train.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    test_y = dataset_test['Weekly_Sales']\n",
    "    test_x = dataset_test.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    print(dataset_train.shape, dataset_test.shape)\n",
    "    dataset_train = None\n",
    "    dataset_test = None\n",
    "    gc.collect()\n",
    "    predicted, model = train_and_predict(train_x, train_y, test_x,linear_reg)\n",
    "    weights = test_x['IsHoliday'].replace(True, 5).replace(False, 1)\n",
    "    error = calculate_error(test_y, predicted, weights)\n",
    "    error_cv += error\n",
    "    print(fold, error)\n",
    "    if error < best_error:\n",
    "        print('Find best model')\n",
    "        best_error = error\n",
    "        best_model = model\n",
    "error_cv /= 5\n",
    "\n",
    "print(error_cv, best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:44:51.105990Z",
     "start_time": "2021-01-09T20:44:25.898648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100454, 15) (26662, 15)\n",
      "MAE:  8797.53845409179\n",
      "MSE:  269896463.5067527\n",
      "RMSE:  16428.525907906427\n",
      "R2:  0.5481729521014078\n",
      "0 8797.53845409179\n",
      "Find best model\n",
      "(100524, 15) (26592, 15)\n",
      "MAE:  5913.114398443049\n",
      "MSE:  191387484.8215916\n",
      "RMSE:  13834.286567134266\n",
      "R2:  -211.0013744752935\n",
      "1 5913.114398443049\n",
      "Find best model\n",
      "(100605, 15) (26511, 15)\n",
      "MAE:  2045.4429251708984\n",
      "MSE:  11754864.286524469\n",
      "RMSE:  3428.5367558952125\n",
      "R2:  -5527.672491497649\n",
      "2 2045.4429251708984\n",
      "Find best model\n",
      "(103416, 15) (23700, 15)\n",
      "MAE:  4625.652011397226\n",
      "MSE:  146556760.81753868\n",
      "RMSE:  12106.06297759675\n",
      "R2:  -313.60211289277987\n",
      "3 4625.652011397226\n",
      "(103465, 15) (23651, 15)\n",
      "MAE:  8088.621259579511\n",
      "MSE:  224726969.18384466\n",
      "RMSE:  14990.896210161842\n",
      "R2:  0.5939599479486593\n",
      "4 8088.621259579511\n",
      "5894.073809736495 2045.4429251708984\n"
     ]
    }
   ],
   "source": [
    "# xg\n",
    "\n",
    "best_model = None\n",
    "error_cv = 0\n",
    "best_error = np.iinfo(np.int32).max\n",
    "for fold in range(5):\n",
    "    dataset_train = splits.loc[splits['fold'] != fold]\n",
    "    dataset_test = splits.loc[splits['fold'] == fold]\n",
    "    \n",
    "    train_y = dataset_train['Weekly_Sales']\n",
    "    train_x = dataset_train.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    test_y = dataset_test['Weekly_Sales']\n",
    "    test_x = dataset_test.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    print(dataset_train.shape, dataset_test.shape)\n",
    "    dataset_train = None\n",
    "    dataset_test = None\n",
    "    gc.collect()\n",
    "    predicted, model = train_and_predict(train_x, train_y, test_x,XGBRegressor)\n",
    "    weights = test_x['IsHoliday'].replace(True, 5).replace(False, 1)\n",
    "    error = calculate_error(test_y, predicted, weights)\n",
    "    error_cv += error\n",
    "    print(fold, error)\n",
    "    if error < best_error:\n",
    "        print('Find best model')\n",
    "        best_error = error\n",
    "        best_model = model\n",
    "error_cv /= 5\n",
    "\n",
    "print(error_cv, best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:47:22.560349Z",
     "start_time": "2021-01-09T20:47:20.367802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100454, 15) (26662, 15)\n",
      "MAE:  51211.77208831215\n",
      "MSE:  3637412831.5182123\n",
      "RMSE:  60310.967754780824\n",
      "R2:  -5.089303580712587\n",
      "0 51211.77208831215\n",
      "Find best model\n",
      "(100524, 15) (26592, 15)\n",
      "MAE:  41537.00066016356\n",
      "MSE:  2631711546.2443175\n",
      "RMSE:  51300.21000195143\n",
      "R2:  -2914.166921945735\n",
      "1 41537.00066016356\n",
      "Find best model\n",
      "(100605, 15) (26511, 15)\n",
      "MAE:  49632.409800385605\n",
      "MSE:  3457178322.7973814\n",
      "RMSE:  58797.77481161495\n",
      "R2:  -1626015.7897781094\n",
      "2 49632.409800385605\n",
      "(103416, 15) (23700, 15)\n",
      "MAE:  32888.508531650885\n",
      "MSE:  1495063246.7927623\n",
      "RMSE:  38666.0477265619\n",
      "R2:  -3208.3371450459567\n",
      "3 32888.508531650885\n",
      "Find best model\n",
      "(103465, 15) (23651, 15)\n",
      "MAE:  22310.804759238134\n",
      "MSE:  747557699.1957633\n",
      "RMSE:  27341.501407123993\n",
      "R2:  -0.35069844173669096\n",
      "4 22310.804759238134\n",
      "Find best model\n",
      "39516.099167950066 22310.804759238134\n"
     ]
    }
   ],
   "source": [
    "# ridge_reg\n",
    "\n",
    "best_model = None\n",
    "error_cv = 0\n",
    "best_error = np.iinfo(np.int32).max\n",
    "for fold in range(5):\n",
    "    dataset_train = splits.loc[splits['fold'] != fold]\n",
    "    dataset_test = splits.loc[splits['fold'] == fold]\n",
    "    \n",
    "    train_y = dataset_train['Weekly_Sales']\n",
    "    train_x = dataset_train.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    test_y = dataset_test['Weekly_Sales']\n",
    "    test_x = dataset_test.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    print(dataset_train.shape, dataset_test.shape)\n",
    "    dataset_train = None\n",
    "    dataset_test = None\n",
    "    gc.collect()\n",
    "    predicted, model = train_and_predict(train_x, train_y, test_x,ridge_reg)\n",
    "    weights = test_x['IsHoliday'].replace(True, 5).replace(False, 1)\n",
    "    error = calculate_error(test_y, predicted, weights)\n",
    "    error_cv += error\n",
    "    print(fold, error)\n",
    "    if error < best_error:\n",
    "        print('Find best model')\n",
    "        best_error = error\n",
    "        best_model = model\n",
    "error_cv /= 5\n",
    "\n",
    "print(error_cv, best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T20:47:34.929976Z",
     "start_time": "2021-01-09T20:47:27.748744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100454, 15) (26662, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3683705822770.441, tolerance: 878765655.4543722\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2272391559841.7397, tolerance: 485020687.97893673\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2518991164790.5396, tolerance: 636538399.6014284\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2581184087123.367, tolerance: 572914923.108286\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1739461457214.7507, tolerance: 366154783.2889798\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1936517189988.6855, tolerance: 455121421.5246701\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1671489076895.676, tolerance: 385795891.6109469\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1419210512790.1404, tolerance: 339729454.51605004\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1631543744118.5532, tolerance: 385740250.6029063\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  52536.5637570735\n",
      "MSE:  3834127515.627921\n",
      "RMSE:  61920.332005149336\n",
      "R2:  -5.418618807169305\n",
      "0 52536.5637570735\n",
      "Find best model\n",
      "(100524, 15) (26592, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3751722209368.4604, tolerance: 887422380.8674309\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1209754072886.6116, tolerance: 283262415.34051543\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2328587757044.203, tolerance: 495592715.35053986\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2564625860253.234, tolerance: 640343928.7705666\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2612966994130.27, tolerance: 585787594.0445486\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1953852918531.5989, tolerance: 458607149.14933324\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1695259874876.4712, tolerance: 391912140.4722703\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1416574121398.7644, tolerance: 338488901.0202744\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1647342186683.817, tolerance: 389504264.79532254\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  44653.56731536238\n",
      "MSE:  3071311977.3729615\n",
      "RMSE:  55419.41877512756\n",
      "R2:  -3401.1156673460932\n",
      "1 44653.56731536238\n",
      "Find best model\n",
      "(100605, 15) (26511, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3678942570596.0815, tolerance: 881578995.9367157\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1165864366893.2158, tolerance: 278907936.8185206\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2371002024123.312, tolerance: 505462557.5109432\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2590770077756.241, tolerance: 645764341.9682616\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2568447105080.4795, tolerance: 580438236.6851368\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1539991230397.2463, tolerance: 328565371.98388445\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2052338078487.1333, tolerance: 474525087.85277873\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1700080835048.1318, tolerance: 391969162.67859554\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1641681431529.0417, tolerance: 384956130.5245758\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  45699.82440729018\n",
      "MSE:  2887585836.520794\n",
      "RMSE:  53736.261839848834\n",
      "R2:  -1358119.0082005311\n",
      "2 45699.82440729018\n",
      "(103416, 15) (23700, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3863140141913.737, tolerance: 908729279.3098483\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1149915959276.2021, tolerance: 270176684.72483957\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2233639361817.9004, tolerance: 483721035.7359908\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2977172383471.7393, tolerance: 709222384.0717968\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2778419282713.0747, tolerance: 614845860.2388612\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2073455294716.6086, tolerance: 470154794.2734215\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1670303366622.9065, tolerance: 392119326.98458946\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1289331639.0756836, tolerance: 257098055.3245233\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1973975175160.7993, tolerance: 487313746.153798\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  35827.0034627384\n",
      "MSE:  1785098581.6642323\n",
      "RMSE:  42250.42699978584\n",
      "R2:  -3830.9336643408196\n",
      "3 35827.0034627384\n",
      "Find best model\n",
      "(103465, 15) (23651, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3852427948725.5244, tolerance: 903173589.5783219\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1174234287203.3782, tolerance: 275456372.0542913\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2261138568716.581, tolerance: 491459733.64055884\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2964147571041.87, tolerance: 703767066.5718378\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2767075371116.707, tolerance: 610231645.6476979\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2079260166809.665, tolerance: 471863325.1997264\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1676764406936.9053, tolerance: 392633599.5532071\n",
      "  positive)\n",
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 797545290326.6614, tolerance: 260048717.13469005\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  23631.516993661262\n",
      "MSE:  821284516.3678696\n",
      "RMSE:  28658.06197857541\n",
      "R2:  -0.4839091586829585\n",
      "4 23631.516993661262\n",
      "Find best model\n",
      "40469.69518722514 23631.516993661262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonia\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1993222418891.9949, tolerance: 487937710.99179536\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "# lasso reg\n",
    "\n",
    "best_model = None\n",
    "error_cv = 0\n",
    "best_error = np.iinfo(np.int32).max\n",
    "for fold in range(5):\n",
    "    dataset_train = splits.loc[splits['fold'] != fold]\n",
    "    dataset_test = splits.loc[splits['fold'] == fold]\n",
    "    \n",
    "    train_y = dataset_train['Weekly_Sales']\n",
    "    train_x = dataset_train.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    test_y = dataset_test['Weekly_Sales']\n",
    "    test_x = dataset_test.drop(columns=['Weekly_Sales', 'fold'])\n",
    "    print(dataset_train.shape, dataset_test.shape)\n",
    "    dataset_train = None\n",
    "    dataset_test = None\n",
    "    gc.collect()\n",
    "    predicted, model = train_and_predict(train_x, train_y, test_x,lasso_reg)\n",
    "    weights = test_x['IsHoliday'].replace(True, 5).replace(False, 1)\n",
    "    error = calculate_error(test_y, predicted, weights)\n",
    "    error_cv += error\n",
    "    print(fold, error)\n",
    "    if error < best_error:\n",
    "        print('Find best model')\n",
    "        best_error = error\n",
    "        best_model = model\n",
    "error_cv /= 5\n",
    "\n",
    "print(error_cv, best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
